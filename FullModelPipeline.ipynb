{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15484b0d-b0d1-4f88-b4f1-e446737a7494",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "3a22c4dd-7522-42d0-aee0-9dd287628ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import chromadb\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import csv\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import pathlib\n",
    "import textwrap\n",
    "from langchain.vectorstores import Chroma\n",
    "from vertexai.preview import generative_models\n",
    "import multiprocessing\n",
    "from scipy.stats import percentileofscore\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from readability import Readability\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "import sys\n",
    "import site\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch\n",
    "from FlagEmbedding import FlagReranker\n",
    "from vertexai import preview\n",
    "from typing import Dict\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "import gradio as gr\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310d4cb-a74a-4de1-af37-c9f5343d42b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "3e8e35c7-7e80-4bd1-8150-ce3e7c3e9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_to_label(score):\n",
    "    # Map the score back to the corresponding label\n",
    "    if score < 16.666:\n",
    "        return \"pants-fire\"\n",
    "    elif score < 33.333:\n",
    "        return \"false\"\n",
    "    elif score < 50:\n",
    "        return \"barely-true\"\n",
    "    elif score < 66.666:\n",
    "        return \"half-true\"\n",
    "    elif score < 83.333:\n",
    "        return \"mostly-true\"\n",
    "    else:\n",
    "        return \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ed7db0-5ef5-462e-a137-47fba7d74914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(confidence):\n",
    "    if confidence < 0.166:\n",
    "        return \"pants-fire\"\n",
    "    elif confidence < 0.33:\n",
    "        return \"false\"\n",
    "    elif confidence < 0.5:\n",
    "        return \"barely-true\"\n",
    "    elif confidence < 0.666:\n",
    "        return \"half-true\"\n",
    "    elif confidence < 0.833:\n",
    "        return \"mostly-true\" \n",
    "    else:\n",
    "        return \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7c6eb6-6d77-4dd0-a816-9c84941f3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(tokens, model):\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a5fb90a-8a69-40ff-9097-cad5d43a0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_embeddings(text, n, model):\n",
    "    words = text.split()\n",
    "    ngrams = [words[i:i + n] for i in range(len(words) - n + 1)]  \n",
    "    embeddings = [model.wv[gram] for gram in ngrams if all(word in model.wv for word in gram)]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a998364-0447-45d1-bf81-38fbe64e00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16831eb1-a5ff-415d-9469-5a58b6ad59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_strings(text):\n",
    "    return '' if len(text) < 7 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f66a2d7b-8ec1-4192-8ffe-9c9acacfe19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d190ec3-4e6c-4408-b3bc-5dc908d8861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_chunks(text, min_words=75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        if len(current_chunk) + len(words) < min_words:\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if any(sentence.endswith(p) for p in ['.', '!', '?', '¡', '¿']):\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = words\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23075dc-4ef8-4d14-af84-d27004a90796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_clean(text):\n",
    "    # Remove spaces before or after \"'\" mark\n",
    "    text = text.replace(\" '\", \"'\").replace(\"' \", \"'\")\n",
    "    # Remove white space before \",\"\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    # Remove white space before or after \"”\" or \"“\" character\n",
    "    text = text.replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n",
    "    text = text.replace(\"Score:\", \" Score:\")\n",
    "    text = text.replace(\" ’\", \"’\").replace(\"’ \", \"’\")\n",
    "    text = text.replace(\"$ \", \"$\")\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c575404-dc61-4b43-b58b-5596fc718658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(input):\n",
    "    truth_scores = predict_tabular_classification_sample(project=\"dsc-180a-b09\",\n",
    "                                                         endpoint_id=\"4607809140427849728\",\n",
    "                                                         instance_dict={\"article\": input})\n",
    "    \n",
    "    reordered_indices = [truth_scores[0]['classes'].index(c) for c in ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']]\n",
    "    classes_reordered = [truth_scores[0]['classes'][i] for i in reordered_indices]\n",
    "    scores_reordered = [truth_scores[0]['scores'][i] for i in reordered_indices]\n",
    "    \n",
    "    # Define color transition from red to green\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0, 1, len(classes_reordered)))\n",
    "    # Plot the bar chart with color transition\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(classes_reordered, scores_reordered, color=colors)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Predictive Auto ML Truthfulness Scores')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Scores')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfe6bc5-78b8-493f-99f5-4b94339cd9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_score(label):\n",
    "    # Invert the scale and linearly map the values\n",
    "    score = (5 - final_score_mapping[label]) * 100 / 5\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc13c70-5996-459d-a8a7-10464c3a6226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABC News for Left\n",
    "def abc_updated_news():\n",
    "    abc_url = \"https://abcnews.go.com/\"\n",
    "    abc = requests.get(abc_url)\n",
    "    abc_soup = BeautifulSoup(abc.content, 'html')\n",
    "    abc_soup_url = abc_soup.find('a', {'class': 'AnchorLink News News--xl'}).get('href')\n",
    "    abc_top_article = requests.get(abc_soup_url)\n",
    "    abc_soup = BeautifulSoup(abc_top_article.content)\n",
    "    abc_content = abc_soup.find('div', {'data-testid': 'prism-article-body'}).text\n",
    "    cleaned_abc = abc_content.replace('\\'', '')\n",
    "    abc_headline = abc_soup.find('div', {'data-testid': 'prism-headline'}).text\n",
    "    return abc_headline, cleaned_abc, abc_soup_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92e3b54d-8f1b-4e76-a3a1-1019fb0d8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fox news for right \n",
    "def fox_updated_news():\n",
    "    fox_url = \"https://moxie.foxnews.com/google-publisher/latest.xml\"\n",
    "    fox = requests.get(fox_url)\n",
    "    fox_soup = BeautifulSoup(fox.content, 'xml')\n",
    "    fox_link = fox_soup.find('item').find('link')\n",
    "    fox_link_str = str(fox_link)\n",
    "    fox_link = fox_link_str[6:-7]\n",
    "    fox_headline = fox_soup.find('item').find('title').text\n",
    "    fox_content = fox_soup.find('item').find('content:encoded').text\n",
    "    fox_content_soup = BeautifulSoup(fox_content, 'html.parser')\n",
    "    for strong_tag in fox_content_soup.find_all('strong'):\n",
    "        strong_tag.extract()\n",
    "    cleaned_fox = fox_content_soup.get_text(strip=True)\n",
    "    cleaned_fox = cleaned_fox.replace('\\xa0', '')\n",
    "    cleaned_fox = cleaned_fox.replace('\\\\', '')\n",
    "    return fox_headline, cleaned_fox, fox_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7295562-3fd4-4b41-96ff-de1196e2ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPR as center\n",
    "def npr_updated_news():\n",
    "    npr_url = \"https://www.npr.org/\"\n",
    "    npr = requests.get(npr_url)\n",
    "    npr_soup = BeautifulSoup(npr.content, 'html')\n",
    "    npr_soup_url = npr_soup.find('div', {'class': 'story-text'})\n",
    "    npr_soup_url = npr_soup_url.find_all('a')[1]['href']\n",
    "    npr_article_soup = requests.get(npr_soup_url)\n",
    "    npr_article_soup = BeautifulSoup(npr_article_soup.content)\n",
    "    npr_headline = npr_article_soup.find('div', {'class': 'storytitle'}).text\n",
    "    all_text = npr_article_soup.find('div', {'id': 'storytext'}).find_all('p')\n",
    "    full_text = ''\n",
    "    for i in all_text[2:]:\n",
    "        full_text+=i.text\n",
    "    full_text = full_text.replace('\\'', '').replace('\\n', '')\n",
    "    npr_headline = npr_headline.replace('\\n', '')\n",
    "    return npr_headline, full_text, npr_soup_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa664d2a-7cc3-46e3-95d5-e279253c472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def npr_output_parse(npr_output):\n",
    "    npr_output = re.sub(r'``', '\"', npr_output)\n",
    "    npr_output = re.sub(r'\" ', '\"', npr_output)\n",
    "    npr_output = re.sub(r'\"([^ ])','\" \\\\1', npr_output)\n",
    "    return npr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a8f7354-124c-495d-aeef-2dcd68172b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fox_output_parse(fox_output):\n",
    "    fox_output = re.sub(r'``', '\"', fox_output)\n",
    "    fox_output = re.sub(r'\" ', '\"', fox_output)\n",
    "    fox_output = re.sub(r'\"([^ ])','\" \\\\1', fox_output)\n",
    "    return fox_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "587bfcd2-905e-4de0-aaae-02434485f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abc_output_parse(abc_output):\n",
    "    abc_output = re.sub(r'``', '\"', abc_output)\n",
    "    abc_output = re.sub(r'\"\\s+', '\"', abc_output)\n",
    "    return abc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "ddf83687-913f-4b43-84d2-9ada6bed6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_label(number):\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "    return reverse_mapping[number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a068f085-ec92-406d-a444-c2d9febca579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_combine(automl):\n",
    "    scaling_dict = {\"pants-fire\": 0, \"false\": 0.2, \"barely-true\": 0.4, \"half-true\": 0.6, \"mostly-true\": 0.8, \"true\": 1}\n",
    "    average_score = 0\n",
    "    for label, score in automl:\n",
    "        average_score += scaling_dict[label] * score\n",
    "    \n",
    "    bounded_score = max(0.25, min(average_score, 0.80))\n",
    "    normalized_score = (bounded_score - 0.25) / (0.80 - 0.25)\n",
    "    print(bounded_score)\n",
    "    print(normalized_score)\n",
    "    scaled_variable = normalized_score * 100\n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff905ef1-f846-42f4-9454-54747f87215d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "4c04f40a-88a8-4176-b467-cc51767e3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact articles\n",
    "pf_articles = pd.read_csv(\"Webscraping/politifact_articles.csv\")\n",
    "pf_articles = pf_articles.drop(columns='Unnamed: 0')\n",
    "pf_articles = pf_articles.drop(columns='Tldr_text_statements')\n",
    "pf_articles.rename(columns={'Statement': 'Title'}, inplace=True)\n",
    "pf_articles = pf_articles.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "bd711031-7b63-4af5-a8d2-e79755f84199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact truth datasets\n",
    "pf_statements = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_truthometer_df.csv\")\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0')\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0.1')\n",
    "pf_statements = pf_statements.drop(columns='Tldr_text_statements')\n",
    "pf_statements = pf_statements.dropna()\n",
    "pf_statements_full = pf_statements\n",
    "pf_statements = pf_statements.sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "4fb33555-68fa-4d47-9ec1-44a8184ac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factcheckorg_articles = pd.read_csv(\"Webscraping/factcheckorg_webscrape_200pages.csv\")\n",
    "factcheckorg_articles['List_data'].fillna('', inplace=True)\n",
    "factcheckorg_articles['List_data'] = factcheckorg_articles['List_data'].apply(filter_short_strings)\n",
    "factcheckorg_articles = factcheckorg_articles.dropna(subset=['Text'])\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace('Para leer en español, vea esta traducción de Google Translate.', '')\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace(r' Editor’s Note:.*$', '', regex=True)\n",
    "factcheckorg_articles = factcheckorg_articles.reset_index()\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['index', 'Unnamed: 0'])\n",
    "factcheckorg_articles['Title_and_Date'] = factcheckorg_articles['Title'] + ' , ' + factcheckorg_articles['Date']\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['Title', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ea624713-513c-49a2-978f-37efef9e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciencefeedbackorg_articles = pd.read_csv(\"Webscraping/science_feedback.csv\")\n",
    "sciencefeedbackorg_articles = sciencefeedbackorg_articles.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d2ea378e-cae2-4d27-805d-491e0f1ac8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scicheckorg_articles = pd.read_csv(\"Webscraping/scicheck_data.csv\")\n",
    "scicheckorg_articles['Title_and_Date'] = scicheckorg_articles['Title'] + ' , ' + scicheckorg_articles['Date']\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['Title', 'Date', 'Unnamed: 0'])\n",
    "scicheckorg_articles.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "cfd6e8a2-e10b-4b7c-921b-460916bb0e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The deficit has fallen under Joe Biden. It’s s...</td>\n",
       "      <td>President Joe Biden has often touted that his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Biden-versus-Trump economy: Who did better...</td>\n",
       "      <td>Get ready: In the 2024 presidential race, the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  The deficit has fallen under Joe Biden. It’s s...   \n",
       "1  The Biden-versus-Trump economy: Who did better...   \n",
       "\n",
       "                                                Text  \n",
       "0  President Joe Biden has often touted that his ...  \n",
       "1  Get ready: In the 2024 presidential race, the ...  "
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "a870e965-3d62-4788-9224-dcd6dcb3b942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Claimer</th>\n",
       "      <th>Statement</th>\n",
       "      <th>Truth_value</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16628</th>\n",
       "      <td>Brian Kemp</td>\n",
       "      <td>The \"left\" is blatantly attempting to disrupt ...</td>\n",
       "      <td>false</td>\n",
       "      <td>With Republican presidential nominee Donald Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15316</th>\n",
       "      <td>Bloggers</td>\n",
       "      <td>Greta Thunberg “going on hunger strike until T...</td>\n",
       "      <td>false</td>\n",
       "      <td>The headline of a story on a web page titled \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Claimer                                          Statement  \\\n",
       "16628  Brian Kemp  The \"left\" is blatantly attempting to disrupt ...   \n",
       "15316    Bloggers  Greta Thunberg “going on hunger strike until T...   \n",
       "\n",
       "      Truth_value                                               Text  \n",
       "16628       false  With Republican presidential nominee Donald Tr...  \n",
       "15316       false  The headline of a story on a web page titled \"...  "
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_statements.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "534141a6-27c5-4797-af33-50d0bfafc7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>List_data</th>\n",
       "      <th>Title_and_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Supreme Court ruled that states may not r...</td>\n",
       "      <td></td>\n",
       "      <td>Role of Illinois Circuit Court Judge Misrepres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People vaccinated with an authorized or approv...</td>\n",
       "      <td></td>\n",
       "      <td>Blood Donations from COVID-19 Vaccine Recipien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text List_data  \\\n",
       "0   The Supreme Court ruled that states may not r...             \n",
       "1  People vaccinated with an authorized or approv...             \n",
       "\n",
       "                                      Title_and_Date  \n",
       "0  Role of Illinois Circuit Court Judge Misrepres...  \n",
       "1  Blood Donations from COVID-19 Vaccine Recipien...  "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factcheckorg_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "f1595727-3fe7-4aaf-a2a6-e45ba6ea821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chemical found in Cheerios, Quaker Oats may ca...</td>\n",
       "      <td>Unsupported</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Researchers have “already perfected the abilit...</td>\n",
       "      <td>Misleading</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim        label\n",
       "0  Chemical found in Cheerios, Quaker Oats may ca...  Unsupported\n",
       "1  Researchers have “already perfected the abilit...   Misleading"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sciencefeedbackorg_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "bb902110-6fc7-48d0-b31d-2118131f6c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title_and_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People vaccinated with an authorized or approv...</td>\n",
       "      <td>Blood Donations from COVID-19 Vaccine Recipien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>An international study of around 99 million pe...</td>\n",
       "      <td>Study Largely Confirms Known, Rare COVID-19 Va...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  People vaccinated with an authorized or approv...   \n",
       "1  An international study of around 99 million pe...   \n",
       "\n",
       "                                      Title_and_Date  \n",
       "0  Blood Donations from COVID-19 Vaccine Recipien...  \n",
       "1  Study Largely Confirms Known, Rare COVID-19 Va...  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scicheckorg_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bc940-1167-49b0-8f5e-dbfc2f759ace",
   "metadata": {},
   "source": [
    "## Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64759e20-b8fd-4df0-8912-70d1dee5823a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Irisa's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b01f76a6-61cb-4d35-bf1e-62943ea87541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haaretz investigation reveals discrepancies in...</td>\n",
       "      <td>A viral Oct. 28 social media post claimed that...</td>\n",
       "      <td>Haaretz, an Israeli newspaper, said on X that ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin has historically … and I think large...</td>\n",
       "      <td>In 2016, Wisconsin helped to swing the preside...</td>\n",
       "      <td>Although Wisconsin has voted for more Democrat...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Haaretz investigation reveals discrepancies in...   \n",
       "1  Wisconsin has historically … and I think large...   \n",
       "\n",
       "                                             article  \\\n",
       "0  A viral Oct. 28 social media post claimed that...   \n",
       "1  In 2016, Wisconsin helped to swing the preside...   \n",
       "\n",
       "                                             summary  label  \n",
       "0  Haaretz, an Israeli newspaper, said on X that ...    4.0  \n",
       "1  Although Wisconsin has voted for more Democrat...    3.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "def read_dataset(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    df = df.drop(columns=[\"percentages\", \"check_nums\"]).drop_duplicates().dropna()\n",
    "    \n",
    "    mapping = {\n",
    "        \"TRUE\": 0,\n",
    "        \"mostly-true\": 1,\n",
    "        \"half-true\": 2,\n",
    "        \"barely-true\": 3,\n",
    "        \"FALSE\": 4,\n",
    "        \"pants-fire\": 5\n",
    "    }\n",
    "    \n",
    "    df[\"label\"] = df[\"label\"].map(mapping)\n",
    "    \n",
    "    df = df[pd.to_numeric(df[\"label\"], errors=\"coerce\").notna()]\n",
    "    df = df[[\"content\",\"article\",\"summaries\",\"label\"]]\n",
    "    df[\"content\"] = df[\"content\"].str.replace(r'[“\\”]', '', regex=True)\n",
    "    df[\"summaries\"] = df[\"summaries\"].str.replace(r'[\\[\\]\\'\"]', '', regex=True)\n",
    "    df.columns = [\"title\", \"article\", \"summary\", \"label\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_dataset(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/politifact_data_combined_prev_ratings.csv\")\n",
    "df = df = df[df['summary'] != '']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bdcfd0-bbe8-4634-b7eb-6b5b0ff3072a",
   "metadata": {},
   "source": [
    "#### Feature 1: Sentiment Analysis  (pos=1, neg=-1, neu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35d77ca1-c785-4a5a-90f1-29ca01571b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sentiment Analysis Using NLTK\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment\"] = df[\"article\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbcd02-d41e-4b4a-a628-69f3f4854279",
   "metadata": {},
   "source": [
    "#### Feature 2: Quality of Writing (Type-Token Ratio (TTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3a4e11-4f71-4471-ae8f-c48ac3a54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove stopwords and punctuation & Make lowercase\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    cleaned_text = ''.join([char for char in text if char not in punctuation])\n",
    "    return cleaned_text\n",
    "\n",
    "df[\"article\"] = df[\"article\"].apply(lambda x: x.lower())\n",
    "df[\"article\"] = df[\"article\"].apply(remove_punctuation)\n",
    "df[\"article\"] = df[\"article\"].apply(remove_stopwords)\n",
    "\n",
    "# 2. TTR = unique_words/total_words\n",
    "\n",
    "df['ttr'] = df['article'].apply(lambda x: x.split()).apply(lambda words: len(set(words)) / len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bdc9b7-8635-48d1-806e-04b50612abf5",
   "metadata": {},
   "source": [
    "#### Feature 3: Expressiveness (Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be196c4b-426a-4d32-b967-0017077a373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Open List of Adjectives (Link: https://gist.github.com/hugsy/8910dc78d208e40de42deb29e62df913)\n",
    "    ### Additional Sources: https://github.com/taikuukaits/SimpleWordlists/tree/master\n",
    "\n",
    "with open(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/adjectives.txt\", \"r\") as file:\n",
    "    adjectives = [line.strip() for line in file]\n",
    "    \n",
    "# 2. Count adjectives\n",
    "\n",
    "def count_adjectives(text):\n",
    "    words = text.split()\n",
    "    adjective_count = sum(1 for word in words if word.lower() in adjectives) / len(words)\n",
    "    return adjective_count\n",
    "\n",
    "df[\"adjectives\"] = df[\"article\"].apply(count_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c1df5-3bd2-454a-8bc5-8640c00047bd",
   "metadata": {},
   "source": [
    "#### Predictions (One vs Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffcafcc7-77d9-4876-a410-5ec26973b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"title\",\"article\",\"summary\",\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "irisa_X_train, X_test, y_train, y_test_multi = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0241af3f-6386-42a7-b418-222b7c6f9db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_758</th>\n",
       "      <th>embedding_759</th>\n",
       "      <th>embedding_760</th>\n",
       "      <th>embedding_761</th>\n",
       "      <th>embedding_762</th>\n",
       "      <th>embedding_763</th>\n",
       "      <th>embedding_764</th>\n",
       "      <th>embedding_765</th>\n",
       "      <th>embedding_766</th>\n",
       "      <th>embedding_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>-0.312834</td>\n",
       "      <td>-0.058632</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.206303</td>\n",
       "      <td>-0.012405</td>\n",
       "      <td>-0.213400</td>\n",
       "      <td>0.012543</td>\n",
       "      <td>0.059778</td>\n",
       "      <td>-0.477253</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.133215</td>\n",
       "      <td>0.202031</td>\n",
       "      <td>0.083877</td>\n",
       "      <td>-0.113434</td>\n",
       "      <td>0.403692</td>\n",
       "      <td>-0.508136</td>\n",
       "      <td>0.286420</td>\n",
       "      <td>-0.118071</td>\n",
       "      <td>-0.143064</td>\n",
       "      <td>0.107624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4591</th>\n",
       "      <td>0.206780</td>\n",
       "      <td>0.100856</td>\n",
       "      <td>-0.000326</td>\n",
       "      <td>-0.014940</td>\n",
       "      <td>0.068091</td>\n",
       "      <td>-0.324811</td>\n",
       "      <td>0.183446</td>\n",
       "      <td>0.481784</td>\n",
       "      <td>0.113302</td>\n",
       "      <td>-0.112033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054831</td>\n",
       "      <td>0.010560</td>\n",
       "      <td>0.430600</td>\n",
       "      <td>-0.092158</td>\n",
       "      <td>-0.163002</td>\n",
       "      <td>0.297586</td>\n",
       "      <td>0.039089</td>\n",
       "      <td>-0.314918</td>\n",
       "      <td>0.233020</td>\n",
       "      <td>0.139470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4127</th>\n",
       "      <td>-0.484168</td>\n",
       "      <td>-0.321923</td>\n",
       "      <td>0.238735</td>\n",
       "      <td>0.242504</td>\n",
       "      <td>-0.075716</td>\n",
       "      <td>-0.076495</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>0.726652</td>\n",
       "      <td>-0.593499</td>\n",
       "      <td>-0.229378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185068</td>\n",
       "      <td>0.256736</td>\n",
       "      <td>-0.039607</td>\n",
       "      <td>-0.021535</td>\n",
       "      <td>0.273586</td>\n",
       "      <td>-0.375381</td>\n",
       "      <td>0.151440</td>\n",
       "      <td>-0.081353</td>\n",
       "      <td>0.051139</td>\n",
       "      <td>0.006516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>-0.238995</td>\n",
       "      <td>-0.556372</td>\n",
       "      <td>-0.080801</td>\n",
       "      <td>-0.130556</td>\n",
       "      <td>0.354441</td>\n",
       "      <td>-0.015081</td>\n",
       "      <td>0.034525</td>\n",
       "      <td>0.307757</td>\n",
       "      <td>-0.091992</td>\n",
       "      <td>0.278078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059024</td>\n",
       "      <td>-0.198954</td>\n",
       "      <td>-0.031907</td>\n",
       "      <td>-0.170072</td>\n",
       "      <td>0.423216</td>\n",
       "      <td>-0.487244</td>\n",
       "      <td>0.151088</td>\n",
       "      <td>0.195280</td>\n",
       "      <td>-0.005206</td>\n",
       "      <td>-0.133398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>0.105305</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>-0.036647</td>\n",
       "      <td>-0.247716</td>\n",
       "      <td>0.340607</td>\n",
       "      <td>0.054135</td>\n",
       "      <td>-0.182309</td>\n",
       "      <td>0.975725</td>\n",
       "      <td>-0.209552</td>\n",
       "      <td>-0.427320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148062</td>\n",
       "      <td>-0.369040</td>\n",
       "      <td>-0.025342</td>\n",
       "      <td>-0.334527</td>\n",
       "      <td>0.103794</td>\n",
       "      <td>-0.080074</td>\n",
       "      <td>0.188419</td>\n",
       "      <td>-0.357470</td>\n",
       "      <td>0.340472</td>\n",
       "      <td>-0.144098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.226494</td>\n",
       "      <td>-0.448733</td>\n",
       "      <td>-0.275574</td>\n",
       "      <td>-0.380240</td>\n",
       "      <td>-0.109136</td>\n",
       "      <td>-0.125699</td>\n",
       "      <td>0.213254</td>\n",
       "      <td>0.272815</td>\n",
       "      <td>-0.302471</td>\n",
       "      <td>0.374626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080435</td>\n",
       "      <td>0.128979</td>\n",
       "      <td>-0.168187</td>\n",
       "      <td>-0.021564</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>-0.611549</td>\n",
       "      <td>0.110881</td>\n",
       "      <td>-0.158066</td>\n",
       "      <td>-0.217355</td>\n",
       "      <td>0.056725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>-0.151881</td>\n",
       "      <td>0.068466</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>-0.146621</td>\n",
       "      <td>0.294401</td>\n",
       "      <td>-0.500240</td>\n",
       "      <td>0.217544</td>\n",
       "      <td>0.409588</td>\n",
       "      <td>-0.165157</td>\n",
       "      <td>-0.026576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302055</td>\n",
       "      <td>-0.131888</td>\n",
       "      <td>0.011877</td>\n",
       "      <td>-0.016099</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>-0.209183</td>\n",
       "      <td>0.327115</td>\n",
       "      <td>-0.477465</td>\n",
       "      <td>0.049188</td>\n",
       "      <td>0.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0.118058</td>\n",
       "      <td>0.073457</td>\n",
       "      <td>0.083286</td>\n",
       "      <td>-0.092973</td>\n",
       "      <td>0.206668</td>\n",
       "      <td>0.067941</td>\n",
       "      <td>0.164632</td>\n",
       "      <td>0.753002</td>\n",
       "      <td>-0.091234</td>\n",
       "      <td>0.100234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330742</td>\n",
       "      <td>-0.308487</td>\n",
       "      <td>-0.211451</td>\n",
       "      <td>-0.422116</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.050793</td>\n",
       "      <td>-0.572129</td>\n",
       "      <td>0.219668</td>\n",
       "      <td>-0.131846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>0.014237</td>\n",
       "      <td>0.204254</td>\n",
       "      <td>0.103944</td>\n",
       "      <td>-0.147323</td>\n",
       "      <td>0.161504</td>\n",
       "      <td>-0.388124</td>\n",
       "      <td>0.470038</td>\n",
       "      <td>0.562142</td>\n",
       "      <td>-0.027968</td>\n",
       "      <td>-0.039667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153926</td>\n",
       "      <td>-0.272475</td>\n",
       "      <td>0.102795</td>\n",
       "      <td>-0.495124</td>\n",
       "      <td>-0.197697</td>\n",
       "      <td>0.097794</td>\n",
       "      <td>0.082199</td>\n",
       "      <td>-0.568608</td>\n",
       "      <td>0.147847</td>\n",
       "      <td>0.083071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4163</th>\n",
       "      <td>0.162132</td>\n",
       "      <td>-0.076939</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>0.370795</td>\n",
       "      <td>0.006681</td>\n",
       "      <td>-0.007656</td>\n",
       "      <td>-0.254665</td>\n",
       "      <td>0.688399</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.259529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.472284</td>\n",
       "      <td>-0.320561</td>\n",
       "      <td>0.013307</td>\n",
       "      <td>-0.219552</td>\n",
       "      <td>0.083501</td>\n",
       "      <td>-0.119624</td>\n",
       "      <td>-0.085762</td>\n",
       "      <td>-0.326220</td>\n",
       "      <td>0.079101</td>\n",
       "      <td>0.053696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1349 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "2531    -0.312834    -0.058632     0.006572     0.206303    -0.012405   \n",
       "4591     0.206780     0.100856    -0.000326    -0.014940     0.068091   \n",
       "4127    -0.484168    -0.321923     0.238735     0.242504    -0.075716   \n",
       "3624    -0.238995    -0.556372    -0.080801    -0.130556     0.354441   \n",
       "5494     0.105305     0.324336    -0.036647    -0.247716     0.340607   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1235     0.226494    -0.448733    -0.275574    -0.380240    -0.109136   \n",
       "1225    -0.151881     0.068466     0.001087    -0.146621     0.294401   \n",
       "1488     0.118058     0.073457     0.083286    -0.092973     0.206668   \n",
       "1728     0.014237     0.204254     0.103944    -0.147323     0.161504   \n",
       "4163     0.162132    -0.076939     0.063866     0.370795     0.006681   \n",
       "\n",
       "      embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "2531    -0.213400     0.012543     0.059778    -0.477253    -0.065189  ...   \n",
       "4591    -0.324811     0.183446     0.481784     0.113302    -0.112033  ...   \n",
       "4127    -0.076495    -0.188093     0.726652    -0.593499    -0.229378  ...   \n",
       "3624    -0.015081     0.034525     0.307757    -0.091992     0.278078  ...   \n",
       "5494     0.054135    -0.182309     0.975725    -0.209552    -0.427320  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1235    -0.125699     0.213254     0.272815    -0.302471     0.374626  ...   \n",
       "1225    -0.500240     0.217544     0.409588    -0.165157    -0.026576  ...   \n",
       "1488     0.067941     0.164632     0.753002    -0.091234     0.100234  ...   \n",
       "1728    -0.388124     0.470038     0.562142    -0.027968    -0.039667  ...   \n",
       "4163    -0.007656    -0.254665     0.688399     0.002170     0.259529  ...   \n",
       "\n",
       "      embedding_758  embedding_759  embedding_760  embedding_761  \\\n",
       "2531      -0.133215       0.202031       0.083877      -0.113434   \n",
       "4591       0.054831       0.010560       0.430600      -0.092158   \n",
       "4127      -0.185068       0.256736      -0.039607      -0.021535   \n",
       "3624      -0.059024      -0.198954      -0.031907      -0.170072   \n",
       "5494      -0.148062      -0.369040      -0.025342      -0.334527   \n",
       "...             ...            ...            ...            ...   \n",
       "1235       0.080435       0.128979      -0.168187      -0.021564   \n",
       "1225      -0.302055      -0.131888       0.011877      -0.016099   \n",
       "1488      -0.330742      -0.308487      -0.211451      -0.422116   \n",
       "1728       0.153926      -0.272475       0.102795      -0.495124   \n",
       "4163      -0.472284      -0.320561       0.013307      -0.219552   \n",
       "\n",
       "      embedding_762  embedding_763  embedding_764  embedding_765  \\\n",
       "2531       0.403692      -0.508136       0.286420      -0.118071   \n",
       "4591      -0.163002       0.297586       0.039089      -0.314918   \n",
       "4127       0.273586      -0.375381       0.151440      -0.081353   \n",
       "3624       0.423216      -0.487244       0.151088       0.195280   \n",
       "5494       0.103794      -0.080074       0.188419      -0.357470   \n",
       "...             ...            ...            ...            ...   \n",
       "1235       0.531519      -0.611549       0.110881      -0.158066   \n",
       "1225       0.192200      -0.209183       0.327115      -0.477465   \n",
       "1488       0.059300      -0.000108      -0.050793      -0.572129   \n",
       "1728      -0.197697       0.097794       0.082199      -0.568608   \n",
       "4163       0.083501      -0.119624      -0.085762      -0.326220   \n",
       "\n",
       "      embedding_766  embedding_767  \n",
       "2531      -0.143064       0.107624  \n",
       "4591       0.233020       0.139470  \n",
       "4127       0.051139       0.006516  \n",
       "3624      -0.005206      -0.133398  \n",
       "5494       0.340472      -0.144098  \n",
       "...             ...            ...  \n",
       "1235      -0.217355       0.056725  \n",
       "1225       0.049188       0.296875  \n",
       "1488       0.219668      -0.131846  \n",
       "1728       0.147847       0.083071  \n",
       "4163       0.079101       0.053696  \n",
       "\n",
       "[1349 rows x 768 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "fe89f1e2-a703-4d53-9fca-d05760e04900",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_percentile = percentileofscore(irisa_X_train['sentiment'], irisa_X_train['sentiment'])[0]\n",
    "ttr_percentile = percentileofscore(irisa_X_train['ttr'], irisa_X_train['ttr'])[0]\n",
    "adjectives_percentile = percentileofscore(irisa_X_train['adjectives'], irisa_X_train['adjectives'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35e44ff5-4638-43ea-a6bd-888fd653535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.1362281822052"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abb7fb54-938e-48c8-9c56-9f31c9a827c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531063829787234\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    #KNeighborsClassifier(2),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    #DecisionTreeClassifier(max_depth=5),\n",
    "    #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    #MLPClassifier(alpha=1, max_iter=1000),\n",
    "    #AdaBoostClassifier(),\n",
    "    GaussianNB()]\n",
    "    #QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    irisa_clf = OneVsOneClassifier(classifier).fit(irisa_X_train, y_train)\n",
    "    predictions = irisa_clf.predict(X_test)\n",
    "    print(accuracy_score(y_test_multi, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b625419f-69d7-4c77-85b5-ee89d614f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new article input\n",
    "#clickbait\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_title = tfidf_vectorizer.fit_transform([\"Example news title\"])\n",
    "tfidf_article = tfidf_vectorizer.transform([news])\n",
    "cosine_sim = cosine_similarity(tfidf_title, tfidf_article)\n",
    "irisa_clickbait = cosine_sim.diagonal()[0]\n",
    "\n",
    "#sentiment prediction\n",
    "irisa_sentiment = analyzer.polarity_scores(news)[\"compound\"]\n",
    "\n",
    "#quality of writing prediction\n",
    "words = news.split()\n",
    "irisa_qor_ratio = len(set(words)) / len(words)\n",
    "\n",
    "#sensationalism\n",
    "irisa_sensationalism = count_adjectives(news)\n",
    "\n",
    "#adding to df for prediction\n",
    "irisa_data = {\n",
    "    \"sentiment\": [irisa_sentiment],\n",
    "    \"ttr\": [irisa_qor_ratio],\n",
    "    \"adjectives\": [irisa_sensationalism]\n",
    "}\n",
    "\n",
    "irisa_pred_df = pd.DataFrame(irisa_data)\n",
    "\n",
    "#irisa final prediction\n",
    "final_prediction = irisa_clf.predict(irisa_pred_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac4ceafd-f4de-4495-816a-37c026b768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "        \"TRUE\": 0,\n",
    "        \"mostly-true\": 1,\n",
    "        \"half-true\": 2,\n",
    "        \"barely-true\": 3,\n",
    "        \"FALSE\": 4,\n",
    "        \"pants-fire\": 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a833f776-d017-4cdf-be5d-24eff49e651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "irisa_final_label_pred = number_to_label(final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1593140-a52a-4cee-9608-dec37d282ee4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lohit's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cbdbade9-b53d-4371-8da8-b5fb3a61e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load and clean\n",
    "data = pd.read_csv(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/politifact_data_combined_prev_ratings.csv\")\n",
    "noise_labels = set(['full-flop', 'half-flip', 'no-flip'])\n",
    "data = data.query(\"label not in ['full-flop', 'half-flip', 'no-flip']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f71b4ea-b8b4-4700-984e-038732c80bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>when/where</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>speaker</th>\n",
       "      <th>documented_time</th>\n",
       "      <th>percentages</th>\n",
       "      <th>check_nums</th>\n",
       "      <th>summaries</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>stated on October 28, 2023 in a screenshot sha...</td>\n",
       "      <td>“Haaretz investigation reveals discrepancies i...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Madison Czopek</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>['0%' '0%' '2%' '7%' '67%' '21%']</td>\n",
       "      <td>[  5   3  16  54 473 152]</td>\n",
       "      <td>['Haaretz, an Israeli newspaper, said on X tha...</td>\n",
       "      <td>A viral Oct. 28 social media post claimed that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>stated on May 30, 2023 in Interview:</td>\n",
       "      <td>“Wisconsin has historically … and I think larg...</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Laura Schulte</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>['12%' '21%' '18%' '19%' '21%' '5%']</td>\n",
       "      <td>[26 45 39 41 44 11]</td>\n",
       "      <td>['Although Wisconsin has voted for more Democr...</td>\n",
       "      <td>In 2016, Wisconsin helped to swing the preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             media                                         when/where  \\\n",
       "0  Instagram posts  stated on October 28, 2023 in a screenshot sha...   \n",
       "1     Scott Walker               stated on May 30, 2023 in Interview:   \n",
       "\n",
       "                                             content        label  \\\n",
       "0  “Haaretz investigation reveals discrepancies i...        FALSE   \n",
       "1  “Wisconsin has historically … and I think larg...  barely-true   \n",
       "\n",
       "          speaker   documented_time                           percentages  \\\n",
       "0  Madison Czopek  October 31, 2023     ['0%' '0%' '2%' '7%' '67%' '21%']   \n",
       "1   Laura Schulte  October 31, 2023  ['12%' '21%' '18%' '19%' '21%' '5%']   \n",
       "\n",
       "                  check_nums  \\\n",
       "0  [  5   3  16  54 473 152]   \n",
       "1        [26 45 39 41 44 11]   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  ['Haaretz, an Israeli newspaper, said on X tha...   \n",
       "1  ['Although Wisconsin has voted for more Democr...   \n",
       "\n",
       "                                             article  \n",
       "0  A viral Oct. 28 social media post claimed that...  \n",
       "1  In 2016, Wisconsin helped to swing the preside...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97123229-8329-4fa2-ba91-7349c2157e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6658758402530645\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       0.60      0.85      0.70      1438\n",
      "        TRUE       0.64      0.41      0.50       555\n",
      " barely-true       0.78      0.57      0.66       805\n",
      "   half-true       0.72      0.78      0.75       830\n",
      " mostly-true       0.60      0.61      0.60       757\n",
      "  pants-fire       0.89      0.52      0.66       673\n",
      "\n",
      "    accuracy                           0.67      5058\n",
      "   macro avg       0.70      0.62      0.64      5058\n",
      "weighted avg       0.69      0.67      0.66      5058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data[['content', 'article']]\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the text data with unigrams and bigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['content'] + \" \" + X_train['article'])\n",
    "X_test_tfidf = tfidf.transform(X_test['content'] + \" \" + X_test['article'])\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "04d8927b-127a-4082-ac21-ab6f54de48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barely-true'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the test instance\n",
    "X_test_instance = [\"example_title\" + \" \" + news]\n",
    "\n",
    "# Vectorize the test instance using the same TF-IDF vectorizer trained on the training data\n",
    "X_test_instance_tfidf = tfidf.transform(X_test_instance)\n",
    "\n",
    "# Make predictions for the test instance\n",
    "y_pred_instance = classifier.predict(X_test_instance_tfidf)\n",
    "\n",
    "y_pred_proba = classifier.predict_proba(X_test_instance_tfidf)\n",
    "positive_class_proba = y_pred_proba\n",
    "overall_score = (positive_class_proba[0][0] * 0.2) + (positive_class_proba[0][1] * 1) + (positive_class_proba[0][2] * 0.4) + (positive_class_proba[0][3] * 0.6) + (positive_class_proba[0][4] * 0.8) + (positive_class_proba[0][5] * 0.0)\n",
    "lohit_predicted_label = predict_label(overall_score)\n",
    "lohit_predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29231e0a-25f2-46c4-9634-319bb1399c98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nick's Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c03c0a-54c1-4c92-8efe-d65e3b2c81b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Factor 1: Flesch-Kincaid Grade Level Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5501b0f3-7a9f-4734-abdd-24f3fe1f547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.902802547770705"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Readability(news_test)\n",
    "fk = r.flesch_kincaid()\n",
    "flesch_score = fk.score\n",
    "if flesch_score > 12:\n",
    "    diff = flesch_score - 12\n",
    "    fk_rating = 100 - (diff * 6)\n",
    "elif flesch_score < 8:\n",
    "    diff = 8 - flesch_score\n",
    "    fk_rating = 100 - (diff * 6)\n",
    "else:\n",
    "    fk_rating = 100\n",
    "fk_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040db8a-0027-42d1-baf4-9442bbc01f75",
   "metadata": {},
   "source": [
    "#### Factor 2: Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "d0e02001-425c-4a43-a1fe-a411db7ab7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "moving_sentiment_value = 0\n",
    "number_of_paragraphs = 0\n",
    "paragraphs = news.split('\\n\\n')\n",
    "for i in paragraphs:\n",
    "    cleaned_text = ' '.join(i.split()).replace(\"\\'\", '')\n",
    "    compound_sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
    "    moving_sentiment_value += compound_sentiment_score\n",
    "    number_of_paragraphs += 1\n",
    "overall_sentiment = moving_sentiment_value / number_of_paragraphs\n",
    "overall_sent_score = 100\n",
    "if overall_sentiment < -0.2:\n",
    "    overall_sent_score = 100 + (overall_sentiment * 100)\n",
    "overall_sent_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42ad2c-1f9d-493f-9937-f64045b0d156",
   "metadata": {},
   "source": [
    "#### Factor 3: Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fd7a2b6-8f1b-4abe-8940-4866da024ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait = pd.read_csv(\"Data/Clickbait_Data/clickbait_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f33592dc-9ac0-4006-888e-aaa14b8372c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier = Linear SVM, Score (test, accuracy) = 95.67, Training time = 53.45 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Best --> Classifier = Linear SVM, Score (test, accuracy) = 95.67\n"
     ]
    }
   ],
   "source": [
    "#determining clickbait\n",
    "names = [\"Linear SVM\"] \n",
    "\n",
    "classifiers = [SVC(kernel=\"linear\", C=0.025, probability=True)]\n",
    "\n",
    "\n",
    "#Preprocess, train/test split, and \n",
    "clickbait['PreprocessedTitle'] = clickbait['headline'].apply(preprocess_text)\n",
    "X_train_click, X_test_click, y_train_click, y_test_click = train_test_split(clickbait['PreprocessedTitle'], clickbait['clickbait'], test_size=0.2, random_state=42)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train_click)\n",
    "X_test_counts = count_vectorizer.transform(X_test_click)\n",
    "\n",
    "\n",
    "max_score = 0.0\n",
    "max_class = ''\n",
    "# iterate over classifiers\n",
    "for name, clf_ in zip(names, classifiers):\n",
    "    start_time = time.time()\n",
    "    clf_.fit(X_train_counts, y_train_click)\n",
    "    score = 100.0 * clf_.score(X_test_counts, y_test_click)\n",
    "    print('Classifier = %s, Score (test, accuracy) = %.2f,' %(name, score), 'Training time = %.2f seconds' % (time.time() - start_time))\n",
    "    \n",
    "    if score > max_score:\n",
    "        clf_best = clf_\n",
    "        max_score = score\n",
    "        max_class = name\n",
    "\n",
    "print(80*'-' )\n",
    "print('Best --> Classifier = %s, Score (test, accuracy) = %.2f' %(max_class, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "daa425b8-74cc-4eb4-beab-dd743a0e2dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title_processed = preprocess_text(\"example news\")\n",
    "article_title_vectorized = count_vectorizer.transform([article_title_processed])\n",
    "clickbait_probability = clf_best.predict_proba(article_title_vectorized)\n",
    "confidence_not_clickbait = clickbait_probability[:, 0]\n",
    "confidence_not_clickbait = confidence_not_clickbait[0]\n",
    "nick_predicted_label = predict_label(confidence_not_clickbait)\n",
    "nick_predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d2e7eb9-4c0c-4222-9396-134558bd88f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors_combined = ((confidence_not_clickbait * 100) + overall_sent_score + fk_rating) / 3\n",
    "nicks_predicted_label = predict_label(factors_combined)\n",
    "nicks_predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d0b35-b00d-416b-9f2e-49297edf5394",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Henry's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "bfb2670e-63d3-4394-864b-b9d13324796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages', '/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/tmpw04tfh_6', '/Users/nicholasshor/Library/Python/3.11/lib/python/site-packages', '/Users/nicholasshor/Library/Python/3.11/lib/python/site-packages']\n"
     ]
    }
   ],
   "source": [
    "user_site_packages = site.USER_SITE\n",
    "sys.path.append(user_site_packages)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2adfb37c-62fa-4855-87d9-1568f01c2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ID', 'Label', 'Statement', 'Subject(s)', 'Speaker','Speaker\\'s Job Title', 'State Info', 'Party Affiliation','Barely True', 'False', 'Half True', 'Mostly True', 'Pants on Fire','Context']\n",
    "df = pd.read_csv('/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/Liar_plus/train.tsv', delimiter='\\t', header = None, quoting=csv.QUOTE_NONE)\n",
    "df = df.drop(columns=[0, 15])\n",
    "df = df.rename(columns=dict(zip(df.columns, columns)))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c278f493-ae85-4889-956c-d9a89761d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_ = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6bbef1e2-a483-434d-ae2e-70fce94133e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = df['Statement'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "#tokenizeing the statements\n",
    "tokenized_statements = [tokenizer(statement, return_tensors=\"pt\", truncation=True, padding=True) for statement in statements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21ff52c7-d5cb-4d81-a004-66f7d29ba50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "henry_model = model_.to(device)\n",
    "\n",
    "# Move tokenized statements to GPU\n",
    "tokenized_statements_gpu = [inputs.to(device) for inputs in tokenized_statements]\n",
    "\n",
    "# Extract BERT embeddings\n",
    "with torch.no_grad():\n",
    "    henry_model.eval()\n",
    "    statement_embeddings = [henry_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy() for inputs in tokenized_statements_gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7dbbea4-d8dc-4ca1-881e-b7954b5cb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten\n",
    "X_embeddings = np.vstack(statement_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2262821c-236d-47a5-8804-64dfdcb0cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the truth counts with the embeddings\n",
    "X_embeddings_df = pd.DataFrame(X_embeddings, columns=[f\"embedding_{i}\" for i in range(X_embeddings.shape[1])])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings_df, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "01f9e16c-a8c9-4d4c-9d97-3063743aa981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.24      0.04      0.08       200\n",
      "       false       0.32      0.34      0.33       297\n",
      "   half-true       0.23      0.41      0.30       273\n",
      " mostly-true       0.28      0.45      0.34       268\n",
      "  pants-fire       1.00      0.01      0.02        84\n",
      "        true       0.34      0.12      0.18       227\n",
      "\n",
      "    accuracy                           0.27      1349\n",
      "   macro avg       0.40      0.23      0.21      1349\n",
      "weighted avg       0.33      0.27      0.24      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=3000, random_state=42, min_samples_split = 2, min_samples_leaf = 1, max_depth = 30)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cf7ec-1738-409b-b4d1-4e54bd2f544f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Code for pred scores combining and model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f2a78759-5f2b-47e3-84ca-e1913f797b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score_mapping = {\n",
    "        \"true\": 0,\n",
    "        \"TRUE\": 0,\n",
    "        \" true\":0,\n",
    "        \"mostly-true\": 1,\n",
    "        \" mostly-true\": 1,\n",
    "        \"half-true\": 2,\n",
    "        \" half-true\": 2,\n",
    "        \"barely-true\": 3,\n",
    "        \" barely-true\": 3,\n",
    "        \"false\": 4,\n",
    "        \" false\": 4,\n",
    "        \"FALSE\": 4,\n",
    "        \"pants-fire\": 5,\n",
    "        \" pants-fire\": 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6dffe6f7-855b-4d73-a00d-99a9c6542756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'half-true'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_as_scores = []\n",
    "for i in all_ratings:\n",
    "    labels_as_scores.append(label_to_score(i))\n",
    "pred_models_ave = sum(labels_as_scores) / len(labels_as_scores)\n",
    "final_pred_label = score_to_label(pred_models_ave)\n",
    "final_pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced89dd-106a-4d60-adb3-a000944d564e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d85e0467-d390-42b5-80fa-15c959c0fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FactCheckOrg Article Chunking\n",
    "factcheckorg_articles['chunks_text'] = factcheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "factcheckorg_articles['chunkslistdata'] = factcheckorg_articles['List_data'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = factcheckorg_articles['chunks_text'].apply(len).max()\n",
    "max_chunks_list_data = factcheckorg_articles['chunkslistdata'].apply(len).max()\n",
    "max_total_chunks = max(max_chunks_text, max_chunks_list_data)\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_total_chunks + 1):\n",
    "    factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "    factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['chunks_text', 'chunkslistdata', 'Text', 'List_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "6d04d655-1dd9-4e78-9104-3b41b6bea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Statement Text Chunking\n",
    "pf_statements['chunks'] = pf_statements['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_statements['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_statements[f'chunk_{i}'] = pf_statements['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_statements = pf_statements.drop(columns=['chunks', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9383641a-2e4c-45a3-b363-95aba9f5de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Articles Chunking\n",
    "pf_articles['chunks'] = pf_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_articles['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_articles = pf_articles.drop(columns=['chunks', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2fe76bdb-2700-4532-9380-1daec03e19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SciCheckOrg Articles Chunking\n",
    "scicheckorg_articles['chunks_text'] = scicheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = scicheckorg_articles['chunks_text'].apply(len).max()\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_chunks_text + 1):\n",
    "    scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['chunks_text', 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fe66e-bac9-461d-a7e2-a4311b23adec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23b31956-0dbb-4bf3-8b2c-b7df2e88b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63a961bb-41c9-4e28-bb96-360fbca000af",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_CONTEXT_VDB = chroma_client.create_collection(name=\"RAG_CONTEXT_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72fc7d10-34c5-4142-9965-9a674f004bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_STATEMENTS_VDB = chroma_client.create_collection(name=\"RAG_STATEMENTS_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "40ea61f4-0dfe-4b95-9b43-46d3687fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding pf statement justifications to Context VDB\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "chunks_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements.iterrows():\n",
    "    statement = row['Statement']\n",
    "    claimer = row['Claimer']\n",
    "    for col in pf_statements.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                metadata_list.append({\"Statement\": statement, \"Context\": \"Yes\", \"Claimer\": claimer})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ad55249-93cb-4196-8111-00946af61458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter justifications to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9fce4111-b9e1-4692-b32e-d23ed3930d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding politifact truth-o-meter statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements_full.iterrows():\n",
    "    truth_value = row['Truth_value']\n",
    "    claimer = row['Claimer']\n",
    "    statement = row['Statement']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value,\"Claimer\": claimer})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "afc8f317-5f3b-4ea7-81a1-fee50c26be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c059fb88-3459-455a-9d21-33b62b13b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding factcheck.org data to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in factcheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in factcheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1\n",
    "        elif col.startswith('chunklist'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44369d55-9b8a-4e94-b4f4-876ff2e9bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n"
     ]
    }
   ],
   "source": [
    "#Adding factcheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6af85c54-a3a7-4588-8d13-834688bc1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding SciCheckOrg articles to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in scicheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in scicheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0bbd3a9-cd3b-4ecf-8491-5d1bf01df15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188549"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_CONTEXT_VDB.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4b02868-a4e4-4db1-8264-8adc05216e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Adding scicheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c40685c-37f7-4f1a-8bb2-d0780a3c343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding ScienceFeedbackOrg statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in sciencefeedbackorg_articles.iterrows():\n",
    "    truth_value = row['label']\n",
    "    statement = row['claim']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13af0dcf-fc27-446e-a249-d5869954d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf sciencefeedback statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337cf1-74e2-4e5a-ba6b-1f9917e09d02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## FULL GEN AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a0574039-4d76-4d3f-b385-417b500bbfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading live examples\n",
    "abc_headline, abc_content, abc_link = abc_updated_news()\n",
    "npr_headline, npr_content, npr_link = npr_updated_news()\n",
    "fox_headline, fox_content, fox_link = fox_updated_news()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4e77-5130-4be5-8292-aaebc511f392",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Perspective API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2f83baf7-d65d-42fd-81f3-c945bf22b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSPECTIVE_API_KEY = 'AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5c58d5c6-946d-422f-84b3-1df34ddc35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds of output\n",
    "attributeThresholds = {\n",
    "    'INSULT': 0.8,\n",
    "    'TOXICITY': 0.8,\n",
    "    'THREAT': 0.5,\n",
    "    'SEXUALLY_EXPLICIT': 0.5,\n",
    "    'PROFANITY': 0.8\n",
    "}\n",
    "requestedAttributes = {}\n",
    "for key in attributeThresholds:\n",
    "    requestedAttributes[key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4ca06-631a-4226-b203-e12f632d1f97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Flag Embedding Reranker import/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "983ba8a0-b319-48b3-95d9-a818a1166c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b381903-1774-4c4a-860a-09e06405ca12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### AutoML Predict Function and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "671777f2-2aab-4429-8ad8-7b2f1d06939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tabular_classification_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instance_dict: Dict,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # for more info on the instance schema, please use get_model_sample.py\n",
    "    # and look at the yaml found in instance_schema_uri\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/tabular_classification_1.0.0.yaml for the format of the predictions.\n",
    "    prediction_list=[]\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        prediction_list.append(dict(prediction))\n",
    "    return prediction_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4bf32e4b-daac-45ec-8d61-d11db12d6ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1836276352405548, 0.008846416138112545, 0.01037078443914652, 0.09028053283691406, 0.1769963353872299, 0.5298783183097839]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing=predict_tabular_classification_sample(\n",
    "    project=\"dsc-180a-b09\",\n",
    "    endpoint_id=\"4607809140427849728\",\n",
    "    instance_dict={\"article\": news})\n",
    "testing[0]['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359fa40-6fc2-4c82-961c-3d800bdb28da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Liar Liar Plus Dataset Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40919e43-d3fc-4033-9a27-6a1ecf18f5a7",
   "metadata": {},
   "source": [
    "This section is only used for testing the performance of the generative model and is not necessary for ones own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "b3c8d02e-822a-43bf-a08b-4c0f6234e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GenAI_article_truth_processing(news_article, history, examples, headline):\n",
    "#     history_output = []\n",
    "\n",
    "        \n",
    "#     #instantiating RAG re-ranking mecahnism\n",
    "#     reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "#     #converting full news article to string\n",
    "#     news_article = f\"\"\"{news_article}\"\"\"\n",
    "#     example_news_provider = f\"\"\"{news_article}\"\"\"\n",
    "#     headline = f\"\"\"{headline}\"\"\"\n",
    "\n",
    "#     #setting up pre-processed examples correctly\n",
    "#     if news_article == \"ABC\" or news_article == \"abc\" or news_article == \"Abc\":\n",
    "#         news_article = abc_content\n",
    "#         headline = abc_headline\n",
    "#     elif news_article == \"NPR\" or news_article == \"npr\" or news_article == \"Npr\":\n",
    "#         news_article = npr_content\n",
    "#         headline = npr_headline\n",
    "#     elif news_article == \"FOX\" or news_article == \"Fox\" or news_article == \"fox\":\n",
    "#         news_article = fox_content\n",
    "#         headline = fox_headline\n",
    "#     overall_synopsis = \"Overall Score\"\n",
    "        \n",
    "#     #getting history for context\n",
    "#     history = history or []\n",
    "\n",
    "#     #predictive models\n",
    "#     #IRISAS PREDICTIONS\n",
    "\n",
    "#     #sentiment prediction\n",
    "#     irisa_sentiment = analyzer.polarity_scores(news_article)[\"compound\"]\n",
    "    \n",
    "#     #quality of writing prediction\n",
    "#     words = news_article.split()\n",
    "#     irisa_qor_ratio = len(set(words)) / len(words)\n",
    "    \n",
    "#     #sensationalism\n",
    "#     irisa_sensationalism = count_adjectives(news_article)\n",
    "    \n",
    "#     #adding to df for prediction\n",
    "#     irisa_data = {\n",
    "#         \"sentiment\": [irisa_sentiment],\n",
    "#         \"ttr\": [irisa_qor_ratio],\n",
    "#         \"adjectives\": [irisa_sensationalism]\n",
    "#     }\n",
    "    \n",
    "#     irisa_pred_df = pd.DataFrame(irisa_data)\n",
    "    \n",
    "#     #irisa final prediction\n",
    "#     irisa_final_prediction = irisa_clf.predict(irisa_pred_df)[0]\n",
    "#     irisa_final_label_pred = number_to_label(irisa_final_prediction)\n",
    "\n",
    "#     #irisas prediction percentile\n",
    "#     sentiment_percentile = percentileofscore(irisa_X_train['sentiment'], irisa_pred_df['sentiment'])[0]\n",
    "#     ttr_percentile = percentileofscore(irisa_X_train['ttr'], irisa_pred_df['ttr'])[0]\n",
    "#     adjectives_percentile = percentileofscore(irisa_X_train['adjectives'], irisa_pred_df['adjectives'])[0]\n",
    "\n",
    "#     #LOHITS PREDICTIONS\n",
    "#     X_test_instance = [headline + \" \" + news_article]\n",
    "\n",
    "#     # Vectorize the test instance using the same TF-IDF vectorizer trained on the training data\n",
    "#     X_test_instance_tfidf = tfidf.transform(X_test_instance)\n",
    "    \n",
    "#     # Make predictions for the test instance\n",
    "#     y_pred_instance = classifier.predict(X_test_instance_tfidf)\n",
    "    \n",
    "#     y_pred_proba = classifier.predict_proba(X_test_instance_tfidf)\n",
    "#     positive_class_proba = y_pred_proba\n",
    "#     overall_score = (positive_class_proba[0][0] * 0.2) + (positive_class_proba[0][1] * 1) + (positive_class_proba[0][2] * 0.4) + (positive_class_proba[0][3] * 0.6) + (positive_class_proba[0][4] * 0.8) + (positive_class_proba[0][5] * 0.0)\n",
    "#     lohit_ngram_prediction = predict_label(overall_score)\n",
    "    \n",
    "#     truth_scores = predict_tabular_classification_sample(project=\"dsc-180a-b09\",\n",
    "#                                                          endpoint_id=\"4607809140427849728\",\n",
    "#                                                          instance_dict={\"article\": news})\n",
    "#     reordered_indices = [truth_scores[0]['classes'].index(c) for c in ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']]\n",
    "#     classes_reordered = [truth_scores[0]['classes'][i] for i in reordered_indices]\n",
    "#     scores_reordered = [truth_scores[0]['scores'][i] for i in reordered_indices]\n",
    "#     final_automl_score = (scores_reordered[0] * 0) + (scores_reordered[1] * 0.2) + (scores_reordered[2] * 0.4) + (scores_reordered[3] * 0.6) + (scores_reordered[4] * 0.8) + (scores_reordered[5] * 1)\n",
    "#     auto_ml_score_label = predict_label(final_automl_score)\n",
    "#     final_pred = (overall_score + final_automl_score) / 2\n",
    "    \n",
    "#     lohit_final_prediction = predict_label(final_pred)\n",
    "\n",
    "\n",
    "#     #NICKS PREDICTIONS\n",
    "#     # #readability\n",
    "#     # r = Readability(news_article)\n",
    "#     # fk = r.flesch_kincaid()\n",
    "#     # flesch_score = fk.score\n",
    "#     # if flesch_score > 12:\n",
    "#     #     diff = flesch_score - 12\n",
    "#     #     fk_rating = 100 - (diff * 10)\n",
    "#     # elif flesch_score < 8:\n",
    "#     #     diff = 8 - flesch_score\n",
    "#     #     fk_rating = 100 - (diff * 10)\n",
    "#     # else:\n",
    "#     #     fk_rating = 100\n",
    "\n",
    "#     #sentiment\n",
    "#     sia = SentimentIntensityAnalyzer()\n",
    "#     moving_sentiment_value = 0\n",
    "#     number_of_paragraphs = 0\n",
    "#     paragraphs = news_article.split('\\n\\n')\n",
    "#     for i in paragraphs:\n",
    "#         cleaned_text = ' '.join(i.split()).replace(\"\\'\", '')\n",
    "#         compound_sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
    "#         moving_sentiment_value += compound_sentiment_score\n",
    "#         number_of_paragraphs += 1\n",
    "#     overall_sentiment = moving_sentiment_value / number_of_paragraphs\n",
    "#     overall_sent_score = 100\n",
    "#     if overall_sentiment < -0.2:\n",
    "#         overall_sent_score = 100 + (overall_sentiment * 100)\n",
    "\n",
    "#     #clickbait\n",
    "#     if len(headline) > 0:\n",
    "#         article_title_processed = preprocess_text(headline)\n",
    "#         article_title_vectorized = count_vectorizer.transform([article_title_processed])\n",
    "#         clickbait_probability = clf_best.predict_proba(article_title_vectorized)\n",
    "#         confidence_not_clickbait = clickbait_probability[:, 0]\n",
    "#         confidence_not_clickbait = confidence_not_clickbait[0]\n",
    "#         nick_predicted_label = predict_label(confidence_not_clickbait)\n",
    "#     else:\n",
    "#         confidence_not_clickbait = 0\n",
    "\n",
    "#     if confidence_not_clickbait == 0:\n",
    "#         factors_combined = (overall_sent_score)\n",
    "#         nicks_predicted_label = predict_label(factors_combined)\n",
    "#     else:\n",
    "#         factors_combined = ((confidence_not_clickbait * 100) + overall_sent_score) / 2\n",
    "#         nicks_predicted_label = predict_label(factors_combined)\n",
    "\n",
    "#     #HENRYS PREDICTIONS\n",
    "#     # Tokenize the single text example\n",
    "#     tokenized_news = tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "#     # Use GPU if available\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model_henry = model_.to(device)\n",
    "    \n",
    "#     # Move tokenized example to GPU\n",
    "#     tokenized_news_gpu = tokenized_news.to(device)\n",
    "    \n",
    "#     # Extract BERT embeddings for the tokenized example\n",
    "#     with torch.no_grad():\n",
    "#         model_henry.eval()\n",
    "#         statement_embedding = model_henry(**tokenized_news_gpu).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    \n",
    "#     # Use the RandomForestClassifier to predict the label for the single text example\n",
    "#     y_pred_news = rf_classifier.predict(statement_embedding)\n",
    "    \n",
    "#     henry_final_prediction = y_pred_news[0]\n",
    "\n",
    "#     #combining all group mates predictive scores\n",
    "#     all_ratings = [irisa_final_label_pred,lohit_final_prediction,nicks_predicted_label,henry_final_prediction]\n",
    "#     labels_as_scores = []\n",
    "#     for i in all_ratings:\n",
    "#         labels_as_scores.append(label_to_score(i))\n",
    "#     pred_models_ave = sum(labels_as_scores) / len(labels_as_scores)\n",
    "#     final_pred_label = score_to_label(pred_models_ave)\n",
    "\n",
    "#     # if len(headline) > 0:\n",
    "#     #     pred_score_output = f\"\"\"The overall score created from our predictive models is {pred_models_ave}. This means the article has been evaluated to be {final_pred_label}. The individual scores of the predictive models are as follows.\n",
    "#     #     Full-text n-gram analysis: {lohit_ngram_prediction}\n",
    "#     #     Full-text BERT embedding prediction: {henry_final_prediction}\n",
    "#     #     Google AUTO ML full-text analysis: {auto_ml_score_label}\n",
    "#     #     Readability Score: {round(fk_rating,2)}\n",
    "#     #     Not Clickbait Probability: {round(confidence_not_clickbait*100,2)}%\n",
    "#     #     Quality of Writing Percentile: {round(ttr_percentile,2)}%\n",
    "#     #     Sensationalism Score Percentile: {round(adjectives_percentile,2)}%\n",
    "#     #     Sentiment Score Percentile: {round(sentiment_percentile,2)}%\"\"\"\n",
    "#     # else:\n",
    "#     #     pred_score_output = f\"\"\"The overall score created from our predictive models is {pred_models_ave}. This means the article has been \n",
    "#     #     evaluated to be {final_pred_label}. The individual scores of the predictive models are as follows.\n",
    "#     #     Full-text n-gram analysis: {lohit_ngram_prediction}\n",
    "#     #     Full-text BERT embedding prediction: {henry_final_prediction}\n",
    "#     #     Google AUTO ML full-text analysis: {auto_ml_score_label}\n",
    "#     #     Readability Score: {round(fk_rating,2)}\n",
    "#     #     Quality of Writing Percentile: {round(ttr_percentile,2)}\n",
    "#     #     Sensationalism Score Percentile: {round(adjectives_percentile,2)}\n",
    "#     #     Sentiment Score Percentile: {round(sentiment_percentile,2)}\"\"\"\n",
    "        \n",
    "#     #Pre-processed examples output\n",
    "#     if example_news_provider == \"ABC\":\n",
    "#         history_output.append([news_article, abc_final_output])\n",
    "#         return history_output, history_output, pred_score_output, overall_synopsis\n",
    "#     elif example_news_provider == \"NPR\":\n",
    "#         history_output.append([news_article, npr_final_output])\n",
    "#         return history_output, history_output, pred_score_output, overall_synopsis\n",
    "#     elif example_news_provider == \"FOX\":\n",
    "#         history_output.append([news_article, fox_final_output])\n",
    "#         return history_output, history_output, pred_score_output, overall_synopsis\n",
    "        \n",
    "    \n",
    "#     #GEN AI\n",
    "#     #instantiating gemini pro model\n",
    "#     PROJECT_ID = \"gen-lang-client-0321728687\"\n",
    "#     REGION = \"us-central1\"\n",
    "#     vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "#     model = generative_models.GenerativeModel(\"gemini-pro\")\n",
    "#     config = {\"max_output_tokens\": 2048, \"temperature\": 0.0}\n",
    "    \n",
    "#     safety_config = {\n",
    "#     generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "#     generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "#     generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "#     generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "#     }\n",
    "#     chat = model.start_chat()\n",
    "\n",
    "#     #PerspectiveAPI output check instantiation\n",
    "#     client = discovery.build(\n",
    "#       \"commentanalyzer\",\n",
    "#       \"v1alpha1\",\n",
    "#       developerKey=PERSPECTIVE_API_KEY,\n",
    "#       discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "#       static_discovery=False,\n",
    "#         )\n",
    "    \n",
    "#     #chunking news article for improved processing\n",
    "#     chunked_article_list = tokenize_into_chunks(news_article, 50)\n",
    "    \n",
    "#     #getting context and fact checks from vector database based on the provided input\n",
    "#     all_response_text = []\n",
    "#     context_list = []\n",
    "#     for i in range(len(chunked_article_list)):\n",
    "#         input = chunked_article_list[i]\n",
    "#         context = RAG_CONTEXT_VDB.query(\n",
    "#             query_texts=[input],\n",
    "#             n_results=7,\n",
    "#         )\n",
    "#         context_list.append(context)\n",
    "        \n",
    "#     fact_checks_list=[]\n",
    "#     for i in range(len(chunked_article_list)):\n",
    "#         input = chunked_article_list[i]\n",
    "#         fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "#             query_texts=[input],\n",
    "#             n_results=7,\n",
    "#         )\n",
    "#         fact_checks_list.append(fact_checks)\n",
    "\n",
    "#     #creating history list so that gen ai model has additional context when analyzing chunked statements \n",
    "#     for i in range(len(context_list)):\n",
    "#         input=chunked_article_list[i]\n",
    "#         fact_checks = fact_checks_list[i]\n",
    "#         context = context_list[i]\n",
    "#         prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "#         next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "        \n",
    "#         history = [prev_chunk, input, next_chunk]\n",
    "        \n",
    "#         #re-ranking RAG results for fact check statements from RAG_STATEMENTS_VDB\n",
    "#         statement_rerank_list = []\n",
    "#         for j in range(len(fact_checks['ids'][0])):\n",
    "#             reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "#             statement_rerank_list.append(reranking_statementSearch)\n",
    "    \n",
    "        \n",
    "#         scores = reranker.compute_score(statement_rerank_list)\n",
    "#         combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "#         sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "#         sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "#         #re-ranking RAG results for context statements from RAG_CONTEXT_VDB\n",
    "#         context_rerank_list = []\n",
    "#         for k in range(len(context['ids'][0])):\n",
    "#             reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "#             context_rerank_list.append(reranking_contextSearch)\n",
    "            \n",
    "#         scores = reranker.compute_score(context_rerank_list)\n",
    "#         combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "#         sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "#         sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "\n",
    "#         #getting top 3 most relevant pieces of context and fact checks from RAG\n",
    "#         context_window = 3\n",
    "#         prepared_context = []\n",
    "#         prepared_fact_checks = []\n",
    "#         for i in range(context_window):\n",
    "#             prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "#             prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])\n",
    "\n",
    "#         #Changing chunks from list of strings to one combined string for Gen AI processing\n",
    "#         chunk_history_string = ''\n",
    "#         for chunk in history:\n",
    "#             if chunk != None:\n",
    "#                 chunk_history_string += chunk + \" \"\n",
    "\n",
    "\n",
    "#         #generating initial response with prompt template\n",
    "#         responses = model.generate_content(f\"\"\"Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "#         knowledge I have provided you below, as well as information you already have access to answer the question.\n",
    "#         Use the additional information I've provided below within the ((())) symbols to help you. \n",
    "#         (((\n",
    "#         Refer to these fact-checked statements to determine your answer and be sure to pay close attention to the \n",
    "#         metadata that is provided: {prepared_fact_checks}.\n",
    "#         Also use the following context to help answer the question: {prepared_context}.\n",
    "#         You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "#         Ensure that you use all this information and think about this question step-by-step using the provided information.\n",
    "#         )))\n",
    "#         <<<\n",
    "#         Question: How true is the following statement? + {input}. You must provide your rating by giving back just a single label from\n",
    "#         the following list that is listed in increasing levels of truthfulness. The labels are as follows. \n",
    "#         ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']. Your response must match the format provided exactly.\n",
    "#         >>>\n",
    "#        \"\"\",\n",
    "#             generation_config=config,\n",
    "#             stream=True,\n",
    "#             safety_settings=safety_config,                          \n",
    "#         )\n",
    "        \n",
    "#         #obtaining individual responses\n",
    "#         response_text = \"\"\n",
    "#         #response_text += \"Statement: \" + input\n",
    "#         for response in responses:\n",
    "#             try:\n",
    "#                 response_text += response.text\n",
    "#             except (IndexError, ValueError) as e:\n",
    "#                 continue\n",
    "#         response_text = response_text.replace(\"\\n\\n\", \". \")\n",
    "#         all_response_text.append(response_text)\n",
    "        \n",
    "                                  \n",
    "#     #combining all responses    \n",
    "#     entire_text_string = \"\"\n",
    "#     for text in all_response_text:\n",
    "#         entire_text_string += text\n",
    "#     cleaned_text = entire_text_string\n",
    "    \n",
    "#     #this section is finding and removing the statements that can't be rated by the chatbot\n",
    "#     unratable_sentences = []\n",
    "#     rated_sentences = []\n",
    "\n",
    "#     final_text = \"\"\n",
    "#     for response in all_response_text:\n",
    "#         if \"article does not\" in response.lower() or \"context does not\" in response.lower() or \"statement is not\" in response.lower() or \"Statement: Score\" in response:\n",
    "#             unratable_sentences.append(response)\n",
    "#         else:\n",
    "#             rated_sentences.append(response)\n",
    "#             final_text += response\n",
    "    \n",
    "#     not_enough_context = len(unratable_sentences)\n",
    "#     enough_context = len(rated_sentences)\n",
    "#     all_statements_count = len(all_response_text)\n",
    "\n",
    "\n",
    "#     #total score calculation with regex\n",
    "#     pattern = r'Score:\\s(\\d+)\\.'\n",
    "#     total_score = 0\n",
    "#     matches = re.findall(pattern, cleaned_text)\n",
    "#     for match in matches:\n",
    "#         score = int(match)\n",
    "#         total_score += score\n",
    "#     if enough_context == 0:\n",
    "#         average_score = 0\n",
    "#     else:\n",
    "#         average_score = total_score / enough_context\n",
    "#     rounded_average = round(average_score, 1)\n",
    "\n",
    "#     # #creating output in nice format for user\n",
    "#     # output_intro = f\"\"\"{enough_context} out of {all_statements_count} statements in the text could be rated. The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {rounded_average}/100. Some of the lowest rated statements are provided below.\"\"\"\n",
    "#     # tweaking_output = re.sub(r'(Score:\\s*\\d+\\.)(?!\\s*Explanation:)', r'\\1 Explanation:', final_text)\n",
    "#     # parts = re.split(r\"(?=Statement:)\", tweaking_output)\n",
    "#     # split_parts=[]\n",
    "#     # # Clean each part and add to split_parts\n",
    "#     # for part in parts:\n",
    "#     #     cleaned_part = output_clean(part)\n",
    "#     #     split_parts.append(cleaned_part)\n",
    "    \n",
    "#     # # Initialize variables to store lowest scores and their respective entries\n",
    "#     # lowest_scores = [(float('inf'), ''), (float('inf'), ''), (float('inf'), '')]\n",
    "    \n",
    "#     # # Iterate through each string entry\n",
    "#     # for entry in split_parts:\n",
    "#     #     # Find all occurrences of \"Score: \" followed by a number until a \".\"\n",
    "#     #     scores = re.findall(r' Score:\\s(\\d+)\\.', entry)\n",
    "#     #     # Convert scores to integers and update lowest_scores if necessary\n",
    "#     #     for score in scores:\n",
    "#     #         score_int = int(score)\n",
    "#     #         if score_int < lowest_scores[-1][0]:\n",
    "#     #             lowest_scores[-1] = (score_int, entry)\n",
    "#     #             lowest_scores.sort()\n",
    "                \n",
    "#     # # Extract the entries for the three lowest scores\n",
    "#     # lowest_entries = [entry for score, entry in lowest_scores]\n",
    "\n",
    "#     # #reformatting for better readability\n",
    "#     # summary_output = \"\"\n",
    "#     # for statement in lowest_entries:\n",
    "#     #     # Replace \"Statement:\", \"Score:\", and \"Explanation:\" with a new line followed by the keyword\n",
    "#     #     formatted_statement = re.sub(r'(Statement:|Score:|Explanation:)', r'\\n\\1', statement)\n",
    "#     #     # Append the formatted statement to the output\n",
    "#     #     summary_output += formatted_statement.strip() + \"\\n\"\n",
    "    \n",
    "#     #     # Add a new line after each statement\n",
    "#     #     summary_output += \"\\n\"\n",
    "\n",
    "#     # output = output_intro + \"\\n\\n\" + summary_output\n",
    "\n",
    "#     # #Perspective API output safety check\n",
    "#     # analyze_request = {\n",
    "#     #   'comment': { 'text': output},\n",
    "#     #   'requestedAttributes': requestedAttributes\n",
    "#     # }\n",
    "#     # response = client.comments().analyze(body=analyze_request).execute()\n",
    "    \n",
    "#     # attributes_surpassed = []\n",
    "#     # for key in response['attributeScores']:\n",
    "#     #     if response['attributeScores'][key]['summaryScore']['value'] > attributeThresholds[key]:\n",
    "#     #         attributes_surpassed.append((key, response['attributeScores'][key]['summaryScore']['value']))\n",
    "    \n",
    "#     # #crafting output warning message if necessary or regular output message  \n",
    "    \n",
    "#     # if len(attributes_surpassed) == 1:\n",
    "#     #     attributes_violated = \"\"\n",
    "#     #     for i in attributes_surpassed:\n",
    "#     #         attributes_violated += i[0] + \" \"\n",
    "#     #     warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}category so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "#     #     history_output.append([news_article, warning_message])\n",
    "        \n",
    "#     # elif len(attributes_surpassed) > 1:\n",
    "#     #     attributes_violated = \"\"\n",
    "#     #     counter = 1\n",
    "#     #     attributes_count = len(attributes_surpassed)\n",
    "#     #     for i in attributes_surpassed:\n",
    "#     #         attributes_violated += i[0] + \" \"\n",
    "#     #         if counter < attributes_count:\n",
    "#     #             attributes_violated += \"and \"\n",
    "#     #         counter += 1\n",
    "#     #     warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}categories so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "#     #     history_output.append([news_article, warning_message])\n",
    "\n",
    "#     # else:\n",
    "#     overall_synopsis = \"overall analysis\"\n",
    "#     history_output.append([news_article, all_response_text])\n",
    "#     return history_output, history_output, final_pred_label, overall_synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6d52eb5a-93b3-4b58-8060-8c878ad4c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_liar_plus = pd.read_csv(\"Data/Liar_plus/train.tsv\", delimiter='\\t', header=None)\n",
    "liar_liar_plus = liar_liar_plus[[3, 2]]\n",
    "liar_liar_plus.dropna(inplace=True)\n",
    "llp_statements = liar_liar_plus[3].sample(n=500, random_state=42)\n",
    "llp_labels = liar_liar_plus[2].sample(n=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d8e76cef-2794-4659-8c98-02655571549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "ename": "ServiceUnavailable",
     "evalue": "503 The service is currently unavailable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:173\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     prefetch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callable_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_prefetch_first_result_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_StreamingResponseIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefetch_first_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefetch_first\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:95\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__init__\u001b[0;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefetch_first_result:\n\u001b[0;32m---> 95\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stored_first_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# for instance). If this happens don't store the first result.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:540\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/grpc/_channel.py:966\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"The service is currently unavailable.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:172.217.12.138:443 {grpc_message:\"The service is currently unavailable.\", grpc_status:14, created_time:\"2024-03-09T17:39:21.003647-08:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m llp_statements:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         model_load \u001b[38;5;241m=\u001b[39m \u001b[43mGenAI_article_truth_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m         gen_score \u001b[38;5;241m=\u001b[39m label_to_score(model_load[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m      6\u001b[0m         pred_score \u001b[38;5;241m=\u001b[39m label_to_score(model_load[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n",
      "Cell \u001b[0;32mIn[178], line 316\u001b[0m, in \u001b[0;36mGenAI_article_truth_processing\u001b[0;34m(news_article, history, examples, headline)\u001b[0m\n\u001b[1;32m    314\u001b[0m response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m#response_text += \"Statement: \" + input\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:520\u001b[0m, in \u001b[0;36m_GenerativeModel._generate_content_streaming\u001b[0;34m(self, contents, generation_config, safety_settings, tools)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    A stream of GenerationResponse objects\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    514\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_request(\n\u001b[1;32m    515\u001b[0m     contents\u001b[38;5;241m=\u001b[39mcontents,\n\u001b[1;32m    516\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    517\u001b[0m     safety_settings\u001b[38;5;241m=\u001b[39msafety_settings,\n\u001b[1;32m    518\u001b[0m     tools\u001b[38;5;241m=\u001b[39mtools,\n\u001b[1;32m    519\u001b[0m )\n\u001b[0;32m--> 520\u001b[0m response_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prediction_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response_stream:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_response(chunk)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/cloud/aiplatform_v1beta1/services/prediction_service/client.py:1634\u001b[0m, in \u001b[0;36mPredictionServiceClient.stream_generate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1629\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1630\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmodel),)),\n\u001b[1;32m   1631\u001b[0m )\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1634\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/api_core/grpc_helpers.py:177\u001b[0m, in \u001b[0;36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StreamingResponseIterator(\n\u001b[1;32m    174\u001b[0m         result, prefetch_first_result\u001b[38;5;241m=\u001b[39mprefetch_first\n\u001b[1;32m    175\u001b[0m     )\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: 503 The service is currently unavailable."
     ]
    }
   ],
   "source": [
    "label_prediction = []\n",
    "for i in llp_statements:\n",
    "    try:\n",
    "        model_load = GenAI_article_truth_processing(i,[], [], [])\n",
    "        gen_score = label_to_score(model_load[0][0][1][0]) * 0.8\n",
    "        pred_score = label_to_score(model_load[2]) * 0.2\n",
    "        overall_score = (gen_score + pred_score)\n",
    "        final_label = score_to_label(overall_score)\n",
    "        label_prediction.append(final_label)\n",
    "    except (ValueError, IndexError, KeyError) as e:\n",
    "        label_prediction.append([model_load[0][0][1][0], model_load[2]])\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5ee87702-6322-46fb-be96-8d306039fbc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c41fc598-47eb-4d14-97bc-92b304b413f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = llp_labels.tolist()\n",
    "len(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "829e266f-fe10-4716-8ad2-903455dbaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_same_values = 0\n",
    "\n",
    "for value1, value2 in zip(label_prediction, labels_list):\n",
    "    if value1 == value2:\n",
    "        count_same_values += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bafb046e-735b-4c7c-b667-5ecc2b9cb274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241206030150754"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_same_values / (len(label_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437b56d-bb04-4331-907f-d611f7030277",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### GEN AI Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "34385bfe-546f-4a04-9777-b437011329c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenAI_article_truth_processing(news_article, history, examples, headline):\n",
    "    history_output = []\n",
    "\n",
    "        \n",
    "    #instantiating RAG re-ranking mecahnism\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "    #converting full news article to string\n",
    "    news_article = f\"\"\"{news_article}\"\"\"\n",
    "    example_news_provider = f\"\"\"{news_article}\"\"\"\n",
    "    headline = f\"\"\"{headline}\"\"\"\n",
    "\n",
    "    #setting up pre-processed examples correctly\n",
    "    if news_article == \"ABC\" or news_article == \"abc\" or news_article == \"Abc\":\n",
    "        news_article = abc_content\n",
    "        headline = abc_headline\n",
    "    elif news_article == \"NPR\" or news_article == \"npr\" or news_article == \"Npr\":\n",
    "        news_article = npr_content\n",
    "        headline = npr_headline\n",
    "    elif news_article == \"FOX\" or news_article == \"Fox\" or news_article == \"fox\":\n",
    "        news_article = fox_content\n",
    "        headline = fox_headline\n",
    "    overall_synopsis = \"Overall Score\"\n",
    "        \n",
    "    #getting history for context\n",
    "    history = history or []\n",
    "\n",
    "    #predictive models\n",
    "    #IRISAS PREDICTIONS\n",
    "\n",
    "    #sentiment prediction\n",
    "    irisa_sentiment = analyzer.polarity_scores(news_article)[\"compound\"]\n",
    "    \n",
    "    #quality of writing prediction\n",
    "    words = news_article.split()\n",
    "    irisa_qor_ratio = len(set(words)) / len(words)\n",
    "    \n",
    "    #sensationalism\n",
    "    irisa_sensationalism = count_adjectives(news_article)\n",
    "    \n",
    "    #adding to df for prediction\n",
    "    irisa_data = {\n",
    "        \"sentiment\": [irisa_sentiment],\n",
    "        \"ttr\": [irisa_qor_ratio],\n",
    "        \"adjectives\": [irisa_sensationalism]\n",
    "    }\n",
    "    \n",
    "    irisa_pred_df = pd.DataFrame(irisa_data)\n",
    "    \n",
    "    #irisa final prediction\n",
    "    irisa_final_prediction = irisa_clf.predict(irisa_pred_df)[0]\n",
    "    irisa_final_label_pred = number_to_label(irisa_final_prediction)\n",
    "\n",
    "    #irisas prediction percentile\n",
    "    sentiment_percentile = percentileofscore(irisa_X_train['sentiment'], irisa_pred_df['sentiment'])[0]\n",
    "    ttr_percentile = percentileofscore(irisa_X_train['ttr'], irisa_pred_df['ttr'])[0]\n",
    "    adjectives_percentile = percentileofscore(irisa_X_train['adjectives'], irisa_pred_df['adjectives'])[0]\n",
    "\n",
    "    #LOHITS PREDICTIONS\n",
    "    X_test_instance = [headline + \" \" + news_article]\n",
    "\n",
    "    # Vectorize the test instance using the same TF-IDF vectorizer trained on the training data\n",
    "    X_test_instance_tfidf = tfidf.transform(X_test_instance)\n",
    "    \n",
    "    # Make predictions for the test instance\n",
    "    y_pred_instance = classifier.predict(X_test_instance_tfidf)\n",
    "    \n",
    "    y_pred_proba = classifier.predict_proba(X_test_instance_tfidf)\n",
    "    positive_class_proba = y_pred_proba\n",
    "    overall_score = (positive_class_proba[0][0] * 0.2) + (positive_class_proba[0][1] * 1) + (positive_class_proba[0][2] * 0.4) + (positive_class_proba[0][3] * 0.6) + (positive_class_proba[0][4] * 0.8) + (positive_class_proba[0][5] * 0.0)\n",
    "    lohit_ngram_prediction = predict_label(overall_score)\n",
    "    \n",
    "    truth_scores = predict_tabular_classification_sample(project=\"dsc-180a-b09\",\n",
    "                                                         endpoint_id=\"4607809140427849728\",\n",
    "                                                         instance_dict={\"article\": news_article})\n",
    "    reordered_indices = [truth_scores[0]['classes'].index(c) for c in ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']]\n",
    "    classes_reordered = [truth_scores[0]['classes'][i] for i in reordered_indices]\n",
    "    scores_reordered = [truth_scores[0]['scores'][i] for i in reordered_indices]\n",
    "\n",
    "    scaling_dict = {\"pants-fire\": 0, \"false\": 0.2, \"barely-true\": 0.4, \"half-true\": 0.6, \"mostly-true\": 0.8, \"true\": 1}\n",
    "    zipped_automl = zip(classes_reordered, scores_reordered)\n",
    "    zipped_list = list(zipped_automl)\n",
    "    \n",
    "    #automl is a list of tuples with the label/confidence score\n",
    "    final_automl_score = scale_and_combine(zipped_list)\n",
    "    auto_ml_score_label = predict_label(final_automl_score)\n",
    "    lohit_final_prediction = predict_label(final_automl_score)\n",
    "\n",
    "\n",
    "    #NICKS PREDICTIONS\n",
    "    #readability\n",
    "    r = Readability(news_article)\n",
    "    fk = r.flesch_kincaid()\n",
    "    flesch_score = fk.score\n",
    "    if flesch_score > 12:\n",
    "        diff = flesch_score - 12\n",
    "        fk_rating = 100 - (diff * 5)\n",
    "    elif flesch_score < 8:\n",
    "        diff = 8 - flesch_score\n",
    "        fk_rating = 100 - (diff * 5)\n",
    "    else:\n",
    "        fk_rating = 100\n",
    "\n",
    "    #sentiment\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    moving_sentiment_value = 0\n",
    "    number_of_paragraphs = 0\n",
    "    paragraphs = news_article.split('\\n\\n')\n",
    "    for i in paragraphs:\n",
    "        cleaned_text = ' '.join(i.split()).replace(\"\\'\", '')\n",
    "        compound_sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
    "        moving_sentiment_value += compound_sentiment_score\n",
    "        number_of_paragraphs += 1\n",
    "    overall_sentiment = moving_sentiment_value / number_of_paragraphs\n",
    "    overall_sent_score = 100\n",
    "    if overall_sentiment < -0.2:\n",
    "        overall_sent_score = 100 + (overall_sentiment * 100)\n",
    "\n",
    "    #clickbait\n",
    "    if len(headline) > 0:\n",
    "        article_title_processed = preprocess_text(headline)\n",
    "        article_title_vectorized = count_vectorizer.transform([article_title_processed])\n",
    "        clickbait_probability = clf_best.predict_proba(article_title_vectorized)\n",
    "        confidence_not_clickbait = clickbait_probability[:, 0]\n",
    "        confidence_not_clickbait = confidence_not_clickbait[0]\n",
    "        nick_predicted_label = predict_label(confidence_not_clickbait)\n",
    "    else:\n",
    "        confidence_not_clickbait = 0\n",
    "\n",
    "    if confidence_not_clickbait == 0:\n",
    "        factors_combined = (overall_sent_score + fk_rating) / 2\n",
    "        nicks_predicted_label = predict_label(factors_combined)\n",
    "    else:\n",
    "        factors_combined = ((confidence_not_clickbait * 100) + overall_sent_score + fk_rating) / 3\n",
    "        nicks_predicted_label = predict_label(factors_combined)\n",
    "\n",
    "    #HENRYS PREDICTIONS\n",
    "    # Tokenize the single text example\n",
    "    tokenized_news = tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_henry = model_.to(device)\n",
    "    \n",
    "    # Move tokenized example to GPU\n",
    "    tokenized_news_gpu = tokenized_news.to(device)\n",
    "    \n",
    "    # Extract BERT embeddings for the tokenized example\n",
    "    with torch.no_grad():\n",
    "        model_henry.eval()\n",
    "        statement_embedding = model_henry(**tokenized_news_gpu).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Use the RandomForestClassifier to predict the label for the single text example\n",
    "    y_pred_news = rf_classifier.predict(statement_embedding)\n",
    "    \n",
    "    henry_final_prediction = y_pred_news[0]\n",
    "\n",
    "    #combining all group mates predictive scores\n",
    "    all_ratings = [irisa_final_label_pred,lohit_final_prediction,nicks_predicted_label,henry_final_prediction]\n",
    "    labels_as_scores = []\n",
    "    for i in all_ratings:\n",
    "        labels_as_scores.append(label_to_score(i))\n",
    "    pred_models_ave = sum(labels_as_scores) / len(labels_as_scores)\n",
    "    final_pred_label = score_to_label(pred_models_ave)\n",
    "\n",
    "    if len(headline) > 0:\n",
    "        pred_score_output = f\"\"\"The overall score created from our predictive models is {pred_models_ave}. This means the article has been evaluated to be {final_pred_label}. The individual scores of the predictive models are as follows.\n",
    "        Full-text n-gram analysis: {lohit_ngram_prediction}\n",
    "        Full-text BERT embedding prediction: {henry_final_prediction}\n",
    "        Google AUTO ML full-text analysis: {auto_ml_score_label}\n",
    "        Readability Score: {round(fk_rating,2)}\n",
    "        Not Clickbait Probability: {round(confidence_not_clickbait*100,2)}%\n",
    "        Quality of Writing Percentile: {round(ttr_percentile,2)}%\n",
    "        Sensationalism Score Percentile: {round(adjectives_percentile,2)}%\n",
    "        Sentiment Score Percentile: {round(sentiment_percentile,2)}%\"\"\"\n",
    "    else:\n",
    "        pred_score_output = f\"\"\"The overall score created from our predictive models is {pred_models_ave}. This means the article has been \n",
    "        evaluated to be {final_pred_label}. The individual scores of the predictive models are as follows.\n",
    "        Full-text n-gram analysis: {lohit_ngram_prediction}\n",
    "        Full-text BERT embedding prediction: {henry_final_prediction}\n",
    "        Google AUTO ML full-text analysis: {auto_ml_score_label}\n",
    "        Readability Score: {round(fk_rating,2)}\n",
    "        Quality of Writing Percentile: {round(ttr_percentile,2)}\n",
    "        Sensationalism Score Percentile: {round(adjectives_percentile,2)}\n",
    "        Sentiment Score Percentile: {round(sentiment_percentile,2)}\"\"\"\n",
    "        \n",
    "    #Pre-processed examples output\n",
    "    if example_news_provider == \"ABC\" or example_news_provider == \"ABC\" or example_news_provider == \"Abc\":\n",
    "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\/100', abc_final_output)\n",
    "        score = float(match.group(1))\n",
    "        overall_score = (pred_models_ave * 0.2) + (score * 0.8)\n",
    "        overall_score = round(overall_score, 2)\n",
    "        if overall_score < 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall pants-on-fire rating. Most of, or all of the information in this article is falsified or lacks huge amounts of context and is very misleading. This article should be read with extreme caution. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 33.333 and overall_score > 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall false rating. Most of the information in this article is falsified or lacks context and therefore can be quite misleading. This article should be read with very much caution as much of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 50 and overall_score > 33.333:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall barely-true rating. A majority of the information in this article is falsified or lacks context which could make it misleading. This article should be read with caution as over half of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 66.666 and overall_score > 50:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall half-true rating. This news article contains both truthful and false or misleading content. This article should be read with caution as some of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 83.333 and overall_score > 66.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall mostly-true rating. This news article contains mostly truthful information, but still contains some false or misleading content. This article should be read with caution as a small set of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"   \n",
    "        else:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall true rating. This news article contains almost entirely truthful information. This article contains highly accurate and non-misleading information and can be reliably trusted. Still use caution though as not every statement here may be completely true. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        history_output.append([news_article, abc_final_output])\n",
    "        return history_output, history_output, pred_score_output, overall_synopsis\n",
    "    elif example_news_provider == \"NPR\" or example_news_provider == \"NPR\" or example_news_provider == \"Npr\":\n",
    "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\/100', npr_final_output)\n",
    "        score = float(match.group(1))\n",
    "        overall_score = (pred_models_ave * 0.2) + (score * 0.8)\n",
    "        overall_score = round(overall_score, 2)\n",
    "        if overall_score < 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall pants-on-fire rating. Most of, or all of the information in this article is falsified or lacks huge amounts of context and is very misleading. This article should be read with extreme caution. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 33.333 and overall_score > 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall false rating. Most of the information in this article is falsified or lacks context and therefore can be quite misleading. This article should be read with very much caution as much of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 50 and overall_score > 33.333:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall barely-true rating. A majority of the information in this article is falsified or lacks context which could make it misleading. This article should be read with caution as over half of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 66.666 and overall_score > 50:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall half-true rating. This news article contains both truthful and false or misleading content. This article should be read with caution as some of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 83.333 and overall_score > 66.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall mostly-true rating. This news article contains mostly truthful information, but still contains some false or misleading content. This article should be read with caution as a small set of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"   \n",
    "        else:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall true rating. This news article contains almost entirely truthful information. This article contains highly accurate and non-misleading information and can be reliably trusted. Still use caution though as not every statement here may be completely true. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        history_output.append([news_article, npr_final_output])\n",
    "        return history_output, history_output, pred_score_output, overall_synopsis\n",
    "    elif example_news_provider == \"FOX\" or example_news_provider == \"fox\" or example_news_provider == \"Fox\":\n",
    "        match = re.search(r'(\\d+(?:\\.\\d+)?)\\/100', fox_final_output)\n",
    "        score = float(match.group(1))\n",
    "        overall_score = (pred_models_ave * 0.2) + (score * 0.8)\n",
    "        overall_score = round(overall_score, 2)\n",
    "        if overall_score < 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall pants-on-fire rating. Most of, or all of the information in this article is falsified or lacks huge amounts of context and is very misleading. This article should be read with extreme caution. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 33.333 and overall_score > 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall false rating. Most of the information in this article is falsified or lacks context and therefore can be quite misleading. This article should be read with very much caution as much of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 50 and overall_score > 33.333:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall barely-true rating. A majority of the information in this article is falsified or lacks context which could make it misleading. This article should be read with caution as over half of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 66.666 and overall_score > 50:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall half-true rating. This news article contains both truthful and false or misleading content. This article should be read with caution as some of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 83.333 and overall_score > 66.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall mostly-true rating. This news article contains mostly truthful information, but still contains some false or misleading content. This article should be read with caution as a small set of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"   \n",
    "        else:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall true rating. This news article contains almost entirely truthful information. This article contains highly accurate and non-misleading information and can be reliably trusted. Still use caution though as not every statement here may be completely true. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        history_output.append([news_article, fox_final_output])\n",
    "        return history_output, history_output, pred_score_output, overall_synopsis\n",
    "        \n",
    "    \n",
    "    #GEN AI\n",
    "    #instantiating gemini pro model\n",
    "    PROJECT_ID = \"gen-lang-client-0321728687\"\n",
    "    REGION = \"us-central1\"\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "    model = generative_models.GenerativeModel(\"gemini-pro\")\n",
    "    config = {\"max_output_tokens\": 2048, \"temperature\": 0.0}\n",
    "    \n",
    "    safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    }\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    #PerspectiveAPI output check instantiation\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=PERSPECTIVE_API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "        )\n",
    "    \n",
    "    #chunking news article for improved processing\n",
    "    chunked_article_list = tokenize_into_chunks(news_article, 50)\n",
    "    \n",
    "    #getting context and fact checks from vector database based on the provided input\n",
    "    all_response_text = []\n",
    "    context_list = []\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        context = RAG_CONTEXT_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        context_list.append(context)\n",
    "        \n",
    "    fact_checks_list=[]\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        fact_checks_list.append(fact_checks)\n",
    "\n",
    "    #creating history list so that gen ai model has additional context when analyzing chunked statements \n",
    "    for i in range(len(context_list)):\n",
    "        input=chunked_article_list[i]\n",
    "        fact_checks = fact_checks_list[i]\n",
    "        context = context_list[i]\n",
    "        \n",
    "        prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "        next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "        history = [prev_chunk, input, next_chunk]\n",
    "        \n",
    "        #re-ranking RAG results for fact check statements from RAG_STATEMENTS_VDB\n",
    "        statement_rerank_list = []\n",
    "        for j in range(len(fact_checks['ids'][0])):\n",
    "            reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "            statement_rerank_list.append(reranking_statementSearch)\n",
    "    \n",
    "        \n",
    "        scores = reranker.compute_score(statement_rerank_list)\n",
    "        combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "        #re-ranking RAG results for context statements from RAG_CONTEXT_VDB\n",
    "        context_rerank_list = []\n",
    "        for k in range(len(context['ids'][0])):\n",
    "            reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "            context_rerank_list.append(reranking_contextSearch)\n",
    "            \n",
    "        scores = reranker.compute_score(context_rerank_list)\n",
    "        combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "\n",
    "        #getting top 3 most relevant pieces of context and fact checks from RAG\n",
    "        context_window = 3\n",
    "        prepared_context = []\n",
    "        prepared_fact_checks = []\n",
    "        for i in range(context_window):\n",
    "            prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "            prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])\n",
    "\n",
    "        #Changing chunks from list of strings to one combined string for Gen AI processing\n",
    "        chunk_history_string = ''\n",
    "        for chunk in history:\n",
    "            if chunk != None:\n",
    "                chunk_history_string += chunk + \" \"\n",
    "\n",
    "       \n",
    "\n",
    "        responses = model.generate_content(f\"\"\" Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "        knowledge I have provided you below, as well as information you already have access to answer the question.\n",
    "        Use the additional information I've provided below within the ((())) symbols to help you. \n",
    "        (((\n",
    "        Refer to these fact checked statements as well to determine your answer and be sure to pay close attention to the \n",
    "        metadata that is provided: {prepared_fact_checks}.\n",
    "        Use the following context to help answer the question: {prepared_context}.\n",
    "        You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "        Ensure that you use all this information and think about this question step-by-step using the provided information.\n",
    "        Make sure you provide a short explanation of why you chose that score.\n",
    "        )))\n",
    "        <<<\n",
    "        Question: How true is the following statement on a scale of 1-100? + {input}. You must provide the score in this format Score:XX., \n",
    "        followed by your short explanation.\n",
    "        >>>\n",
    "       \"\"\",\n",
    "            generation_config=config,\n",
    "            stream=True,\n",
    "            safety_settings=safety_config,                          \n",
    "        )\n",
    "        \n",
    "        #generating initial response with prompt template\n",
    "       #  responses = model.generate_content(f\"\"\"Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "       #  knowledge you already have access to answer the question.\n",
    "    \n",
    "       #  If you are not very sure of your answer to the question, then use the additional information I've provided below within the \n",
    "       #  ((())) symbols to help you.\n",
    "       #  (((\n",
    "       #  Refer to these fact checked statements as well to determine your answer and be sure to pay close attention to the \n",
    "       #  metadata that is provided: {prepared_fact_checks}.\n",
    "       #  Use the following context to help answer the question: {prepared_context}.\n",
    "       #  You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "       #  Make sure you provide a short explanation of why you chose that score.\n",
    "       #  )))\n",
    "       #  <<<\n",
    "       #  Question: How true is the following statement on a scale of 1-100? + {input}. You must provide the score in this format Score:XX., \n",
    "       #  followed by your short explanation.\n",
    "       #  >>>\n",
    "       # \"\"\",\n",
    "       #      generation_config=config,\n",
    "       #      stream=True,\n",
    "       #      safety_settings=safety_config,                          \n",
    "       #  )\n",
    "\n",
    "        \n",
    "        #obtaining individual responses\n",
    "        response_text = \"\"\n",
    "        response_text += \"Statement: \" + input\n",
    "        for response in responses:\n",
    "            try:\n",
    "                response_text += response.text\n",
    "            except (IndexError, ValueError) as e:\n",
    "                continue\n",
    "        response_text = response_text.replace(\"\\n\\n\", \". \")\n",
    "        all_response_text.append(response_text)\n",
    "        \n",
    "    #combining all responses    \n",
    "    entire_text_string = \"\"\n",
    "    for text in all_response_text:\n",
    "        entire_text_string += text\n",
    "    cleaned_text = entire_text_string\n",
    "    \n",
    "    #this section is finding and removing the statements that can't be rated by the chatbot\n",
    "    unratable_sentences = []\n",
    "    rated_sentences = []\n",
    "\n",
    "    final_text = \"\"\n",
    "    for response in all_response_text:\n",
    "        if \"article does not\" in response.lower() or \"context does not\" in response.lower() or \"Statement: Score\" in response:\n",
    "            unratable_sentences.append(response)\n",
    "        else:\n",
    "            rated_sentences.append(response)\n",
    "            final_text += response\n",
    "    \n",
    "    not_enough_context = len(unratable_sentences)\n",
    "    enough_context = len(rated_sentences)\n",
    "    all_statements_count = len(all_response_text)\n",
    "\n",
    "\n",
    "    #total score calculation with regex\n",
    "    pattern = r'\\b\\s*Score:\\s*(\\d+)\\.'\n",
    "    total_score = 0\n",
    "    matches = re.findall(pattern, cleaned_text)\n",
    "    for match in matches:\n",
    "        score = int(match)\n",
    "        total_score += score\n",
    "    if enough_context == 0:\n",
    "        average_score = 0\n",
    "    else:\n",
    "        average_score = total_score / enough_context\n",
    "    rounded_average = round(average_score, 1)\n",
    "\n",
    "    #creating output in nice format for user\n",
    "    tweaking_output = re.sub(r'(Score:\\s*\\d+\\.)(?!\\s*Explanation:)', r'\\1 Explanation:', final_text)\n",
    "    parts = re.split(r\"(?=Statement:)\", tweaking_output)\n",
    "    split_parts=[]\n",
    "    # Clean each part and add to split_parts\n",
    "    for part in parts:\n",
    "        cleaned_part = output_clean(part)\n",
    "        split_parts.append(cleaned_part)\n",
    "    \n",
    "    # Initialize variables to store lowest scores and their respective entries\n",
    "    lowest_scores = [(float('inf'), ''), (float('inf'), ''), (float('inf'), '')]\n",
    "\n",
    "    # Iterate through each string entry\n",
    "    statements_rated=0\n",
    "    for entry in split_parts:\n",
    "        pattern = r'\\b\\s*Score:\\s*(\\d+)\\.'\n",
    "        # Find all occurrences of \"Score: \" followed by a number until a \".\"\n",
    "        scores = re.findall(pattern, entry)\n",
    "        # Convert scores to integers and update lowest_scores if necessary\n",
    "        for score in scores:\n",
    "            statements_rated+=1\n",
    "            score_int = int(score)\n",
    "            if score_int < lowest_scores[-1][0]:\n",
    "                lowest_scores[-1] = (score_int, entry)\n",
    "                lowest_scores.sort()    \n",
    "    average_score = total_score / statements_rated\n",
    "    rounded_average = round(average_score, 1)\n",
    "    \n",
    "    # Extract the entries for the three lowest scores\n",
    "    lowest_entries = [entry for score, entry in lowest_scores]\n",
    "\n",
    "    #reformatting for better readability\n",
    "    summary_output = \"\"\n",
    "    for statement in lowest_entries:\n",
    "        # Replace \"Statement:\", \"Score:\", and \"Explanation:\" with a new line followed by the keyword\n",
    "        formatted_statement = re.sub(r'(Statement:|Score:|Explanation:)', r'\\n\\1', statement)\n",
    "        # Append the formatted statement to the output\n",
    "        summary_output += formatted_statement.strip() + \"\\n\"\n",
    "    \n",
    "        # Add a new line after each statement\n",
    "        summary_output += \"\\n\"\n",
    "\n",
    "    output_intro = f\"\"\"{statements_rated} out of {all_statements_count} statements in the text could be rated. The following score and explanation is based on these {statements_rated} statements. The average truthfulness score from these {all_statements_count} statements is {rounded_average}/100. Some of the lowest rated statements are provided below.\"\"\"\n",
    "\n",
    "    output = output_intro + \"\\n\\n\" + summary_output\n",
    "\n",
    "    #Perspective API output safety check\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': output},\n",
    "      'requestedAttributes': requestedAttributes\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    \n",
    "    attributes_surpassed = []\n",
    "    for key in response['attributeScores']:\n",
    "        if response['attributeScores'][key]['summaryScore']['value'] > attributeThresholds[key]:\n",
    "            attributes_surpassed.append((key, response['attributeScores'][key]['summaryScore']['value']))\n",
    "    \n",
    "    #crafting output warning message if necessary or regular output message  \n",
    "    \n",
    "    if len(attributes_surpassed) == 1:\n",
    "        attributes_violated = \"\"\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}category so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "        \n",
    "    elif len(attributes_surpassed) > 1:\n",
    "        attributes_violated = \"\"\n",
    "        counter = 1\n",
    "        attributes_count = len(attributes_surpassed)\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "            if counter < attributes_count:\n",
    "                attributes_violated += \"and \"\n",
    "            counter += 1\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}categories so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "\n",
    "    else:\n",
    "        overall_score = (pred_models_ave * 0.2) + (rounded_average * 0.8)\n",
    "        overall_score = round(overall_score, 2)\n",
    "        if overall_score < 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall pants-on-fire rating. Most of, or all of the information in this article is falsified or lacks huge amounts of context and is very misleading. This article should be read with extreme caution. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 33.333 and overall_score > 16.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall false rating. Most of the information in this article is falsified or lacks context and therefore can be quite misleading. This article should be read with very much caution as much of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 50 and overall_score > 33.333:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall barely-true rating. A majority of the information in this article is falsified or lacks context which could make it misleading. This article should be read with caution as over half of the information here may be false. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 66.666 and overall_score > 50:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall half-true rating. This news article contains both truthful and false or misleading content. This article should be read with caution as some of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        elif overall_score < 83.333 and overall_score > 66.666:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall mostly-true rating. This news article contains mostly truthful information, but still contains some false or misleading content. This article should be read with caution as a small set of the information here may be false or lacking context. Please also be aware that potentially not all statements in the text could be rated.\"\"\"   \n",
    "        else:\n",
    "            overall_synopsis = f\"\"\"The overall truthfulness score after combining the Generative and Predictive AI results is {overall_score}/100. This means that the article has an overall true rating. This news article contains almost entirely truthful information. This article contains highly accurate and non-misleading information and can be reliably trusted. Still use caution though as not every statement here may be completely true. Please also be aware that potentially not all statements in the text could be rated.\"\"\"\n",
    "        history_output.append([news_article, output])\n",
    "    return history_output, history_output, pred_score_output, overall_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831f84b-ebdf-4f8b-aa42-e486321e1f86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Example article load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e8045f4b-1289-40f4-a37e-76e2e2ebeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading live examples\n",
    "abc_headline, abc_content, abc_link = abc_updated_news()\n",
    "npr_headline, npr_content, npr_link = npr_updated_news()\n",
    "fox_headline, fox_content, fox_link = fox_updated_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "a769921d-d300-4fc1-97e7-01cbe40ebdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "npr_output = GenAI_article_truth_processing(npr_content, [], [], npr_headline)\n",
    "npr_output = npr_output[0][0][1]\n",
    "npr_output = npr_output_parse(npr_output)\n",
    "npr_final_output = \"Here is a link to the analyzed article. \" + npr_link + \" \\n \" + npr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "99ac30dc-15e2-473a-bcbb-dc4de060a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "fox_output = GenAI_article_truth_processing(fox_content,[], [], fox_headline)\n",
    "fox_output = fox_output[0][0][1]\n",
    "fox_output = fox_output_parse(fox_output)\n",
    "fox_final_output = \"Here is a link to the analyzed article. \" + fox_link + \" \\n \" + fox_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "9dc33966-71c2-4ba6-9765-b8f5bb381e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "abc_output = GenAI_article_truth_processing(abc_content,[], [], abc_headline)\n",
    "abc_output = abc_output[0][0][1]\n",
    "abc_output = abc_output_parse(abc_output)\n",
    "abc_final_output = \"Here is a link to the analyzed article. \" + abc_link + \" \\n \" + abc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386c489-c97b-4066-95bd-e92bfa665554",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Article testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "acc812f6-2d3a-431b-b579-26f4877dcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"Months after leaving the White House, former President Donald Trump began plotting his return to Wall Street. That return, delayed by years of regulatory and legal hurdles, is now on the verge of becoming a reality — and it could make Trump a fortune.\n",
    "\n",
    "US regulators have finally given the green light to a controversial merger between Truth Social owner Trump Media & Technology Group and a blank-check company. The blessing from the Securities and Exchange Commission removes the last major obstacle holding back the deal.\n",
    "\n",
    "The merger, if approved by shareholders, would pave the way for Trump Media to become a publicly-traded company — one where Trump will own a dominant stake that could be worth billions.\n",
    "\n",
    "Digital World Acquisition Corp., the blank-check firm, announced that on Wednesday the SEC signed off on the merger proxy for the deal. A date for a shareholder vote will be set by Friday.\n",
    "\n",
    "“It does look like this deal is going to reach the finish line now — after more than two years of delays,” said Jay Ritter, a finance professor at the University of Florida.\n",
    "\n",
    "Trump stake could be worth $4 billion\n",
    "Shares of Digital World, a special purpose acquisition company, or SPAC, spiked 15% on the major milestone. The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress.\n",
    "\n",
    "Ritter estimates the merger could pave the way for about $270 million of cash coming into Trump Media, funds the company could fuel Truth Social’s growth.\n",
    "\n",
    "Trump is set to hold a dominant position in the newly-combined company, owning roughly 79 million shares, according to new SEC filings.\n",
    "\n",
    "The former president’s stake would be valued at $4 billion based on Digital World’s current trading price of about $50.\n",
    "\n",
    "Of course, as Ritter notes, it would be very difficult for Trump to translate that paper wealth into actual cash.\n",
    "\n",
    "Not only would Trump be subject to a lock-up period that would prevent he and other insiders from selling until six months after the merger, but the new company’s fortunes would be closely associated with the former president. That could make it difficult for Trump to sell even after the lock-up period expires.\n",
    "\n",
    "‘This is a meme stock’\n",
    "Moreover, there are major questions about the sky-high valuation being placed on this media company.\n",
    "\n",
    "“This is a meme stock. The valuation is totally divorced from the fundamental value of the company,” said Ritter.\n",
    "\n",
    "Digital World’s share price values the company at up to about $8 billion on a fully diluted basis, which includes all shares and options that could be converted to common stock, according to Ritter.\n",
    "\n",
    "He described that valuation as “crazy” because Trump Media is generating little revenue and burning through cash.\n",
    "\n",
    "New SEC filings indicate Trump Media’s revenue amounted to just $1.1 million during the third quarter. The company posted a loss of $26 million.\n",
    "\n",
    "Since the merger was first proposed in October 2021, legal, regulatory and financial questions have swirled about the transaction.\n",
    "\n",
    "In November, accountants warned that Trump Media was burning cash so rapidly that it might not survive unless the long-delayed merger with Digital World is completed soon.\n",
    "\n",
    "Shareholder vote looms\n",
    "Now, Trump execs are cheering the green light from the SEC.\n",
    "\n",
    "“Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. “Moving forward, we aim to accelerate our work to build a free speech highway outside the stifling stranglehold of Big Tech.”\n",
    "\n",
    "Eric Swider, Digital World’s CEO, described the SEC approval as a “significant milestone” and said executives are “immensely proud of the strides we’ve taken towards advancing” the merger.\n",
    "\n",
    "One of the final remaining hurdles is for Digital World shareholders to approve the merger in an upcoming vote.\n",
    "\n",
    "The shareholders have enormous incentive to approve the deal because if the merger fails, the blank-check firm would be forced to liquidate. That would leave shareholders with just $10 a share, compared with $50 in the market today.\n",
    "\n",
    "“Anyone who holds shares and votes against the merger is crazy,” said Ritter, the professor.\n",
    "\n",
    "“Then again, I might argue that everyone holding DWAC shares is crazy,” he added, referring to the company’s thin revenue and hefty valuation.\n",
    "\n",
    "Matthew Tuttle, CEO of Tuttle Capital Management, said he’s not surprised by the ups and downs surrounding this merger.\n",
    "\n",
    "“The thing about Trump and anything related to Trump is, love him or hate him, there is going to be drama,” said Tuttle, who purchased options to buy Digital World shares in his personal account. “Really, I would not have expected anything less.”\n",
    "\n",
    "Going forward, Tuttle said Trump Media’s share price will live and die by how everything plays out for Trump personally — from his legal troubles to his potential return to the White House.\n",
    "\n",
    "“Anything bullish for Trump is going to be bullish for the stock,” said Tuttle.\n",
    "\n",
    "Trump is no stranger to Wall Street, where he has a history, one marked by bankruptcies.\n",
    "\n",
    "Although Trump has never filed for personal bankruptcy, he has filed four business bankruptcies — all of them linked to casinos he used to own in Atlantic City.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "41149610-953a-4b48-a447-207b45c26035",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Are Americans paying nearly $500 for an inhaler that would cost just $7 overseas?\n",
    "\n",
    "U.S. Sen. Tammy Baldwin, D-Wis., says there is a vast difference in the cost of prescriptions in the United States and the rest of the world. \n",
    "\n",
    "\"Big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US,\" Baldwin said Feb. 1 in a Facebook post. \"That has got to end. We've got to hold Big Pharma accountable for their price-gouging tactics. I won't stop fighting until we do.\"\n",
    "\n",
    "That massive cost difference piqued our interest.\n",
    "\n",
    "How much would patients pay? \n",
    "When we asked for backup information, Baldwin’s campaign staff directed us to drug pricing websites, news articles and news releases on the cost of Combivent Respimat (ipratropium bromide and albuterol), a combination medication used to treat chronic obstructive pulmonary disease (COPD). \n",
    "\n",
    "Combivent Respimat is available only as a brand-name medication and not available in generic form, according to Medical News Today, which pointed out that the actual price a patient would pay for the medication depends on type of insurance plan, location and pricing at the patient’s pharmacy. Medicare does cover Combivent Respimat. \n",
    "\n",
    "According to Drugs.com, a pricing website, Combivent Respimat costs about $525 for a supply of 4 grams, depending on the pharmacy. \n",
    "\n",
    "It’s also important to note, that on a practical basis, because of insurance and Medicare coverage, few people in the United States would actually pay $500 out of pocket\n",
    "\n",
    "\"Quoted prices are for cash-paying customers and are not valid with insurance plans,\" the website says  says. \n",
    "\n",
    "Another online drug pricing guide, GoodRx, puts the price of Combivent Respimat between about $477 and $584 at Madison, Wisconsin, pharmacies:\n",
    "\n",
    "Walgreens —    $508.39 \n",
    "\n",
    "Walmart —----   $514.45\n",
    "\n",
    "CVS Pharmacy-$508.14\n",
    "\n",
    "Hy-Vee —--------$477.97\n",
    "\n",
    "Costco —---------$584.59\n",
    "\n",
    "Target —----------$508.14\n",
    "\n",
    "FEATURED FACT-CHECK\n",
    "\n",
    "Instagram posts\n",
    "stated on February 15, 2024 in an Instagram post\n",
    "Because “17 million immigrants” were “let in” the U.S, “ foot and mouth disease is back. We got rid of that fifty years ago.”\n",
    "truefalse\n",
    "By Jeff Cercone • February 16, 2024\n",
    "Metro Market —-$511.00\n",
    "\n",
    "Pick ’n Save—---$511.00\n",
    "\n",
    "So, Baldwin is on target on the cost in the US.\n",
    "\n",
    "What about overseas?\n",
    "According to a Jan. 8 news release from U.S. Sen. Bernie Sanders, I-Vt., Combivent Respimat sold for just $7 In France.\n",
    "\n",
    "Sanders, chairman of the Senate Committee on Health, Education, Labor, and Pensions, sent letters to the CEOs of four pharmaceutical companies announcing an investigation into the high prices the companies are charging for inhalers. Baldwin and Democratic Sens. Ben Ray Luján of New Mexico and Ed Markey of Massachusetts also signed the letters.\n",
    "\n",
    "The letters were sent to the four biggest manufacturers of inhalers sold in the United States — AstraZeneca, Boehringer Ingelheim, GlaxoSmithKline (GSK) and Teva.\n",
    "\n",
    "\"It is beyond absurd that Boehringer Ingelheim charges $489 for Combivent Respimat in the United States, but just $7 in France,\" Sanders said in the news release.\n",
    "\n",
    "The news release said the Committee’s source for the price of Combivent Respimat in France was the Navlin international drug pricing database. \n",
    "\n",
    "Baldwin, in the news release, accuses companies of \"jacking up prices and turning record profits.\"\n",
    "\n",
    "Experts weigh in \n",
    "Dr. William B. Feldman noted that Baldwin is referring to list prices here — which are the prices that uninsured patients in the U.S. pay and the prices to which out-of-pocket costs are often tied.\n",
    "\n",
    "\"Manufacturers give sizable (confidential) rebates to insurers, and so the net prices for inhalers in the U.S. are below list prices — but still much higher than the net prices abroad,\" Feldman said in an email to PolitiFact Wisconsin. \n",
    "\n",
    "Feldman, who works at Brigham and Women’s Hospital in Boston and Harvard Medical School, said a key reason inhaler prices remain so high in the U.S. is that there is very little generic competition. \n",
    "\n",
    "\"Brand-name manufacturers have erected large patent thickets that keep generic competitors off the market,\" Feldman said. \" Inhaler prices are low elsewhere, in part, because governments negotiate prices based on the value of the drugs compared to existing therapies.\"\n",
    "\n",
    "David Kreling, professor emeritus in the School of Pharmacy at the University of Wisconsin-Madison, said the U.S. price quoted by Baldwin sounds about right.\n",
    "\n",
    "\"The $500 number may be in the ballpark for U.S. patented (brand-name, newer) drugs,\" Kreling said in an email to PolitiFact Wisconsin. \"That would be consistent with my understanding of market data on sales by firms in the U.S. Things in the $7 range, here, only reside within the off-patent generic drug market (where we have low prices, sometimes at or near lowest in the world).\" \n",
    "\n",
    "Our ruling\n",
    "Baldwin said \"big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US.\"\n",
    "\n",
    "Our review, and that of experts, found the numbers checked out.\n",
    "\n",
    "Experts cite a variety of reasons for the price differences, including very little generic competition in the United States, and few people in the United States would actually pay $500 out of pocket because of insurance and Medicare coverage. \n",
    "\n",
    "For a statement that is accurate but needs clarification or additional information, our rating is Mostly True.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019ce28-d054-4dc4-be10-dd967f3ca57e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Gradio (Website) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "7401e9a3-5946-4525-822d-a00f932f5cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: http://774c35acd4cc675e18.disinformation-destroyers.com\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://774c35acd4cc675e18.disinformation-destroyers.com\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n",
      "0.4255682315677405\n",
      "0.31921496648680087\n"
     ]
    }
   ],
   "source": [
    "block = gr.Blocks()\n",
    "prompt_placeholder = \"Insert your news article here!\"\n",
    "headline_placeholder = \"Paste your news articles headline here!\"\n",
    "with block:\n",
    "    gr.Markdown(\"\"\"<h1><center>Generative AI News Article Truthfulness Evaluator</center></h1>\n",
    "    \"\"\")\n",
    "    examples = gr.Dropdown([\"ABC\", \"NPR\", \"FOX\"], label=\"News Provider\", info=\"Take any of the news providers in the dropdown below and type the name of the provider exactly how you see it into the Article Content Textbox below to get an up-to-date example articles evaluation!\")\n",
    "    message = gr.Textbox(placeholder=prompt_placeholder, label=\"Article Content\",info=\"Paste any news article here, or type in the name of one of the news providers in the dropdown above.\")\n",
    "    headline = gr.Textbox(placeholder=headline_placeholder, label = \"Headline\", info=\"Paste your news articles headline here if available for improved results.\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State()\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    submit.click(make_plot, inputs=[message], outputs=gr.Plot())\n",
    "    pred_info = gr.Textbox(placeholder=\"Predictive Analysis\", label=\"Predictive Analysis\")\n",
    "    overall_synopsis = gr.Textbox(placeholder=\"Overall Rating\", label=\"Overall Rating\")\n",
    "    submit.click(GenAI_article_truth_processing, inputs=[message, state, examples, headline], outputs=[chatbot, state, pred_info, overall_synopsis])\n",
    "block.launch(share=True, share_server_address=\"disinformation-destroyers.com:7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71463b-a048-4600-96ad-0631698b2bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
