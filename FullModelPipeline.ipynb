{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3a22c4dd-7522-42d0-aee0-9dd287628ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import chromadb\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310d4cb-a74a-4de1-af37-c9f5343d42b6",
   "metadata": {},
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "d1ed7db0-5ef5-462e-a137-47fba7d74914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(confidence):\n",
    "    if confidence < 0.166:\n",
    "        return \"pants-fire\"\n",
    "    elif confidence < 0.33:\n",
    "        return \"false\"\n",
    "    elif confidence < 0.5:\n",
    "        return \"barely-true\"\n",
    "    elif confidence < 0.666:\n",
    "        return \"half-true\"\n",
    "    elif confidence < 0.833:\n",
    "        return \"mostly-true\" \n",
    "    else:\n",
    "        return \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fb7c6eb6-6d77-4dd0-a816-9c84941f3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(tokens, model):\n",
    "    embeddings = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7a5fb90a-8a69-40ff-9097-cad5d43a0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_embeddings(text, n, model):\n",
    "    words = text.split()\n",
    "    ngrams = [words[i:i + n] for i in range(len(words) - n + 1)]  \n",
    "    embeddings = [model.wv[gram] for gram in ngrams if all(word in model.wv for word in gram)]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3a998364-0447-45d1-bf81-38fbe64e00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "16831eb1-a5ff-415d-9469-5a58b6ad59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_strings(text):\n",
    "    return '' if len(text) < 7 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f66a2d7b-8ec1-4192-8ffe-9c9acacfe19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2d190ec3-4e6c-4408-b3bc-5dc908d8861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_chunks(text, min_words=75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        if len(current_chunk) + len(words) < min_words:\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if any(sentence.endswith(p) for p in ['.', '!', '?', '¡', '¿']):\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = words\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e23075dc-4ef8-4d14-af84-d27004a90796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_clean(text):\n",
    "    # Remove spaces before or after \"'\" mark\n",
    "    text = text.replace(\" '\", \"'\").replace(\"' \", \"'\")\n",
    "    # Remove white space before \",\"\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    # Remove white space before or after \"”\" or \"“\" character\n",
    "    text = text.replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n",
    "    text = text.replace(\"Score:\", \" Score:\")\n",
    "    text = text.replace(\" ’\", \"’\").replace(\"’ \", \"’\")\n",
    "    text = text.replace(\"$ \", \"$\")\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9c575404-dc61-4b43-b58b-5596fc718658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(input):\n",
    "    truth_scores = predict_tabular_classification_sample(project=\"dsc-180a-b09\",\n",
    "                                                         endpoint_id=\"4607809140427849728\",\n",
    "                                                         instance_dict={\"article\": input})\n",
    "    \n",
    "    reordered_indices = [truth_scores[0]['classes'].index(c) for c in ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']]\n",
    "    classes_reordered = [truth_scores[0]['classes'][i] for i in reordered_indices]\n",
    "    scores_reordered = [truth_scores[0]['scores'][i] for i in reordered_indices]\n",
    "    \n",
    "    # Define color transition from red to green\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0, 1, len(classes_reordered)))\n",
    "    # Plot the bar chart with color transition\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(classes_reordered, scores_reordered, color=colors)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Predictive Auto ML Truthfulness Scores')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Scores')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff905ef1-f846-42f4-9454-54747f87215d",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4c04f40a-88a8-4176-b467-cc51767e3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact articles\n",
    "pf_articles = pd.read_csv(\"Webscraping/politifact_articles.csv\")\n",
    "pf_articles = pf_articles.drop(columns='Unnamed: 0')\n",
    "pf_articles.rename(columns={'Statement': 'Title'}, inplace=True)\n",
    "pf_articles = pf_articles.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bd711031-7b63-4af5-a8d2-e79755f84199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact truth datasets\n",
    "pf_statements = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_truthometer_df.csv\")\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0')\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0.1')\n",
    "pf_statements = pf_statements.dropna()\n",
    "pf_statements_full = pf_statements\n",
    "pf_statements = pf_statements.sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4fb33555-68fa-4d47-9ec1-44a8184ac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factcheckorg_articles = pd.read_csv(\"Webscraping/factcheckorg_webscrape_200pages.csv\")\n",
    "factcheckorg_articles['List_data'].fillna('', inplace=True)\n",
    "factcheckorg_articles['List_data'] = factcheckorg_articles['List_data'].apply(filter_short_strings)\n",
    "factcheckorg_articles = factcheckorg_articles.dropna(subset=['Text'])\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace('Para leer en español, vea esta traducción de Google Translate.', '')\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace(r' Editor’s Note:.*$', '', regex=True)\n",
    "factcheckorg_articles = factcheckorg_articles.reset_index()\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['index'])\n",
    "factcheckorg_articles['Title_and_Date'] = factcheckorg_articles['Title'] + ' , ' + factcheckorg_articles['Date']\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['Title', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ea624713-513c-49a2-978f-37efef9e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciencefeedbackorg_articles = pd.read_csv(\"Webscraping/science_feedback.csv\")\n",
    "sciencefeedbackorg_articles = sciencefeedbackorg_articles.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d2ea378e-cae2-4d27-805d-491e0f1ac8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scicheckorg_articles = pd.read_csv(\"Webscraping/scicheck_data.csv\")\n",
    "scicheckorg_articles['Title_and_Date'] = scicheckorg_articles['Title'] + ' , ' + scicheckorg_articles['Date']\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['Title', 'Date', 'Unnamed: 0'])\n",
    "scicheckorg_articles.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bc940-1167-49b0-8f5e-dbfc2f759ace",
   "metadata": {},
   "source": [
    "## Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64759e20-b8fd-4df0-8912-70d1dee5823a",
   "metadata": {},
   "source": [
    "### Irisa's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "87d9bfb5-6538-4e82-890d-5ded9ad9ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "d1aa714e-1f45-4e8b-a73a-f17432bf7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from scipy.stats import percentileofscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b01f76a6-61cb-4d35-bf1e-62943ea87541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Haaretz investigation reveals discrepancies in...</td>\n",
       "      <td>A viral Oct. 28 social media post claimed that...</td>\n",
       "      <td>Haaretz, an Israeli newspaper, said on X that ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wisconsin has historically … and I think large...</td>\n",
       "      <td>In 2016, Wisconsin helped to swing the preside...</td>\n",
       "      <td>Although Wisconsin has voted for more Democrat...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Haaretz investigation reveals discrepancies in...   \n",
       "1  Wisconsin has historically … and I think large...   \n",
       "\n",
       "                                             article  \\\n",
       "0  A viral Oct. 28 social media post claimed that...   \n",
       "1  In 2016, Wisconsin helped to swing the preside...   \n",
       "\n",
       "                                             summary  label  \n",
       "0  Haaretz, an Israeli newspaper, said on X that ...    4.0  \n",
       "1  Although Wisconsin has voted for more Democrat...    3.0  "
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Cleaning\n",
    "def read_dataset(csv):\n",
    "    df = pd.read_csv(csv)\n",
    "    df = df.drop(columns=[\"percentages\", \"check_nums\"]).drop_duplicates().dropna()\n",
    "    \n",
    "    mapping = {\n",
    "        \"TRUE\": 0,\n",
    "        \"mostly-true\": 1,\n",
    "        \"half-true\": 2,\n",
    "        \"barely-true\": 3,\n",
    "        \"FALSE\": 4,\n",
    "        \"pants-fire\": 5\n",
    "    }\n",
    "    \n",
    "    df[\"label\"] = df[\"label\"].map(mapping)\n",
    "    \n",
    "    df = df[pd.to_numeric(df[\"label\"], errors=\"coerce\").notna()]\n",
    "    df = df[[\"content\",\"article\",\"summaries\",\"label\"]]\n",
    "    df[\"content\"] = df[\"content\"].str.replace(r'[“\\”]', '', regex=True)\n",
    "    df[\"summaries\"] = df[\"summaries\"].str.replace(r'[\\[\\]\\'\"]', '', regex=True)\n",
    "    df.columns = [\"title\", \"article\", \"summary\", \"label\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = read_dataset(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/politifact_data_combined_prev_ratings.csv\")\n",
    "df = df = df[df['summary'] != '']\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bdcfd0-bbe8-4634-b7eb-6b5b0ff3072a",
   "metadata": {},
   "source": [
    "#### Feature 1: Sentiment Analysis  (pos=1, neg=-1, neu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "35d77ca1-c785-4a5a-90f1-29ca01571b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sentiment Analysis Using NLTK\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment\"] = df[\"article\"].apply(lambda x: analyzer.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cbcd02-d41e-4b4a-a628-69f3f4854279",
   "metadata": {},
   "source": [
    "#### Feature 2: Quality of Writing (Type-Token Ratio (TTR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "da3a4e11-4f71-4471-ae8f-c48ac3a54706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove stopwords and punctuation & Make lowercase\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    cleaned_text = ''.join([char for char in text if char not in punctuation])\n",
    "    return cleaned_text\n",
    "\n",
    "df[\"article\"] = df[\"article\"].apply(lambda x: x.lower())\n",
    "df[\"article\"] = df[\"article\"].apply(remove_punctuation)\n",
    "df[\"article\"] = df[\"article\"].apply(remove_stopwords)\n",
    "\n",
    "# 2. TTR = unique_words/total_words\n",
    "\n",
    "df['ttr'] = df['article'].apply(lambda x: x.split()).apply(lambda words: len(set(words)) / len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bdc9b7-8635-48d1-806e-04b50612abf5",
   "metadata": {},
   "source": [
    "#### Feature 3: Expressiveness (Adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "be196c4b-426a-4d32-b967-0017077a373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Open List of Adjectives (Link: https://gist.github.com/hugsy/8910dc78d208e40de42deb29e62df913)\n",
    "    ### Additional Sources: https://github.com/taikuukaits/SimpleWordlists/tree/master\n",
    "\n",
    "with open(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/adjectives.txt\", \"r\") as file:\n",
    "    adjectives = [line.strip() for line in file]\n",
    "    \n",
    "# 2. Count adjectives\n",
    "\n",
    "def count_adjectives(text):\n",
    "    words = text.split()\n",
    "    adjective_count = sum(1 for word in words if word.lower() in adjectives) / len(words)\n",
    "    return adjective_count\n",
    "\n",
    "df[\"adjectives\"] = df[\"article\"].apply(count_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c1df5-3bd2-454a-8bc5-8640c00047bd",
   "metadata": {},
   "source": [
    "#### Predictions (One vs Rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "ffcafcc7-77d9-4876-a410-5ec26973b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"title\",\"article\",\"summary\",\"label\"])\n",
    "y = df[\"label\"]\n",
    "\n",
    "irisa_X_train, X_test, y_train, y_test_multi = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0241af3f-6386-42a7-b418-222b7c6f9db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ttr</th>\n",
       "      <th>adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>0.8047</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.108333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>-0.6533</td>\n",
       "      <td>0.698198</td>\n",
       "      <td>0.090090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3918</th>\n",
       "      <td>0.6187</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.059211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6189</th>\n",
       "      <td>-0.8855</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.094118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>-0.9974</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.061404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>0.9119</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.092857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>0.8156</td>\n",
       "      <td>0.624729</td>\n",
       "      <td>0.049892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6052</th>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.611842</td>\n",
       "      <td>0.100329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>-0.9938</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.044177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0.9385</td>\n",
       "      <td>0.624793</td>\n",
       "      <td>0.072727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment       ttr  adjectives\n",
       "4196     0.8047  0.883333    0.108333\n",
       "5365    -0.6533  0.698198    0.090090\n",
       "3918     0.6187  0.815789    0.059211\n",
       "6189    -0.8855  0.705882    0.094118\n",
       "3999    -0.9974  0.789474    0.061404\n",
       "...         ...       ...         ...\n",
       "4311     0.9119  0.714286    0.092857\n",
       "6014     0.8156  0.624729    0.049892\n",
       "6052     0.4292  0.611842    0.100329\n",
       "6243    -0.9938  0.706827    0.044177\n",
       "893      0.9385  0.624793    0.072727\n",
       "\n",
       "[4698 rows x 3 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisa_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "fe89f1e2-a703-4d53-9fca-d05760e04900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "sentiment_percentile = percentileofscore(irisa_X_train['sentiment'], irisa_pred_df['sentiment'])[0]\n",
    "ttr_percentile = percentileofscore(irisa_X_train['ttr'], irisa_pred_df['ttr'])[0]\n",
    "adjectives_percentile = percentileofscore(irisa_X_train['adjectives'], irisa_pred_df['adjectives'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "35e44ff5-4638-43ea-a6bd-888fd653535d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.02809706257982"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "abb7fb54-938e-48c8-9c56-9f31c9a827c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.531063829787234\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    #KNeighborsClassifier(2),\n",
    "    #SVC(kernel=\"linear\", C=0.025),\n",
    "    #SVC(gamma=2, C=1),\n",
    "    #DecisionTreeClassifier(max_depth=5),\n",
    "    #RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    #MLPClassifier(alpha=1, max_iter=1000),\n",
    "    #AdaBoostClassifier(),\n",
    "    GaussianNB()]\n",
    "    #QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    irisa_clf = OneVsOneClassifier(classifier).fit(irisa_X_train, y_train)\n",
    "    predictions = irisa_clf.predict(X_test)\n",
    "    print(accuracy_score(y_test_multi, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b625419f-69d7-4c77-85b5-ee89d614f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing new article input\n",
    "#clickbait\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_title = tfidf_vectorizer.fit_transform([title_news])\n",
    "tfidf_article = tfidf_vectorizer.transform([news])\n",
    "cosine_sim = cosine_similarity(tfidf_title, tfidf_article)\n",
    "irisa_clickbait = cosine_sim.diagonal()[0]\n",
    "\n",
    "#sentiment prediction\n",
    "irisa_sentiment = analyzer.polarity_scores(news)[\"compound\"]\n",
    "\n",
    "#quality of writing prediction\n",
    "words = news.split()\n",
    "irisa_qor_ratio = len(set(words)) / len(words)\n",
    "\n",
    "#sensationalism\n",
    "irisa_sensationalism = count_adjectives(news)\n",
    "\n",
    "#adding to df for prediction\n",
    "irisa_data = {\n",
    "    \"sentiment\": [irisa_sentiment],\n",
    "    \"ttr\": [irisa_qor_ratio],\n",
    "    \"adjectives\": [irisa_sensationalism]\n",
    "}\n",
    "\n",
    "irisa_pred_df = pd.DataFrame(irisa_data)\n",
    "\n",
    "#irisa final prediction\n",
    "final_prediction = irisa_clf.predict(irisa_pred_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a833f776-d017-4cdf-be5d-24eff49e651f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1593140-a52a-4cee-9608-dec37d282ee4",
   "metadata": {},
   "source": [
    "### Lohit's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "5114ee62-ffb4-4324-b821-3b3635dd2631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "cbdbade9-b53d-4371-8da8-b5fb3a61e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load and clean\n",
    "data = pd.read_csv(\"/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/politifact_data_combined_prev_ratings.csv\")\n",
    "noise_labels = set(['full-flop', 'half-flip', 'no-flip'])\n",
    "data = data.query(\"label not in ['full-flop', 'half-flip', 'no-flip']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3f71b4ea-b8b4-4700-984e-038732c80bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media</th>\n",
       "      <th>when/where</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>speaker</th>\n",
       "      <th>documented_time</th>\n",
       "      <th>percentages</th>\n",
       "      <th>check_nums</th>\n",
       "      <th>summaries</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instagram posts</td>\n",
       "      <td>stated on October 28, 2023 in a screenshot sha...</td>\n",
       "      <td>“Haaretz investigation reveals discrepancies i...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Madison Czopek</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>['0%' '0%' '2%' '7%' '67%' '21%']</td>\n",
       "      <td>[  5   3  16  54 473 152]</td>\n",
       "      <td>['Haaretz, an Israeli newspaper, said on X tha...</td>\n",
       "      <td>A viral Oct. 28 social media post claimed that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>stated on May 30, 2023 in Interview:</td>\n",
       "      <td>“Wisconsin has historically … and I think larg...</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Laura Schulte</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>['12%' '21%' '18%' '19%' '21%' '5%']</td>\n",
       "      <td>[26 45 39 41 44 11]</td>\n",
       "      <td>['Although Wisconsin has voted for more Democr...</td>\n",
       "      <td>In 2016, Wisconsin helped to swing the preside...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             media                                         when/where  \\\n",
       "0  Instagram posts  stated on October 28, 2023 in a screenshot sha...   \n",
       "1     Scott Walker               stated on May 30, 2023 in Interview:   \n",
       "\n",
       "                                             content        label  \\\n",
       "0  “Haaretz investigation reveals discrepancies i...        FALSE   \n",
       "1  “Wisconsin has historically … and I think larg...  barely-true   \n",
       "\n",
       "          speaker   documented_time                           percentages  \\\n",
       "0  Madison Czopek  October 31, 2023     ['0%' '0%' '2%' '7%' '67%' '21%']   \n",
       "1   Laura Schulte  October 31, 2023  ['12%' '21%' '18%' '19%' '21%' '5%']   \n",
       "\n",
       "                  check_nums  \\\n",
       "0  [  5   3  16  54 473 152]   \n",
       "1        [26 45 39 41 44 11]   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  ['Haaretz, an Israeli newspaper, said on X tha...   \n",
       "1  ['Although Wisconsin has voted for more Democr...   \n",
       "\n",
       "                                             article  \n",
       "0  A viral Oct. 28 social media post claimed that...  \n",
       "1  In 2016, Wisconsin helped to swing the preside...  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "97123229-8329-4fa2-ba91-7349c2157e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6658758402530645\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       FALSE       0.60      0.85      0.70      1438\n",
      "        TRUE       0.64      0.41      0.50       555\n",
      " barely-true       0.78      0.57      0.66       805\n",
      "   half-true       0.72      0.78      0.75       830\n",
      " mostly-true       0.60      0.61      0.60       757\n",
      "  pants-fire       0.89      0.52      0.66       673\n",
      "\n",
      "    accuracy                           0.67      5058\n",
      "   macro avg       0.70      0.62      0.64      5058\n",
      "weighted avg       0.69      0.67      0.66      5058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data[['content', 'article']]\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the text data with unigrams and bigrams\n",
    "tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['content'] + \" \" + X_train['article'])\n",
    "X_test_tfidf = tfidf.transform(X_test['content'] + \" \" + X_test['article'])\n",
    "\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\\n\")\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "04d8927b-127a-4082-ac21-ab6f54de48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'barely-true'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the test instance\n",
    "X_test_instance = [title_news + \" \" + news]\n",
    "\n",
    "# Vectorize the test instance using the same TF-IDF vectorizer trained on the training data\n",
    "X_test_instance_tfidf = tfidf.transform(X_test_instance)\n",
    "\n",
    "# Make predictions for the test instance\n",
    "y_pred_instance = classifier.predict(X_test_instance_tfidf)\n",
    "\n",
    "y_pred_proba = classifier.predict_proba(X_test_instance_tfidf)\n",
    "positive_class_proba = y_pred_proba\n",
    "overall_score = (positive_class_proba[0][0] * 0.2) + (positive_class_proba[0][1] * 1) + (positive_class_proba[0][2] * 0.4) + (positive_class_proba[0][3] * 0.6) + (positive_class_proba[0][4] * 0.8) + (positive_class_proba[0][5] * 0.0)\n",
    "lohit_predicted_label = predict_label(overall_score)\n",
    "lohit_predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29231e0a-25f2-46c4-9634-319bb1399c98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nick's Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fed78a2c-d2e3-46df-9051-2fff14357b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from readability import Readability\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c03c0a-54c1-4c92-8efe-d65e3b2c81b3",
   "metadata": {},
   "source": [
    "#### Factor 1: Flesch-Kincaid Grade Level Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0877509e-b9dc-4899-ac1d-bd72c38b27af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = Readability(news)\n",
    "fk = r.flesch_kincaid()\n",
    "flesch_score = fk.score\n",
    "if flesch_score > 12:\n",
    "    diff = flesch_score - 12\n",
    "    fk_rating = 100 - (diff * 10)\n",
    "elif flesch_score < 8:\n",
    "    diff = 8 - flesch_score\n",
    "    fk_rating = 100 - (diff * 10)\n",
    "else:\n",
    "    fk_rating = 100\n",
    "fk_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040db8a-0027-42d1-baf4-9442bbc01f75",
   "metadata": {},
   "source": [
    "#### Factor 2: Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d0e02001-425c-4a43-a1fe-a411db7ab7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "moving_sentiment_value = 0\n",
    "number_of_paragraphs = 0\n",
    "paragraphs = news.split('\\n\\n')\n",
    "for i in paragraphs:\n",
    "    cleaned_text = ' '.join(i.split()).replace(\"\\'\", '')\n",
    "    compound_sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
    "    moving_sentiment_value += compound_sentiment_score\n",
    "    number_of_paragraphs += 1\n",
    "overall_sentiment = moving_sentiment_value / number_of_paragraphs\n",
    "overall_sent_score = 100\n",
    "if overall_sentiment < -0.2:\n",
    "    overall_sent_score = 100 + (overall_sentiment * 100)\n",
    "overall_sent_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42ad2c-1f9d-493f-9937-f64045b0d156",
   "metadata": {},
   "source": [
    "#### Factor 3: Clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5fd7a2b6-8f1b-4abe-8940-4866da024ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clickbait = pd.read_csv(\"Data/Clickbait_Data/clickbait_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f33592dc-9ac0-4006-888e-aaa14b8372c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier = Linear SVM, Score (test, accuracy) = 95.67, Training time = 52.51 seconds\n",
      "--------------------------------------------------------------------------------\n",
      "Best --> Classifier = Linear SVM, Score (test, accuracy) = 95.67\n"
     ]
    }
   ],
   "source": [
    "#determining clickbait\n",
    "names = [\"Linear SVM\"] \n",
    "\n",
    "classifiers = [SVC(kernel=\"linear\", C=0.025, probability=True)]\n",
    "\n",
    "\n",
    "#Preprocess, train/test split, and \n",
    "clickbait['PreprocessedTitle'] = clickbait['headline'].apply(preprocess_text)\n",
    "X_train_click, X_test_click, y_train_click, y_test_click = train_test_split(clickbait['PreprocessedTitle'], clickbait['clickbait'], test_size=0.2, random_state=42)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train_click)\n",
    "X_test_counts = count_vectorizer.transform(X_test_click)\n",
    "\n",
    "\n",
    "max_score = 0.0\n",
    "max_class = ''\n",
    "# iterate over classifiers\n",
    "for name, clf_ in zip(names, classifiers):\n",
    "    start_time = time.time()\n",
    "    clf_.fit(X_train_counts, y_train_click)\n",
    "    score = 100.0 * clf.score(X_test_counts, y_test_click)\n",
    "    print('Classifier = %s, Score (test, accuracy) = %.2f,' %(name, score), 'Training time = %.2f seconds' % (time.time() - start_time))\n",
    "    \n",
    "    if score > max_score:\n",
    "        clf_best = clf_\n",
    "        max_score = score\n",
    "        max_class = name\n",
    "\n",
    "print(80*'-' )\n",
    "print('Best --> Classifier = %s, Score (test, accuracy) = %.2f' %(max_class, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "daa425b8-74cc-4eb4-beab-dd743a0e2dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'half-true'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_title_processed = preprocess_text(title_news)\n",
    "article_title_vectorized = count_vectorizer.transform([article_title_processed])\n",
    "clickbait_probability = clf_best.predict_proba(article_title_vectorized)\n",
    "confidence_not_clickbait = clickbait_probability[:, 0]\n",
    "confidence_not_clickbait = confidence_not_clickbait[0]\n",
    "nick_predicted_label = predict_label(confidence_not_clickbait)\n",
    "nick_predicted_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d0b35-b00d-416b-9f2e-49297edf5394",
   "metadata": {},
   "source": [
    "### Henry's Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "bfb2670e-63d3-4394-864b-b9d13324796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python311.zip', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/lib-dynload', '', '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages', '/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/tmpkipf7d_d', '/Users/nicholasshor/Library/Python/3.11/lib/python/site-packages', '/Users/nicholasshor/Library/Python/3.11/lib/python/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import site\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "user_site_packages = site.USER_SITE\n",
    "sys.path.append(user_site_packages)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "2adfb37c-62fa-4855-87d9-1568f01c2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ID', 'Label', 'Statement', 'Subject(s)', 'Speaker','Speaker\\'s Job Title', 'State Info', 'Party Affiliation','Barely True', 'False', 'Half True', 'Mostly True', 'Pants on Fire','Context']\n",
    "df = pd.read_csv('/Users/nicholasshor/Desktop/School/FifthYear/Fall/DSC180A/Data/Liar_plus/train.tsv', delimiter='\\t', header = None, quoting=csv.QUOTE_NONE)\n",
    "df = df.drop(columns=[0, 15])\n",
    "df = df.rename(columns=dict(zip(df.columns, columns)))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "c278f493-ae85-4889-956c-d9a89761d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_ = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "6bbef1e2-a483-434d-ae2e-70fce94133e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statements = df['Statement'].tolist()\n",
    "labels = df['Label'].tolist()\n",
    "#tokenizeing the statements\n",
    "tokenized_statements = [tokenizer(statement, return_tensors=\"pt\", truncation=True, padding=True) for statement in statements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "21ff52c7-d5cb-4d81-a004-66f7d29ba50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "henry_model = model_.to(device)\n",
    "\n",
    "# Move tokenized statements to GPU\n",
    "tokenized_statements_gpu = [inputs.to(device) for inputs in tokenized_statements]\n",
    "\n",
    "# Extract BERT embeddings\n",
    "with torch.no_grad():\n",
    "    henry_model.eval()\n",
    "    statement_embeddings = [henry_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy() for inputs in tokenized_statements_gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "c7dbbea4-d8dc-4ca1-881e-b7954b5cb247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten\n",
    "X_embeddings = np.vstack(statement_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "2262821c-236d-47a5-8804-64dfdcb0cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the truth counts with the embeddings\n",
    "X_embeddings_df = pd.DataFrame(X_embeddings, columns=[f\"embedding_{i}\" for i in range(X_embeddings.shape[1])])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings_df, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "01f9e16c-a8c9-4d4c-9d97-3063743aa981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " barely-true       0.24      0.04      0.08       200\n",
      "       false       0.32      0.34      0.33       297\n",
      "   half-true       0.23      0.41      0.30       273\n",
      " mostly-true       0.28      0.45      0.34       268\n",
      "  pants-fire       1.00      0.01      0.02        84\n",
      "        true       0.34      0.12      0.18       227\n",
      "\n",
      "    accuracy                           0.27      1349\n",
      "   macro avg       0.40      0.23      0.21      1349\n",
      "weighted avg       0.33      0.27      0.24      1349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=3000, random_state=42, min_samples_split = 2, min_samples_leaf = 1, max_depth = 30)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced89dd-106a-4d60-adb3-a000944d564e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d85e0467-d390-42b5-80fa-15c959c0fceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FactCheckOrg Article Chunking\n",
    "factcheckorg_articles['chunks_text'] = factcheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "factcheckorg_articles['chunkslistdata'] = factcheckorg_articles['List_data'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = factcheckorg_articles['chunks_text'].apply(len).max()\n",
    "max_chunks_list_data = factcheckorg_articles['chunkslistdata'].apply(len).max()\n",
    "max_total_chunks = max(max_chunks_text, max_chunks_list_data)\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_total_chunks + 1):\n",
    "    factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "    factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['chunks_text', 'chunkslistdata', 'Text', 'List_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6d04d655-1dd9-4e78-9104-3b41b6bea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Statement Text Chunking\n",
    "pf_statements['chunks'] = pf_statements['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_statements['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_statements[f'chunk_{i}'] = pf_statements['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_statements = pf_statements.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9383641a-2e4c-45a3-b363-95aba9f5de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Articles Chunking\n",
    "pf_articles['chunks'] = pf_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_articles['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_articles = pf_articles.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2fe76bdb-2700-4532-9380-1daec03e19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SciCheckOrg Articles Chunking\n",
    "scicheckorg_articles['chunks_text'] = scicheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = scicheckorg_articles['chunks_text'].apply(len).max()\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_chunks_text + 1):\n",
    "    scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['chunks_text', 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fe66e-bac9-461d-a7e2-a4311b23adec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "23b31956-0dbb-4bf3-8b2c-b7df2e88b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "63a961bb-41c9-4e28-bb96-360fbca000af",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_CONTEXT_VDB = chroma_client.create_collection(name=\"RAG_CONTEXT_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "72fc7d10-34c5-4142-9965-9a674f004bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_STATEMENTS_VDB = chroma_client.create_collection(name=\"RAG_STATEMENTS_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "40ea61f4-0dfe-4b95-9b43-46d3687fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding pf statement justifications to Context VDB\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "chunks_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements.iterrows():\n",
    "    statement = row['Statement']\n",
    "    claimer = row['Claimer']\n",
    "    for col in pf_statements.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                metadata_list.append({\"Statement\": statement, \"Context\": \"Yes\", \"Claimer\": claimer})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9ad55249-93cb-4196-8111-00946af61458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter justifications to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9fce4111-b9e1-4692-b32e-d23ed3930d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding politifact truth-o-meter statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements_full.iterrows():\n",
    "    truth_value = row['Truth_value']\n",
    "    claimer = row['Claimer']\n",
    "    statement = row['Statement']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value,\"Claimer\": claimer})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "afc8f317-5f3b-4ea7-81a1-fee50c26be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c059fb88-3459-455a-9d21-33b62b13b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding factcheck.org data to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in factcheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in factcheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1\n",
    "        elif col.startswith('chunklist'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "44369d55-9b8a-4e94-b4f4-876ff2e9bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n"
     ]
    }
   ],
   "source": [
    "#Adding factcheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6af85c54-a3a7-4588-8d13-834688bc1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding SciCheckOrg articles to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in scicheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in scicheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d0bbd3a9-cd3b-4ecf-8491-5d1bf01df15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209615"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_CONTEXT_VDB.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d4b02868-a4e4-4db1-8264-8adc05216e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Adding scicheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1c40685c-37f7-4f1a-8bb2-d0780a3c343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding ScienceFeedbackOrg statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in sciencefeedbackorg_articles.iterrows():\n",
    "    truth_value = row['label']\n",
    "    statement = row['claim']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "13af0dcf-fc27-446e-a249-d5869954d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "#Adding pf sciencefeedback statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337cf1-74e2-4e5a-ba6b-1f9917e09d02",
   "metadata": {},
   "source": [
    "## FULL GEN AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "b44f93a4-9895-4d5e-aeed-8dd8e2b5689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from langchain.vectorstores import Chroma\n",
    "from vertexai.preview import generative_models\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4e77-5130-4be5-8292-aaebc511f392",
   "metadata": {},
   "source": [
    "#### Perspective API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2f83baf7-d65d-42fd-81f3-c945bf22b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSPECTIVE_API_KEY = 'AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5c58d5c6-946d-422f-84b3-1df34ddc35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds of output\n",
    "attributeThresholds = {\n",
    "    'INSULT': 0.8,\n",
    "    'TOXICITY': 0.8,\n",
    "    'THREAT': 0.5,\n",
    "    'SEXUALLY_EXPLICIT': 0.5,\n",
    "    'PROFANITY': 0.8\n",
    "}\n",
    "requestedAttributes = {}\n",
    "for key in attributeThresholds:\n",
    "    requestedAttributes[key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359fa40-6fc2-4c82-961c-3d800bdb28da",
   "metadata": {},
   "source": [
    "### Liar Liar Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6d52eb5a-93b3-4b58-8060-8c878ad4c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_liar_plus = pd.read_csv(\"Data/Liar_plus/train.tsv\", delimiter='\\t', header=None)\n",
    "liar_liar_plus = liar_liar_plus[[3, 2]]\n",
    "liar_liar_plus.dropna(inplace=True)\n",
    "llp_statements = liar_liar_plus[3]\n",
    "llp_labels = liar_liar_plus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "53e04697-0e2f-44df-b892-dd286e4af9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Says the Annies List political group supports ...\n",
       "1        When did the decline of coal start? It started...\n",
       "2        Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        Health care reform legislation is likely to ma...\n",
       "4        The economic turnaround started at the end of ...\n",
       "                               ...                        \n",
       "10237    There are a larger number of shark attacks in ...\n",
       "10238    Democrats have now become the party of the [At...\n",
       "10239    Says an alternative to Social Security that op...\n",
       "10240    On lifting the U.S. Cuban embargo and allowing...\n",
       "10241    The Department of Veterans Affairs has a manua...\n",
       "Name: 3, Length: 10240, dtype: object"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llp_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d8e76cef-2794-4659-8c98-02655571549b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[373], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m llp_statements:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[43mGenAI_article_truth_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      6\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[342], line 156\u001b[0m, in \u001b[0;36mGenAI_article_truth_processing\u001b[0;34m(news_article, history)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#Perspective API output safety check\u001b[39;00m\n\u001b[1;32m    152\u001b[0m analyze_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: output},\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequestedAttributes\u001b[39m\u001b[38;5;124m'\u001b[39m: requestedAttributes\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 156\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalyze_request\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m attributes_surpassed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributeScores\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">"
     ]
    }
   ],
   "source": [
    "label_prediction = []\n",
    "for i in llp_statements:\n",
    "    try:\n",
    "        label_prediction.append(GenAI_article_truth_processing(i,[])[0][0][1])\n",
    "    except (ValueError, IndexError) as e:\n",
    "        label_prediction.append(None)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "219d4d6c-dc30-45bb-af12-6338a7f31bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_prediction)):\n",
    "    # Check if the current value matches the target value\n",
    "    if label_prediction[i] == 'False':\n",
    "        label_prediction[i] = 'false'\n",
    "    elif label_prediction[i] == 'Half-true':\n",
    "        label_prediction[i] = 'half-true'\n",
    "    elif label_prediction[i] == 'Mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'Mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'True':\n",
    "        label_prediction[i] = 'true'\n",
    "    elif label_prediction[i] == 'mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'mostly-true, mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'Pants-on-fire':\n",
    "        label_prediction[i] = 'pants-fire'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1cf10b70-cc1a-46b1-89c6-73262048f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'true'}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_llp_labels = set(llp_labels)\n",
    "unique_llp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a17b1a74-6c21-4dd5-b1c1-17ca0ff37383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Barely true',\n",
       " 'Barely-true',\n",
       " 'Half True',\n",
       " 'Half-True',\n",
       " 'Mostly False',\n",
       " 'Mostly True',\n",
       " 'Mostly-False',\n",
       " 'Mostly-True',\n",
       " None,\n",
       " 'None provided',\n",
       " \"['half-true', 'false']\",\n",
       " \"['half-true', 'mostly-true']\",\n",
       " \"['mostly-true', 'false']\",\n",
       " \"['true', 'false', 'mostly-true', 'mostly-true', 'half-true', 'true', 'half-true', 'half-true', 'mostly-true', 'mostly-true', 'pants-on-fire', 'pants-on-fire', 'mostly-true', 'true']\",\n",
       " \"['true', 'mostly-true', 'mostly-true']\",\n",
       " \"['true', 'mostly-true']\",\n",
       " 'barely-true',\n",
       " 'false',\n",
       " 'half-true',\n",
       " 'half-true\\nmostly-true',\n",
       " 'half-true, false',\n",
       " 'half-true, mostly-true',\n",
       " 'half-true, pants-on-fire',\n",
       " 'half-true, true',\n",
       " 'mostly-false, mostly-true',\n",
       " 'mostly-true',\n",
       " 'mostly-true, half-true',\n",
       " 'mostly-true, mostly-false',\n",
       " 'mostly-true, true',\n",
       " 'pants-fire',\n",
       " 'pants-on-fire',\n",
       " 'pants-on-fire, half-true',\n",
       " 'pants-on-fire, true',\n",
       " 'true',\n",
       " 'true, mostly-true'}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label_predictions = set(label_prediction)\n",
    "unique_label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "62ab7a55-081a-4eec-a10a-3d5dc8f96d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matches = 0\n",
    "for value1, value2 in zip(label_prediction, llp_labels):\n",
    "    if value1 == value2:\n",
    "        num_matches += 1\n",
    "\n",
    "accuracy = num_matches / len(label_prediction) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a62e276a-a999-4e48-9a58-76ed0c9c69c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.44951140065146"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970f88f3-968c-4753-b76a-11d159481eab",
   "metadata": {},
   "source": [
    "#### Example news articles for website display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "c075aa3f-979b-4c76-8e79-0867bf8b1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5dd36be5-f847-46e9-83f4-adfed6d8751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABC News for Left\n",
    "def abc_updated_news():\n",
    "    abc_url = \"https://abcnews.go.com/\"\n",
    "    abc = requests.get(abc_url)\n",
    "    abc_soup = BeautifulSoup(abc.content, 'html')\n",
    "    abc_soup_url = abc_soup.find('a', {'class': 'AnchorLink News News--xl'}).get('href')\n",
    "    abc_top_article = requests.get(abc_soup_url)\n",
    "    abc_soup = BeautifulSoup(abc_top_article.content)\n",
    "    abc_content = abc_soup.find('div', {'data-testid': 'prism-article-body'}).text\n",
    "    cleaned_abc = abc_content.replace('\\'', '')\n",
    "    abc_headline = abc_soup.find('div', {'data-testid': 'prism-headline'}).text\n",
    "    return abc_headline, cleaned_abc, abc_soup_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f111bd0b-2cdd-42c5-b2f3-bf052fafd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fox news for right \n",
    "def fox_updated_news():\n",
    "    fox_url = \"https://moxie.foxnews.com/google-publisher/latest.xml\"\n",
    "    fox = requests.get(fox_url)\n",
    "    fox_soup = BeautifulSoup(fox.content, 'xml')\n",
    "    fox_link = fox_soup.find('item').find('link')\n",
    "    fox_link_str = str(fox_link)\n",
    "    fox_link = fox_link_str[6:-7]\n",
    "    fox_headline = fox_soup.find('item').find('title').text\n",
    "    fox_content = fox_soup.find('item').find('content:encoded').text\n",
    "    cleaned_fox = re.sub(r\"<[^>]*>||\\'\", '', fox_content)\n",
    "    cleaned_fox = cleaned_fox.replace('\\xa0', '')\n",
    "    cleaned_fox = cleaned_fox.replace('\\\\', '')\n",
    "    return fox_headline, cleaned_fox, fox_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fdccc585-3488-43f5-8a1c-c90410d27de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NPR as center\n",
    "def npr_updated_news():\n",
    "    npr_url = \"https://www.npr.org/\"\n",
    "    npr = requests.get(npr_url)\n",
    "    npr_soup = BeautifulSoup(npr.content, 'html')\n",
    "    npr_soup_url = npr_soup.find('div', {'class': 'story-text'})\n",
    "    npr_soup_url = npr_soup_url.find_all('a')[1]['href']\n",
    "    npr_article_soup = requests.get(npr_soup_url)\n",
    "    npr_article_soup = BeautifulSoup(npr_article_soup.content)\n",
    "    npr_headline = npr_article_soup.find('div', {'class': 'storytitle'}).text\n",
    "    all_text = npr_article_soup.find('div', {'id': 'storytext'}).find_all('p')\n",
    "    full_text = ''\n",
    "    for i in all_text[2:]:\n",
    "        full_text+=i.text\n",
    "    full_text = full_text.replace('\\'', '').replace('\\n', '')\n",
    "    npr_headline = npr_headline.replace('\\n', '')\n",
    "    return npr_headline, full_text, npr_soup_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d0be6-c7ef-481e-b3da-dd323a9ef99f",
   "metadata": {},
   "source": [
    "#### FlagEmbedding API implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "5ffbc8d2-b09d-4231-bf90-06c9c6c0a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447112b0-42e6-4b2d-9e48-002bd26f8c23",
   "metadata": {},
   "source": [
    "#### Predictive AI ML implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cbed1f5c-e38c-4e8d-a5ca-e87a7b3ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai import preview\n",
    "from typing import Dict\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d2784b7a-9ac2-4112-9c92-0913dc6da7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tabular_classification_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instance_dict: Dict,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # for more info on the instance schema, please use get_model_sample.py\n",
    "    # and look at the yaml found in instance_schema_uri\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/tabular_classification_1.0.0.yaml for the format of the predictions.\n",
    "    prediction_list=[]\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        prediction_list.append(dict(prediction))\n",
    "    return prediction_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9dd90346-3acc-4bb9-9ebb-68b09b4be933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "testing=predict_tabular_classification_sample(\n",
    "    project=\"dsc-180a-b09\",\n",
    "    endpoint_id=\"4607809140427849728\",\n",
    "    instance_dict={\"article\": news})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "521a6356-6bb7-4657-9fb2-c99fb7f1c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1836276352405548, 0.008846416138112545, 0.01037078443914652, 0.09028053283691406, 0.1769963353872299, 0.5298783183097839]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing[0]['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437b56d-bb04-4331-907f-d611f7030277",
   "metadata": {},
   "source": [
    "#### GEN AI Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "91d7256a-725a-42c3-8cfe-e1919de0eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "34385bfe-546f-4a04-9777-b437011329c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenAI_article_truth_processing(news_article, history, examples, headline):\n",
    "    history_output = []\n",
    "    #Pre-processed examples output\n",
    "    if news_article == \"ABC\":\n",
    "        history_output.append([news_article, abc_final_output])\n",
    "        return history_output, history_output\n",
    "    elif news_article == \"NPR\":\n",
    "        history_output.append([news_article, npr_final_output])\n",
    "        return history_output, history_output\n",
    "    elif news_article == \"FOX\":\n",
    "        history_output.append([news_article, fox_final_output])\n",
    "        return history_output, history_output\n",
    "        \n",
    "    #instantiating RAG re-ranking mecahnism\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "\n",
    "    #converting full news article to string\n",
    "    news_article = f\"\"\"{news_article}\"\"\"\n",
    "    \n",
    "    #getting history for context\n",
    "    history = history or []\n",
    "\n",
    "    #predictive models\n",
    "    #IRISAS PREDICTIONS\n",
    "\n",
    "    #sentiment prediction\n",
    "    irisa_sentiment = analyzer.polarity_scores(news_article)[\"compound\"]\n",
    "    \n",
    "    #quality of writing prediction\n",
    "    words = news_article.split()\n",
    "    irisa_qor_ratio = len(set(words)) / len(words)\n",
    "    \n",
    "    #sensationalism\n",
    "    irisa_sensationalism = count_adjectives(news_article)\n",
    "    \n",
    "    #adding to df for prediction\n",
    "    irisa_data = {\n",
    "        \"sentiment\": [irisa_sentiment],\n",
    "        \"ttr\": [irisa_qor_ratio],\n",
    "        \"adjectives\": [irisa_sensationalism]\n",
    "    }\n",
    "    \n",
    "    irisa_pred_df = pd.DataFrame(irisa_data)\n",
    "    \n",
    "    #irisa final prediction\n",
    "    irisa_final_prediction = irisa_clf.predict(irisa_pred_df)[0]\n",
    "\n",
    "    #irisas prediction percentile\n",
    "    sentiment_percentile = percentileofscore(irisa_X_train['sentiment'], irisa_pred_df['sentiment'])[0]\n",
    "    ttr_percentile = percentileofscore(irisa_X_train['ttr'], irisa_pred_df['ttr'])[0]\n",
    "    adjectives_percentile = percentileofscore(irisa_X_train['adjectives'], irisa_pred_df['adjectives'])[0]\n",
    "\n",
    "    #LOHITS PREDICTIONS\n",
    "    X_test_instance = [headline + \" \" + news_article]\n",
    "\n",
    "    # Vectorize the test instance using the same TF-IDF vectorizer trained on the training data\n",
    "    X_test_instance_tfidf = tfidf.transform(X_test_instance)\n",
    "    \n",
    "    # Make predictions for the test instance\n",
    "    y_pred_instance = classifier.predict(X_test_instance_tfidf)\n",
    "    \n",
    "    y_pred_proba = classifier.predict_proba(X_test_instance_tfidf)\n",
    "    positive_class_proba = y_pred_proba\n",
    "    overall_score = (positive_class_proba[0][0] * 0.2) + (positive_class_proba[0][1] * 1) + (positive_class_proba[0][2] * 0.4) + (positive_class_proba[0][3] * 0.6) + (positive_class_proba[0][4] * 0.8) + (positive_class_proba[0][5] * 0.0)\n",
    "    lohit_final_prediction = predict_label(overall_score)\n",
    "\n",
    "    #NICKS PREDICTIONS\n",
    "    #readability\n",
    "    r = Readability(news_article)\n",
    "    fk = r.flesch_kincaid()\n",
    "    flesch_score = fk.score\n",
    "    if flesch_score > 12:\n",
    "        diff = flesch_score - 12\n",
    "        fk_rating = 100 - (diff * 10)\n",
    "    elif flesch_score < 8:\n",
    "        diff = 8 - flesch_score\n",
    "        fk_rating = 100 - (diff * 10)\n",
    "    else:\n",
    "        fk_rating = 100\n",
    "\n",
    "    #sentiment\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    moving_sentiment_value = 0\n",
    "    number_of_paragraphs = 0\n",
    "    paragraphs = news_article.split('\\n\\n')\n",
    "    for i in paragraphs:\n",
    "        cleaned_text = ' '.join(i.split()).replace(\"\\'\", '')\n",
    "        compound_sentiment_score = sia.polarity_scores(cleaned_text)['compound']\n",
    "        moving_sentiment_value += compound_sentiment_score\n",
    "        number_of_paragraphs += 1\n",
    "    overall_sentiment = moving_sentiment_value / number_of_paragraphs\n",
    "    overall_sent_score = 100\n",
    "    if overall_sentiment < -0.2:\n",
    "        overall_sent_score = 100 + (overall_sentiment * 100)\n",
    "\n",
    "    #clickbait\n",
    "    if len(headline) > 0:\n",
    "        article_title_processed = preprocess_text(headline)\n",
    "        article_title_vectorized = count_vectorizer.transform([article_title_processed])\n",
    "        clickbait_probability = clf_best.predict_proba(article_title_vectorized)\n",
    "        confidence_not_clickbait = clickbait_probability[:, 0]\n",
    "        confidence_not_clickbait = confidence_not_clickbait[0]\n",
    "        nick_predicted_label = predict_label(confidence_not_clickbait)\n",
    "    else:\n",
    "        nick_predicted_label = 0\n",
    "\n",
    "    #HENRYS PREDICTIONS\n",
    "    # Tokenize the single text example\n",
    "    tokenized_news = tokenizer(news, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_henry = model_.to(device)\n",
    "    \n",
    "    # Move tokenized example to GPU\n",
    "    tokenized_news_gpu = tokenized_news.to(device)\n",
    "    \n",
    "    # Extract BERT embeddings for the tokenized example\n",
    "    with torch.no_grad():\n",
    "        model_henry.eval()\n",
    "        statement_embedding = model_henry(**tokenized_news_gpu).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    \n",
    "    # Use the RandomForestClassifier to predict the label for the single text example\n",
    "    y_pred_news = rf_classifier.predict(statement_embedding)\n",
    "    \n",
    "    henry_final_prediction = y_pred_news[0]\n",
    "\n",
    "    #GEN AI\n",
    "    #instantiating gemini pro model\n",
    "    PROJECT_ID = \"gen-lang-client-0321728687\"\n",
    "    REGION = \"us-central1\"\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "    model = generative_models.GenerativeModel(\"gemini-pro\")\n",
    "    config = {\"max_output_tokens\": 2048, \"temperature\": 0.0}\n",
    "    \n",
    "    safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    }\n",
    "    chat = model.start_chat()\n",
    "\n",
    "    #PerspectiveAPI output check instantiation\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=PERSPECTIVE_API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "        )\n",
    "    \n",
    "    #chunking news article for improved processing\n",
    "    chunked_article_list = tokenize_into_chunks(news_article, 50)\n",
    "    \n",
    "    #getting context and fact checks from vector database based on the provided input\n",
    "    all_response_text = []\n",
    "    context_list = []\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        context = RAG_CONTEXT_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        context_list.append(context)\n",
    "        \n",
    "    fact_checks_list=[]\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        fact_checks_list.append(fact_checks)\n",
    "\n",
    "    #creating history list so that gen ai model has additional context when analyzing chunked statements \n",
    "    for i in range(len(context_list)):\n",
    "        input=chunked_article_list[i]\n",
    "        fact_checks = fact_checks_list[i]\n",
    "        context = context_list[i]\n",
    "        prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "        next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "        \n",
    "        history = [prev_chunk, input, next_chunk]\n",
    "        \n",
    "        #re-ranking RAG results for fact check statements from RAG_STATEMENTS_VDB\n",
    "        statement_rerank_list = []\n",
    "        for j in range(len(fact_checks['ids'][0])):\n",
    "            reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "            statement_rerank_list.append(reranking_statementSearch)\n",
    "    \n",
    "        \n",
    "        scores = reranker.compute_score(statement_rerank_list)\n",
    "        combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "        #re-ranking RAG results for context statements from RAG_CONTEXT_VDB\n",
    "        context_rerank_list = []\n",
    "        for k in range(len(context['ids'][0])):\n",
    "            reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "            context_rerank_list.append(reranking_contextSearch)\n",
    "            \n",
    "        scores = reranker.compute_score(context_rerank_list)\n",
    "        combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "\n",
    "        #getting top 3 most relevant pieces of context and fact checks from RAG\n",
    "        context_window = 3\n",
    "        prepared_context = []\n",
    "        prepared_fact_checks = []\n",
    "        for i in range(context_window):\n",
    "            prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "            prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])\n",
    "\n",
    "        #Changing chunks from list of strings to one combined string for Gen AI processing\n",
    "        chunk_history_string = ''\n",
    "        for chunk in history:\n",
    "            if chunk != None:\n",
    "                chunk_history_string += chunk + \" \"\n",
    "\n",
    "\n",
    "        #generating initial response with prompt template\n",
    "        responses = model.generate_content(f\"\"\"Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "        knowledge you already have access to answer the question.\n",
    "    \n",
    "        If you are not very sure of your answer to the question, then use the additional information I've provided below within the \n",
    "        ((())) symbols to help you.\n",
    "        (((\n",
    "        Refer to these fact checked statements as well to determine your answer and be sure to pay close attention to the \n",
    "        metadata that is provided: {prepared_fact_checks}.\n",
    "        Use the following context to help answer the question: {prepared_context}.\n",
    "        You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "        Make sure you provide a short explanation of why you chose that score.\n",
    "        )))\n",
    "        <<<\n",
    "        Question: How true is the following statement on a scale of 1-100? + {input}. You must provide the score in this format Score:XX., \n",
    "        followed by your short explanation.\n",
    "        >>>\n",
    "       \"\"\",\n",
    "            generation_config=config,\n",
    "            stream=True,\n",
    "            safety_settings=safety_config,                          \n",
    "        )\n",
    "\n",
    "        \n",
    "        #obtaining individual responses\n",
    "        response_text = \"\"\n",
    "        response_text += \"Statement: \" + input\n",
    "        for response in responses:\n",
    "            try:\n",
    "                response_text += response.text\n",
    "            except (IndexError, ValueError) as e:\n",
    "                continue\n",
    "        response_text = response_text.replace(\"\\n\\n\", \". \")\n",
    "        all_response_text.append(response_text)\n",
    "        \n",
    "                                  \n",
    "    #combining all responses    \n",
    "    entire_text_string = \"\"\n",
    "    for text in all_response_text:\n",
    "        entire_text_string += text\n",
    "    cleaned_text = entire_text_string\n",
    "    \n",
    "    #this section is finding and removing the statements that can't be rated by the chatbot\n",
    "    unratable_sentences = []\n",
    "    rated_sentences = []\n",
    "\n",
    "    final_text = \"\"\n",
    "    for response in all_response_text:\n",
    "        if \"article does not\" in response.lower() or \"context does not\" in response.lower() or \"statement is not\" in response.lower() or \"Statement: Score\" in response:\n",
    "            unratable_sentences.append(response)\n",
    "        else:\n",
    "            rated_sentences.append(response)\n",
    "            final_text += response\n",
    "    \n",
    "    not_enough_context = len(unratable_sentences)\n",
    "    enough_context = len(rated_sentences)\n",
    "    all_statements_count = len(all_response_text)\n",
    "\n",
    "\n",
    "    #total score calculation with regex\n",
    "    pattern = r'Score:\\s(\\d+)\\.'\n",
    "    total_score = 0\n",
    "    matches = re.findall(pattern, cleaned_text)\n",
    "    for match in matches:\n",
    "        score = int(match)\n",
    "        total_score += score\n",
    "    if enough_context == 0:\n",
    "        average_score = 0\n",
    "    else:\n",
    "        average_score = total_score / enough_context\n",
    "    rounded_average = round(average_score, 1)\n",
    "\n",
    "    #creating output in nice format for user\n",
    "    output_intro = f\"\"\"{enough_context} out of {all_statements_count} statements in the text could be rated. The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {rounded_average}/100. Some of the lowest rated statements are provided below.\"\"\"\n",
    "    tweaking_output = re.sub(r'(Score:\\s*\\d+\\.)(?!\\s*Explanation:)', r'\\1 Explanation:', final_text)\n",
    "    parts = re.split(r\"(?=Statement:)\", tweaking_output)\n",
    "    split_parts=[]\n",
    "    # Clean each part and add to split_parts\n",
    "    for part in parts:\n",
    "        cleaned_part = output_clean(part)\n",
    "        split_parts.append(cleaned_part)\n",
    "    \n",
    "    # Initialize variables to store lowest scores and their respective entries\n",
    "    lowest_scores = [(float('inf'), ''), (float('inf'), ''), (float('inf'), '')]\n",
    "    \n",
    "    # Iterate through each string entry\n",
    "    for entry in split_parts:\n",
    "        # Find all occurrences of \"Score: \" followed by a number until a \".\"\n",
    "        scores = re.findall(r' Score:\\s(\\d+)\\.', entry)\n",
    "        # Convert scores to integers and update lowest_scores if necessary\n",
    "        for score in scores:\n",
    "            score_int = int(score)\n",
    "            if score_int < lowest_scores[-1][0]:\n",
    "                lowest_scores[-1] = (score_int, entry)\n",
    "                lowest_scores.sort()\n",
    "                \n",
    "    # Extract the entries for the three lowest scores\n",
    "    lowest_entries = [entry for score, entry in lowest_scores]\n",
    "\n",
    "    #reformatting for better readability\n",
    "    summary_output = \"\"\n",
    "    for statement in lowest_entries:\n",
    "        # Replace \"Statement:\", \"Score:\", and \"Explanation:\" with a new line followed by the keyword\n",
    "        formatted_statement = re.sub(r'(Statement:|Score:|Explanation:)', r'\\n\\1', statement)\n",
    "        # Append the formatted statement to the output\n",
    "        summary_output += formatted_statement.strip() + \"\\n\"\n",
    "    \n",
    "        # Add a new line after each statement\n",
    "        summary_output += \"\\n\"\n",
    "\n",
    "    output = output_intro + \"\\n\\n\" + summary_output\n",
    "\n",
    "    #Perspective API output safety check\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': output},\n",
    "      'requestedAttributes': requestedAttributes\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    \n",
    "    attributes_surpassed = []\n",
    "    for key in response['attributeScores']:\n",
    "        if response['attributeScores'][key]['summaryScore']['value'] > attributeThresholds[key]:\n",
    "            attributes_surpassed.append((key, response['attributeScores'][key]['summaryScore']['value']))\n",
    "    \n",
    "    #crafting output warning message if necessary or regular output message  \n",
    "    \n",
    "    if len(attributes_surpassed) == 1:\n",
    "        attributes_violated = \"\"\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}category so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "        \n",
    "    elif len(attributes_surpassed) > 1:\n",
    "        attributes_violated = \"\"\n",
    "        counter = 1\n",
    "        attributes_count = len(attributes_surpassed)\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "            if counter < attributes_count:\n",
    "                attributes_violated += \"and \"\n",
    "            counter += 1\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}categories so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "\n",
    "    else:\n",
    "        history_output.append([news_article, output])\n",
    "    return history_output, history_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386c489-c97b-4066-95bd-e92bfa665554",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Article testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "65d2ba84-c597-4088-a7d4-d084ada7eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model generation for output to user\n",
    "    # final_responses = model.generate_content(f\"\"\"Each entry in the list of statements provided below inside <<<>>> begins with a number\n",
    "    # that explains how truthful a statement is and is followed by a text explanation to why that score was chosen. I need you to provide\n",
    "    # explanations for each statement on why they received the scores they did. \n",
    "    # I would like you to format your response like this, and continue to follow it for each statement you choose to include.\n",
    "    \n",
    "    # \"{enough_context} out of {all_statements_count} statements in the text could be rated. \n",
    "    # The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {overall_score}. Some of the lowest rated statements are provided below\"\n",
    "    \n",
    "    # \\nScore: XX\n",
    "    # Statement: \"Statement here\"\n",
    "    # Explanation: \"Explanation here\"\n",
    "\n",
    "    # # <<<\n",
    "    # # {rated_sentences}\n",
    "    # # >>>\"\"\",\n",
    "    #     generation_config=config,\n",
    "    #     stream=True,\n",
    "    #     safety_settings=safety_config,\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # final_response_text = \"\"\n",
    "    # for response in final_responses:\n",
    "    #     final_response_text += response.text\n",
    "    # output = final_response_text.replace(\"\\n\\n\", \". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "acc812f6-2d3a-431b-b579-26f4877dcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"Months after leaving the White House, former President Donald Trump began plotting his return to Wall Street. That return, delayed by years of regulatory and legal hurdles, is now on the verge of becoming a reality — and it could make Trump a fortune.\n",
    "\n",
    "US regulators have finally given the green light to a controversial merger between Truth Social owner Trump Media & Technology Group and a blank-check company. The blessing from the Securities and Exchange Commission removes the last major obstacle holding back the deal.\n",
    "\n",
    "The merger, if approved by shareholders, would pave the way for Trump Media to become a publicly-traded company — one where Trump will own a dominant stake that could be worth billions.\n",
    "\n",
    "Digital World Acquisition Corp., the blank-check firm, announced that on Wednesday the SEC signed off on the merger proxy for the deal. A date for a shareholder vote will be set by Friday.\n",
    "\n",
    "“It does look like this deal is going to reach the finish line now — after more than two years of delays,” said Jay Ritter, a finance professor at the University of Florida.\n",
    "\n",
    "Trump stake could be worth $4 billion\n",
    "Shares of Digital World, a special purpose acquisition company, or SPAC, spiked 15% on the major milestone. The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress.\n",
    "\n",
    "Ritter estimates the merger could pave the way for about $270 million of cash coming into Trump Media, funds the company could fuel Truth Social’s growth.\n",
    "\n",
    "Trump is set to hold a dominant position in the newly-combined company, owning roughly 79 million shares, according to new SEC filings.\n",
    "\n",
    "The former president’s stake would be valued at $4 billion based on Digital World’s current trading price of about $50.\n",
    "\n",
    "Of course, as Ritter notes, it would be very difficult for Trump to translate that paper wealth into actual cash.\n",
    "\n",
    "Not only would Trump be subject to a lock-up period that would prevent he and other insiders from selling until six months after the merger, but the new company’s fortunes would be closely associated with the former president. That could make it difficult for Trump to sell even after the lock-up period expires.\n",
    "\n",
    "‘This is a meme stock’\n",
    "Moreover, there are major questions about the sky-high valuation being placed on this media company.\n",
    "\n",
    "“This is a meme stock. The valuation is totally divorced from the fundamental value of the company,” said Ritter.\n",
    "\n",
    "Digital World’s share price values the company at up to about $8 billion on a fully diluted basis, which includes all shares and options that could be converted to common stock, according to Ritter.\n",
    "\n",
    "He described that valuation as “crazy” because Trump Media is generating little revenue and burning through cash.\n",
    "\n",
    "New SEC filings indicate Trump Media’s revenue amounted to just $1.1 million during the third quarter. The company posted a loss of $26 million.\n",
    "\n",
    "Since the merger was first proposed in October 2021, legal, regulatory and financial questions have swirled about the transaction.\n",
    "\n",
    "In November, accountants warned that Trump Media was burning cash so rapidly that it might not survive unless the long-delayed merger with Digital World is completed soon.\n",
    "\n",
    "Shareholder vote looms\n",
    "Now, Trump execs are cheering the green light from the SEC.\n",
    "\n",
    "“Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. “Moving forward, we aim to accelerate our work to build a free speech highway outside the stifling stranglehold of Big Tech.”\n",
    "\n",
    "Eric Swider, Digital World’s CEO, described the SEC approval as a “significant milestone” and said executives are “immensely proud of the strides we’ve taken towards advancing” the merger.\n",
    "\n",
    "One of the final remaining hurdles is for Digital World shareholders to approve the merger in an upcoming vote.\n",
    "\n",
    "The shareholders have enormous incentive to approve the deal because if the merger fails, the blank-check firm would be forced to liquidate. That would leave shareholders with just $10 a share, compared with $50 in the market today.\n",
    "\n",
    "“Anyone who holds shares and votes against the merger is crazy,” said Ritter, the professor.\n",
    "\n",
    "“Then again, I might argue that everyone holding DWAC shares is crazy,” he added, referring to the company’s thin revenue and hefty valuation.\n",
    "\n",
    "Matthew Tuttle, CEO of Tuttle Capital Management, said he’s not surprised by the ups and downs surrounding this merger.\n",
    "\n",
    "“The thing about Trump and anything related to Trump is, love him or hate him, there is going to be drama,” said Tuttle, who purchased options to buy Digital World shares in his personal account. “Really, I would not have expected anything less.”\n",
    "\n",
    "Going forward, Tuttle said Trump Media’s share price will live and die by how everything plays out for Trump personally — from his legal troubles to his potential return to the White House.\n",
    "\n",
    "“Anything bullish for Trump is going to be bullish for the stock,” said Tuttle.\n",
    "\n",
    "Trump is no stranger to Wall Street, where he has a history, one marked by bankruptcies.\n",
    "\n",
    "Although Trump has never filed for personal bankruptcy, he has filed four business bankruptcies — all of them linked to casinos he used to own in Atlantic City.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "41149610-953a-4b48-a447-207b45c26035",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Are Americans paying nearly $500 for an inhaler that would cost just $7 overseas?\n",
    "\n",
    "U.S. Sen. Tammy Baldwin, D-Wis., says there is a vast difference in the cost of prescriptions in the United States and the rest of the world. \n",
    "\n",
    "\"Big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US,\" Baldwin said Feb. 1 in a Facebook post. \"That has got to end. We've got to hold Big Pharma accountable for their price-gouging tactics. I won't stop fighting until we do.\"\n",
    "\n",
    "That massive cost difference piqued our interest.\n",
    "\n",
    "How much would patients pay? \n",
    "When we asked for backup information, Baldwin’s campaign staff directed us to drug pricing websites, news articles and news releases on the cost of Combivent Respimat (ipratropium bromide and albuterol), a combination medication used to treat chronic obstructive pulmonary disease (COPD). \n",
    "\n",
    "Combivent Respimat is available only as a brand-name medication and not available in generic form, according to Medical News Today, which pointed out that the actual price a patient would pay for the medication depends on type of insurance plan, location and pricing at the patient’s pharmacy. Medicare does cover Combivent Respimat. \n",
    "\n",
    "According to Drugs.com, a pricing website, Combivent Respimat costs about $525 for a supply of 4 grams, depending on the pharmacy. \n",
    "\n",
    "It’s also important to note, that on a practical basis, because of insurance and Medicare coverage, few people in the United States would actually pay $500 out of pocket\n",
    "\n",
    "\"Quoted prices are for cash-paying customers and are not valid with insurance plans,\" the website says  says. \n",
    "\n",
    "Another online drug pricing guide, GoodRx, puts the price of Combivent Respimat between about $477 and $584 at Madison, Wisconsin, pharmacies:\n",
    "\n",
    "Walgreens —    $508.39 \n",
    "\n",
    "Walmart —----   $514.45\n",
    "\n",
    "CVS Pharmacy-$508.14\n",
    "\n",
    "Hy-Vee —--------$477.97\n",
    "\n",
    "Costco —---------$584.59\n",
    "\n",
    "Target —----------$508.14\n",
    "\n",
    "FEATURED FACT-CHECK\n",
    "\n",
    "Instagram posts\n",
    "stated on February 15, 2024 in an Instagram post\n",
    "Because “17 million immigrants” were “let in” the U.S, “ foot and mouth disease is back. We got rid of that fifty years ago.”\n",
    "truefalse\n",
    "By Jeff Cercone • February 16, 2024\n",
    "Metro Market —-$511.00\n",
    "\n",
    "Pick ’n Save—---$511.00\n",
    "\n",
    "So, Baldwin is on target on the cost in the US.\n",
    "\n",
    "What about overseas?\n",
    "According to a Jan. 8 news release from U.S. Sen. Bernie Sanders, I-Vt., Combivent Respimat sold for just $7 In France.\n",
    "\n",
    "Sanders, chairman of the Senate Committee on Health, Education, Labor, and Pensions, sent letters to the CEOs of four pharmaceutical companies announcing an investigation into the high prices the companies are charging for inhalers. Baldwin and Democratic Sens. Ben Ray Luján of New Mexico and Ed Markey of Massachusetts also signed the letters.\n",
    "\n",
    "The letters were sent to the four biggest manufacturers of inhalers sold in the United States — AstraZeneca, Boehringer Ingelheim, GlaxoSmithKline (GSK) and Teva.\n",
    "\n",
    "\"It is beyond absurd that Boehringer Ingelheim charges $489 for Combivent Respimat in the United States, but just $7 in France,\" Sanders said in the news release.\n",
    "\n",
    "The news release said the Committee’s source for the price of Combivent Respimat in France was the Navlin international drug pricing database. \n",
    "\n",
    "Baldwin, in the news release, accuses companies of \"jacking up prices and turning record profits.\"\n",
    "\n",
    "Experts weigh in \n",
    "Dr. William B. Feldman noted that Baldwin is referring to list prices here — which are the prices that uninsured patients in the U.S. pay and the prices to which out-of-pocket costs are often tied.\n",
    "\n",
    "\"Manufacturers give sizable (confidential) rebates to insurers, and so the net prices for inhalers in the U.S. are below list prices — but still much higher than the net prices abroad,\" Feldman said in an email to PolitiFact Wisconsin. \n",
    "\n",
    "Feldman, who works at Brigham and Women’s Hospital in Boston and Harvard Medical School, said a key reason inhaler prices remain so high in the U.S. is that there is very little generic competition. \n",
    "\n",
    "\"Brand-name manufacturers have erected large patent thickets that keep generic competitors off the market,\" Feldman said. \" Inhaler prices are low elsewhere, in part, because governments negotiate prices based on the value of the drugs compared to existing therapies.\"\n",
    "\n",
    "David Kreling, professor emeritus in the School of Pharmacy at the University of Wisconsin-Madison, said the U.S. price quoted by Baldwin sounds about right.\n",
    "\n",
    "\"The $500 number may be in the ballpark for U.S. patented (brand-name, newer) drugs,\" Kreling said in an email to PolitiFact Wisconsin. \"That would be consistent with my understanding of market data on sales by firms in the U.S. Things in the $7 range, here, only reside within the off-patent generic drug market (where we have low prices, sometimes at or near lowest in the world).\" \n",
    "\n",
    "Our ruling\n",
    "Baldwin said \"big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US.\"\n",
    "\n",
    "Our review, and that of experts, found the numbers checked out.\n",
    "\n",
    "Experts cite a variety of reasons for the price differences, including very little generic competition in the United States, and few people in the United States would actually pay $500 out of pocket because of insurance and Medicare coverage. \n",
    "\n",
    "For a statement that is accurate but needs clarification or additional information, our rating is Mostly True.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf46ad3-6be1-4403-bcbd-1da7bdf416e6",
   "metadata": {},
   "source": [
    "#### Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e96e64b1-270d-4bf3-b824-4b2bd8884e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "final_response = GenAI_article_truth_processing(news, [], [], title_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "60a4ebc6-39d0-407b-a053-192a25e6a49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"28 out of 28 statements in the text could be rated. The following score and explanation is based on these 28 statements. The average truthfulness score from these 28 statements is 84.1/100. Some of the lowest rated statements are provided below.\\n\\nStatement: “Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. \\nScore: 50. \\nExplanation: The statement is made by Devin Nunes, who has a history of pushing social media companies to restrict speech that he objects to. He has also filed several defamation lawsuits against media companies and critics. This suggests that he may not be genuinely committed to free expression.\\n\\nStatement: The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress. \\nScore: 70. \\nExplanation: The statement is mostly true because the stock has nearly tripled this year, and Trump's political success in the Republican presidential primary and the merger progress are likely contributing factors.\\n\\nStatement: “Then again, I might argue that everyone holding DWAC shares is crazy,” he added, referring to the company’s thin revenue and hefty valuation. \\nScore: 70. \\nExplanation: The statement is mostly true because the company's revenue is thin and its valuation is hefty.\\n\\n\""
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweaking_output = final_response[0][0][1]\n",
    "tweaking_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b58cae-e6ad-461f-b83b-dbb320f05ea0",
   "metadata": {},
   "source": [
    "#### Live News Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1538,
   "id": "2716a8d8-d050-47e9-89d9-bdf04e46bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr_headline, npr_content, npr_link = npr_updated_news()\n",
    "fox_headline, fox_content, fox_link = fox_updated_news()\n",
    "abc_headline, abc_content, abc_link = abc_updated_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "id": "c9120b6c-d6af-45e6-8de5-84792f5d507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr_output = GenAI_article_truth_processing(npr_content, [], [])\n",
    "npr_output = npr_output[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1549,
   "id": "624915fd-6fa7-41bf-b52c-3caf47143527",
   "metadata": {},
   "outputs": [],
   "source": [
    "npr_final_output = \"Here is a link to the analyzed article. \" + npr_link + \"\\n\" + npr_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1550,
   "id": "d62d33ba-0a9c-429b-a401-a424f74d5ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_output = GenAI_article_truth_processing(fox_content,[], [])\n",
    "fox_output = fox_output[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1551,
   "id": "6bcd86a7-9889-4c2e-a4ee-abb30acdbbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fox_final_output = \"Here is a link to the analyzed article. \" + fox_link + \"\\n\" + fox_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1552,
   "id": "d05f320e-ff14-489e-bf7e-e04fc53482b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_output = GenAI_article_truth_processing(abc_content,[], [])\n",
    "abc_output = abc_output[0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1553,
   "id": "f7f3cbb3-74f5-4f38-8d84-d4ee58118167",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_final_output = \"Here is a link to the analyzed article. \" + abc_link + \"\\n\" + abc_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019ce28-d054-4dc4-be10-dd967f3ca57e",
   "metadata": {},
   "source": [
    "## Gradio (Website) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cc37feb9-8a60-49e8-b852-e0c57e2c22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7401e9a3-5946-4525-822d-a00f932f5cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenAI_article_truth_processing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     state \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mState()\n\u001b[1;32m     12\u001b[0m     submit \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mButton(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEND\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     submit\u001b[38;5;241m.\u001b[39mclick(\u001b[43mGenAI_article_truth_processing\u001b[49m, inputs\u001b[38;5;241m=\u001b[39m[message, state, examples, headline], outputs\u001b[38;5;241m=\u001b[39m[chatbot, state])\n\u001b[1;32m     14\u001b[0m     submit\u001b[38;5;241m.\u001b[39mclick(make_plot, inputs\u001b[38;5;241m=\u001b[39m[message], outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mPlot())\n\u001b[1;32m     15\u001b[0m block\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, share_server_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisinformation-destroyers.com:7000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GenAI_article_truth_processing' is not defined"
     ]
    }
   ],
   "source": [
    "block = gr.Blocks()\n",
    "prompt_placeholder = \"Insert your news article here!\"\n",
    "headline_placeholder = \"Paste your news articles headline here!\"\n",
    "with block:\n",
    "    gr.Markdown(\"\"\"<h1><center>Generative AI News Article Truthfulness Evaluator</center></h1>\n",
    "    \"\"\")\n",
    "    examples = gr.Dropdown([\"ABC\", \"NPR\", \"FOX\"], label=\"News Provider\", info=\"Take any of the news providers in the dropdown below and type the name of the provider exactly how you see it into the Textbox below to get an up-to-date example articles evaluation!\")\n",
    "    message = gr.Textbox(placeholder=prompt_placeholder, info=\"Paste your own news article here, or type in one of the news providers in the dropdown above.\")\n",
    "    headline = gr.Textbox(placeholder=headline_placeholder, info=\"Paste your news articles headline here if available.\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State()\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    submit.click(GenAI_article_truth_processing, inputs=[message, state, examples, headline], outputs=[chatbot, state])\n",
    "    submit.click(make_plot, inputs=[message], outputs=gr.Plot())\n",
    "block.launch(share=True, share_server_address=\"disinformation-destroyers.com:7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019beb2b-50d6-45d9-9656-3007f7a30db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
