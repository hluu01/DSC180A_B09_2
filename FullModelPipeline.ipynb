{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "3a22c4dd-7522-42d0-aee0-9dd287628ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import chromadb\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310d4cb-a74a-4de1-af37-c9f5343d42b6",
   "metadata": {},
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16831eb1-a5ff-415d-9469-5a58b6ad59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_strings(text):\n",
    "    return '' if len(text) < 7 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66a2d7b-8ec1-4192-8ffe-9c9acacfe19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d190ec3-4e6c-4408-b3bc-5dc908d8861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_chunks(text, min_words=75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        if len(current_chunk) + len(words) < min_words:\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if any(sentence.endswith(p) for p in ['.', '!', '?', '¡', '¿']):\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = words\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "e23075dc-4ef8-4d14-af84-d27004a90796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_clean(text):\n",
    "    # Remove spaces before or after \"'\" mark\n",
    "    text = text.replace(\" '\", \"'\").replace(\"' \", \"'\")\n",
    "    # Remove white space before \",\"\n",
    "    text = text.replace(\" ,\", \",\")\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    # Remove white space before or after \"”\" or \"“\" character\n",
    "    text = text.replace(\"“ \", \"“\").replace(\" ”\", \"”\")\n",
    "    text = text.replace(\"Score:\", \" Score:\")\n",
    "    text = text.replace(\" ’\", \"’\").replace(\"’ \", \"’\")\n",
    "    text = text.replace(\"$ \", \"$\")\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "id": "9c575404-dc61-4b43-b58b-5596fc718658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(input):\n",
    "    truth_scores = predict_tabular_classification_sample(project=\"dsc-180a-b09\",\n",
    "                                                         endpoint_id=\"4607809140427849728\",\n",
    "                                                         instance_dict={\"article\": input})\n",
    "    \n",
    "    reordered_indices = [truth_scores[0]['classes'].index(c) for c in ['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true']]\n",
    "    classes_reordered = [truth_scores[0]['classes'][i] for i in reordered_indices]\n",
    "    scores_reordered = [truth_scores[0]['scores'][i] for i in reordered_indices]\n",
    "    \n",
    "    # Define color transition from red to green\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0, 1, len(classes_reordered)))\n",
    "    # Plot the bar chart with color transition\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bars = plt.bar(classes_reordered, scores_reordered, color=colors)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Predictive Auto ML Truthfulness Scores')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Scores')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff905ef1-f846-42f4-9454-54747f87215d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c04f40a-88a8-4176-b467-cc51767e3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact articles\n",
    "pf_articles = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_articles.csv\")\n",
    "pf_articles = pf_articles.drop(columns='Unnamed: 0')\n",
    "pf_articles.rename(columns={'Statement': 'Title'}, inplace=True)\n",
    "pf_articles = pf_articles.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd711031-7b63-4af5-a8d2-e79755f84199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact truth datasets\n",
    "pf_statements = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_truthometer_df.csv\")\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0')\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0.1')\n",
    "pf_statements = pf_statements.dropna()\n",
    "pf_statements_full = pf_statements\n",
    "pf_statements = pf_statements.sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fb33555-68fa-4d47-9ec1-44a8184ac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factcheckorg_articles = pd.read_csv(\"Data/FactCheckOrg/factcheckorg_webscrape_200pages.csv\")\n",
    "factcheckorg_articles['List_data'].fillna('', inplace=True)\n",
    "factcheckorg_articles['List_data'] = factcheckorg_articles['List_data'].apply(filter_short_strings)\n",
    "factcheckorg_articles = factcheckorg_articles.dropna(subset=['Text'])\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace('Para leer en español, vea esta traducción de Google Translate.', '')\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace(r' Editor’s Note:.*$', '', regex=True)\n",
    "factcheckorg_articles = factcheckorg_articles.reset_index()\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['index'])\n",
    "factcheckorg_articles['Title_and_Date'] = factcheckorg_articles['Title'] + ' , ' + factcheckorg_articles['Date']\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['Title', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea624713-513c-49a2-978f-37efef9e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciencefeedbackorg_articles = pd.read_csv(\"Data/ScienceFeedbackOrg/ScienceFeedbackOrg.csv\")\n",
    "sciencefeedbackorg_articles = sciencefeedbackorg_articles.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2ea378e-cae2-4d27-805d-491e0f1ac8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scicheckorg_articles = pd.read_csv(\"Data/FactCheckOrg/scicheck_data.csv\")\n",
    "scicheckorg_articles['Title_and_Date'] = scicheckorg_articles['Title'] + ' , ' + scicheckorg_articles['Date']\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['Title', 'Date'])\n",
    "scicheckorg_articles.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced89dd-106a-4d60-adb3-a000944d564e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85e0467-d390-42b5-80fa-15c959c0fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#FactCheckOrg Article Chunking\n",
    "factcheckorg_articles['chunks_text'] = factcheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "factcheckorg_articles['chunkslistdata'] = factcheckorg_articles['List_data'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = factcheckorg_articles['chunks_text'].apply(len).max()\n",
    "max_chunks_list_data = factcheckorg_articles['chunkslistdata'].apply(len).max()\n",
    "max_total_chunks = max(max_chunks_text, max_chunks_list_data)\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_total_chunks + 1):\n",
    "    factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "    factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['chunks_text', 'chunkslistdata', 'Text', 'List_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d04d655-1dd9-4e78-9104-3b41b6bea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Statement Text Chunking\n",
    "pf_statements['chunks'] = pf_statements['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_statements['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_statements[f'chunk_{i}'] = pf_statements['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_statements = pf_statements.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9383641a-2e4c-45a3-b363-95aba9f5de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#Politifact Articles Chunking\n",
    "pf_articles['chunks'] = pf_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_articles['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_articles = pf_articles.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2fe76bdb-2700-4532-9380-1daec03e19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#SciCheckOrg Articles Chunking\n",
    "scicheckorg_articles['chunks_text'] = scicheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = scicheckorg_articles['chunks_text'].apply(len).max()\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_chunks_text + 1):\n",
    "    scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['chunks_text', 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fe66e-bac9-461d-a7e2-a4311b23adec",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b31956-0dbb-4bf3-8b2c-b7df2e88b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a961bb-41c9-4e28-bb96-360fbca000af",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_CONTEXT_VDB = chroma_client.create_collection(name=\"RAG_CONTEXT_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72fc7d10-34c5-4142-9965-9a674f004bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_STATEMENTS_VDB = chroma_client.create_collection(name=\"RAG_STATEMENTS_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40ea61f4-0dfe-4b95-9b43-46d3687fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding pf statement justifications to Context VDB\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "chunks_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements.iterrows():\n",
    "    statement = row['Statement']\n",
    "    claimer = row['Claimer']\n",
    "    for col in pf_statements.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                metadata_list.append({\"Statement\": statement, \"Context\": \"Yes\", \"Claimer\": claimer})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad55249-93cb-4196-8111-00946af61458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter justifications to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fce4111-b9e1-4692-b32e-d23ed3930d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding politifact truth-o-meter statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements_full.iterrows():\n",
    "    truth_value = row['Truth_value']\n",
    "    claimer = row['Claimer']\n",
    "    statement = row['Statement']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value,\"Claimer\": claimer})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afc8f317-5f3b-4ea7-81a1-fee50c26be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c059fb88-3459-455a-9d21-33b62b13b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding factcheck.org data to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in factcheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in factcheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1\n",
    "        elif col.startswith('chunklist'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44369d55-9b8a-4e94-b4f4-876ff2e9bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "#Adding factcheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6af85c54-a3a7-4588-8d13-834688bc1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding SciCheckOrg articles to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in scicheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in scicheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4b02868-a4e4-4db1-8264-8adc05216e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Adding scicheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c40685c-37f7-4f1a-8bb2-d0780a3c343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding ScienceFeedbackOrg statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in sciencefeedbackorg_articles.iterrows():\n",
    "    truth_value = row['label']\n",
    "    statement = row['claim']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13af0dcf-fc27-446e-a249-d5869954d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf sciencefeedback statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337cf1-74e2-4e5a-ba6b-1f9917e09d02",
   "metadata": {},
   "source": [
    "## FULL GEN AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b44f93a4-9895-4d5e-aeed-8dd8e2b5689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from langchain.vectorstores import Chroma\n",
    "from vertexai.preview import generative_models\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4e77-5130-4be5-8292-aaebc511f392",
   "metadata": {},
   "source": [
    "#### Perspective API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2f83baf7-d65d-42fd-81f3-c945bf22b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSPECTIVE_API_KEY = 'AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5c58d5c6-946d-422f-84b3-1df34ddc35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds of output\n",
    "attributeThresholds = {\n",
    "    'INSULT': 0.8,\n",
    "    'TOXICITY': 0.8,\n",
    "    'THREAT': 0.5,\n",
    "    'SEXUALLY_EXPLICIT': 0.5,\n",
    "    'PROFANITY': 0.8\n",
    "}\n",
    "requestedAttributes = {}\n",
    "for key in attributeThresholds:\n",
    "    requestedAttributes[key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359fa40-6fc2-4c82-961c-3d800bdb28da",
   "metadata": {},
   "source": [
    "### Liar Liar Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6d52eb5a-93b3-4b58-8060-8c878ad4c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_liar_plus = pd.read_csv(\"Data/Liar_plus/train.tsv\", delimiter='\\t', header=None)\n",
    "liar_liar_plus = liar_liar_plus[[3, 2]]\n",
    "liar_liar_plus.dropna(inplace=True)\n",
    "llp_statements = liar_liar_plus[3]\n",
    "llp_labels = liar_liar_plus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "53e04697-0e2f-44df-b892-dd286e4af9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Says the Annies List political group supports ...\n",
       "1        When did the decline of coal start? It started...\n",
       "2        Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        Health care reform legislation is likely to ma...\n",
       "4        The economic turnaround started at the end of ...\n",
       "                               ...                        \n",
       "10237    There are a larger number of shark attacks in ...\n",
       "10238    Democrats have now become the party of the [At...\n",
       "10239    Says an alternative to Social Security that op...\n",
       "10240    On lifting the U.S. Cuban embargo and allowing...\n",
       "10241    The Department of Veterans Affairs has a manua...\n",
       "Name: 3, Length: 10240, dtype: object"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llp_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d8e76cef-2794-4659-8c98-02655571549b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[373], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m llp_statements:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[43mGenAI_article_truth_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      6\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[342], line 156\u001b[0m, in \u001b[0;36mGenAI_article_truth_processing\u001b[0;34m(news_article, history)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#Perspective API output safety check\u001b[39;00m\n\u001b[1;32m    152\u001b[0m analyze_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: output},\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequestedAttributes\u001b[39m\u001b[38;5;124m'\u001b[39m: requestedAttributes\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 156\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalyze_request\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m attributes_surpassed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributeScores\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">"
     ]
    }
   ],
   "source": [
    "label_prediction = []\n",
    "for i in llp_statements:\n",
    "    try:\n",
    "        label_prediction.append(GenAI_article_truth_processing(i,[])[0][0][1])\n",
    "    except (ValueError, IndexError) as e:\n",
    "        label_prediction.append(None)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "219d4d6c-dc30-45bb-af12-6338a7f31bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_prediction)):\n",
    "    # Check if the current value matches the target value\n",
    "    if label_prediction[i] == 'False':\n",
    "        label_prediction[i] = 'false'\n",
    "    elif label_prediction[i] == 'Half-true':\n",
    "        label_prediction[i] = 'half-true'\n",
    "    elif label_prediction[i] == 'Mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'Mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'True':\n",
    "        label_prediction[i] = 'true'\n",
    "    elif label_prediction[i] == 'mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'mostly-true, mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'Pants-on-fire':\n",
    "        label_prediction[i] = 'pants-fire'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1cf10b70-cc1a-46b1-89c6-73262048f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'true'}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_llp_labels = set(llp_labels)\n",
    "unique_llp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a17b1a74-6c21-4dd5-b1c1-17ca0ff37383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Barely true',\n",
       " 'Barely-true',\n",
       " 'Half True',\n",
       " 'Half-True',\n",
       " 'Mostly False',\n",
       " 'Mostly True',\n",
       " 'Mostly-False',\n",
       " 'Mostly-True',\n",
       " None,\n",
       " 'None provided',\n",
       " \"['half-true', 'false']\",\n",
       " \"['half-true', 'mostly-true']\",\n",
       " \"['mostly-true', 'false']\",\n",
       " \"['true', 'false', 'mostly-true', 'mostly-true', 'half-true', 'true', 'half-true', 'half-true', 'mostly-true', 'mostly-true', 'pants-on-fire', 'pants-on-fire', 'mostly-true', 'true']\",\n",
       " \"['true', 'mostly-true', 'mostly-true']\",\n",
       " \"['true', 'mostly-true']\",\n",
       " 'barely-true',\n",
       " 'false',\n",
       " 'half-true',\n",
       " 'half-true\\nmostly-true',\n",
       " 'half-true, false',\n",
       " 'half-true, mostly-true',\n",
       " 'half-true, pants-on-fire',\n",
       " 'half-true, true',\n",
       " 'mostly-false, mostly-true',\n",
       " 'mostly-true',\n",
       " 'mostly-true, half-true',\n",
       " 'mostly-true, mostly-false',\n",
       " 'mostly-true, true',\n",
       " 'pants-fire',\n",
       " 'pants-on-fire',\n",
       " 'pants-on-fire, half-true',\n",
       " 'pants-on-fire, true',\n",
       " 'true',\n",
       " 'true, mostly-true'}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label_predictions = set(label_prediction)\n",
    "unique_label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "62ab7a55-081a-4eec-a10a-3d5dc8f96d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matches = 0\n",
    "for value1, value2 in zip(label_prediction, llp_labels):\n",
    "    if value1 == value2:\n",
    "        num_matches += 1\n",
    "\n",
    "accuracy = num_matches / len(label_prediction) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a62e276a-a999-4e48-9a58-76ed0c9c69c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.44951140065146"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d0be6-c7ef-481e-b3da-dd323a9ef99f",
   "metadata": {},
   "source": [
    "## FlagEmbedding API implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "876b0001-2280-44d3-8fbe-c85337ad6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FlagEmbedding\n",
      "  Downloading FlagEmbedding-1.2.5.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.33.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (4.35.2)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.15.0)\n",
      "Collecting accelerate>=0.20.1\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence_transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (23.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.4.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (2023.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (3.8.6)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (1.3.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (0.1.99)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.3)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers->FlagEmbedding) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence_transformers->FlagEmbedding) (10.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->FlagEmbedding) (1.16.0)\n",
      "Building wheels for collected packages: FlagEmbedding\n",
      "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.2.5-py3-none-any.whl size=43013 sha256=e77a829aed3df41b6f92ab0ddbf4f8d0e659969843a3873d09d006fbfe73b3f1\n",
      "  Stored in directory: /Users/nicholasshor/Library/Caches/pip/wheels/fa/88/19/e943a5c1531d0db10db20cfd3c124a881f2d7e06d7cf4aaac1\n",
      "Successfully built FlagEmbedding\n",
      "Installing collected packages: accelerate, FlagEmbedding\n",
      "Successfully installed FlagEmbedding-1.2.5 accelerate-0.27.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "5ffbc8d2-b09d-4231-bf90-06c9c6c0a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fc753de7334870a884c85a2fb9b222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dd4012ad6c4b37a8e155081635444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd0a03807864e9b954aab3f8db02c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda7d34178c4a45ab7a043599fc4140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b22c646f774f959c2431f528440885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a744c626b34f9488f05e325c4da458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447112b0-42e6-4b2d-9e48-002bd26f8c23",
   "metadata": {},
   "source": [
    "#### Predictive AI ML implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "cbed1f5c-e38c-4e8d-a5ca-e87a7b3ff337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai import preview\n",
    "from typing import Dict\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "d2784b7a-9ac2-4112-9c92-0913dc6da7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tabular_classification_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instance_dict: Dict,\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\n",
    "    \n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # for more info on the instance schema, please use get_model_sample.py\n",
    "    # and look at the yaml found in instance_schema_uri\n",
    "    instance = json_format.ParseDict(instance_dict, Value())\n",
    "    instances = [instance]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # See gs://google-cloud-aiplatform/schema/predict/prediction/tabular_classification_1.0.0.yaml for the format of the predictions.\n",
    "    prediction_list=[]\n",
    "    predictions = response.predictions\n",
    "    for prediction in predictions:\n",
    "        prediction_list.append(dict(prediction))\n",
    "    return prediction_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "9dd90346-3acc-4bb9-9ebb-68b09b4be933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "testing=predict_tabular_classification_sample(\n",
    "    project=\"dsc-180a-b09\",\n",
    "    endpoint_id=\"4607809140427849728\",\n",
    "    instance_dict={\"article\": news})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "id": "521a6356-6bb7-4657-9fb2-c99fb7f1c206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1836276352405548, 0.008846416138112545, 0.01037078443914652, 0.09028053283691406, 0.1769963353872299, 0.5298783183097839]"
      ]
     },
     "execution_count": 1053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing[0]['scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437b56d-bb04-4331-907f-d611f7030277",
   "metadata": {},
   "source": [
    "#### GEN AI Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "91d7256a-725a-42c3-8cfe-e1919de0eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "34385bfe-546f-4a04-9777-b437011329c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenAI_article_truth_processing(news_article, history):\n",
    "    #getting input history for context and starting chat\n",
    "    reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)\n",
    "    news_article = f\"\"\"{news_article}\"\"\"\n",
    "    history = history or []\n",
    "    #instantiating gemini pro\n",
    "    PROJECT_ID = \"gen-lang-client-0321728687\"\n",
    "    REGION = \"us-central1\"\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "    model = generative_models.GenerativeModel(\"gemini-pro\")\n",
    "    config = {\"max_output_tokens\": 2048, \"temperature\": 0.0}\n",
    "    \n",
    "    safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    }\n",
    "    chat = model.start_chat()\n",
    "    history = list(sum(history, ()))\n",
    "\n",
    "    #PerspectiveAPI output check\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=PERSPECTIVE_API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #chunking the news article for improved processing\n",
    "    chunked_article_list = tokenize_into_chunks(news_article, 50)\n",
    "    \n",
    "    #getting context and fact checks from vector database based on the provided input\n",
    "    all_response_text = []\n",
    "    context_list = []\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        context = RAG_CONTEXT_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        context_list.append(context)\n",
    "        \n",
    "    fact_checks_list=[]\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        fact_checks_list.append(fact_checks)\n",
    "\n",
    "\n",
    "    for i in range(len(context_list)):\n",
    "        input=chunked_article_list[i]\n",
    "        fact_checks = fact_checks_list[i]\n",
    "        context = context_list[i]\n",
    "        prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "        next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "        \n",
    "        history = [prev_chunk, input, next_chunk]\n",
    "        \n",
    "    \n",
    "        statement_rerank_list = []\n",
    "        #Iterating through the fact_check data and context data and creating a dictionary for each individual statement for Gen AI processing\n",
    "        for j in range(len(fact_checks['ids'][0])):\n",
    "            reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "            statement_rerank_list.append(reranking_statementSearch)\n",
    "    \n",
    "        \n",
    "        scores = reranker.compute_score(statement_rerank_list)\n",
    "        combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "    \n",
    "        context_rerank_list = []\n",
    "        for k in range(len(context['ids'][0])):\n",
    "            reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "            context_rerank_list.append(reranking_contextSearch)\n",
    "            \n",
    "        scores = reranker.compute_score(context_rerank_list)\n",
    "        combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "        context_window = 3\n",
    "        prepared_context = []\n",
    "        prepared_fact_checks = []\n",
    "        for i in range(context_window):\n",
    "            prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "            prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])\n",
    "\n",
    "        #Changing chunks from list of strings to one combined string for Gen AI processing\n",
    "        chunk_history_string = ''\n",
    "        for chunk in history:\n",
    "            if chunk != None:\n",
    "                chunk_history_string += chunk + \" \"\n",
    "\n",
    "\n",
    "        #generating initial response with prompt template\n",
    "        responses = model.generate_content(f\"\"\"Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "        knowledge you already have access to answer the question.\n",
    "    \n",
    "        If you are not very sure of your answer to the question, then use the additional information I've provided below within the \n",
    "        ((())) symbols to help you.\n",
    "        (((\n",
    "        Refer to these fact checked statements as well to determine your answer and be sure to pay close attention to the \n",
    "        metadata that is provided: {prepared_fact_checks}.\n",
    "        Use the following context to help answer the question: {prepared_context}.\n",
    "        You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "        Make sure you provide a short explanation of why you chose that score.\n",
    "        )))\n",
    "        <<<\n",
    "        Question: How true is the following statement on a scale of 1-100? + {input}. You must provide the score in this format Score:XX., \n",
    "        followed by your short explanation.\n",
    "        >>>\n",
    "       \"\"\",\n",
    "            generation_config=config,\n",
    "            stream=True,\n",
    "            safety_settings=safety_config,                          \n",
    "        )\n",
    "\n",
    "        \n",
    "        #obtaining individual responses\n",
    "        response_text = \"\"\n",
    "        response_text += \"Statement: \" + input\n",
    "        for response in responses:\n",
    "            response_text += response.text\n",
    "        response_text = response_text.replace(\"\\n\\n\", \". \")\n",
    "        all_response_text.append(response_text)\n",
    "        \n",
    "\n",
    "    #combining all responses    \n",
    "    entire_text_string = \"\"\n",
    "    for text in all_response_text:\n",
    "        entire_text_string += text\n",
    "    cleaned_text = entire_text_string\n",
    "    \n",
    "    #this section is finding and removing the statements that can't be rated by the chatbot\n",
    "    unratable_sentences = []\n",
    "    rated_sentences = []\n",
    "    \n",
    "    for response in all_response_text:\n",
    "        if \"article does not\" in response.lower() or \"context does not\" in response.lower() or \"statement is not\" in response.lower():\n",
    "            unratable_sentences.append(response)\n",
    "        else:\n",
    "            rated_sentences.append(response)\n",
    "    \n",
    "    not_enough_context = len(unratable_sentences)\n",
    "    enough_context = len(rated_sentences)\n",
    "    all_statements_count = len(all_response_text)\n",
    "\n",
    "    \n",
    "    #total score calculation with regex\n",
    "    pattern = r'Score:\\s(\\d+)\\.'\n",
    "    total_score = 0\n",
    "    matches = re.findall(pattern, cleaned_text)\n",
    "    for match in matches:\n",
    "        score = int(match)\n",
    "        total_score += score\n",
    "    average_score = total_score / len(rated_sentences)\n",
    "    rounded_average = round(average_score, 1)\n",
    "\n",
    "    #creating output in nice format for user\n",
    "    output_intro = f\"\"\"{enough_context} out of {all_statements_count} statements in the text could be rated. The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {rounded_average}/100. Some of the lowest rated statements are provided below.\"\"\"\n",
    "    tweaking_output = re.sub(r'(Score:\\s*\\d+\\.)(?!\\s*Explanation:)', r'\\1 Explanation:', cleaned_text)\n",
    "    parts = re.split(r\"(?=Statement:)\", tweaking_output)\n",
    "    split_parts=[]\n",
    "    # Clean each part and add to split_parts\n",
    "    for part in parts:\n",
    "        cleaned_part = output_clean(part)\n",
    "        split_parts.append(cleaned_part)\n",
    "    \n",
    "    # Initialize variables to store lowest scores and their respective entries\n",
    "    lowest_scores = [(float('inf'), ''), (float('inf'), ''), (float('inf'), '')]\n",
    "    \n",
    "    # Iterate through each string entry\n",
    "    for entry in split_parts:\n",
    "        # Find all occurrences of \"Score: \" followed by a number until a \".\"\n",
    "        scores = re.findall(r' Score:\\s(\\d+)\\.', entry)\n",
    "        # Convert scores to integers and update lowest_scores if necessary\n",
    "        for score in scores:\n",
    "            score_int = int(score)\n",
    "            if score_int < lowest_scores[-1][0]:\n",
    "                lowest_scores[-1] = (score_int, entry)\n",
    "                lowest_scores.sort()\n",
    "                \n",
    "    # Extract the entries for the three lowest scores\n",
    "    lowest_entries = [entry for score, entry in lowest_scores]\n",
    "\n",
    "    #reformatting for better readability\n",
    "    summary_output = \"\"\n",
    "    for statement in lowest_entries:\n",
    "        # Replace \"Statement:\", \"Score:\", and \"Explanation:\" with a new line followed by the keyword\n",
    "        formatted_statement = re.sub(r'(Statement:|Score:|Explanation:)', r'\\n\\1', statement)\n",
    "        # Append the formatted statement to the output\n",
    "        summary_output += formatted_statement.strip() + \"\\n\"\n",
    "    \n",
    "        # Add a new line after each statement\n",
    "        summary_output += \"\\n\"\n",
    "\n",
    "    output = output_intro + \"\\n\\n\" + summary_output\n",
    "\n",
    "    #Perspective API output safety check\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': output},\n",
    "      'requestedAttributes': requestedAttributes\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    \n",
    "    attributes_surpassed = []\n",
    "    for key in response['attributeScores']:\n",
    "        if response['attributeScores'][key]['summaryScore']['value'] > attributeThresholds[key]:\n",
    "            attributes_surpassed.append((key, response['attributeScores'][key]['summaryScore']['value']))\n",
    "    \n",
    "    #crafting output warning message if necessary or regular output message  \n",
    "    history_output = []\n",
    "    if len(attributes_surpassed) == 1:\n",
    "        attributes_violated = \"\"\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}category so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "        \n",
    "    elif len(attributes_surpassed) > 1:\n",
    "        attributes_violated = \"\"\n",
    "        counter = 1\n",
    "        attributes_count = len(attributes_surpassed)\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "            if counter < attributes_count:\n",
    "                attributes_violated += \"and \"\n",
    "            counter += 1\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}categories so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "\n",
    "    else:\n",
    "        history_output.append([news_article, output])\n",
    "    return history_output, history_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386c489-c97b-4066-95bd-e92bfa665554",
   "metadata": {},
   "source": [
    "#### Article testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "id": "65d2ba84-c597-4088-a7d4-d084ada7eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model generation for output to user\n",
    "    # final_responses = model.generate_content(f\"\"\"Each entry in the list of statements provided below inside <<<>>> begins with a number\n",
    "    # that explains how truthful a statement is and is followed by a text explanation to why that score was chosen. I need you to provide\n",
    "    # explanations for each statement on why they received the scores they did. \n",
    "    # I would like you to format your response like this, and continue to follow it for each statement you choose to include.\n",
    "    \n",
    "    # \"{enough_context} out of {all_statements_count} statements in the text could be rated. \n",
    "    # The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {overall_score}. Some of the lowest rated statements are provided below\"\n",
    "    \n",
    "    # \\nScore: XX\n",
    "    # Statement: \"Statement here\"\n",
    "    # Explanation: \"Explanation here\"\n",
    "\n",
    "    # # <<<\n",
    "    # # {rated_sentences}\n",
    "    # # >>>\"\"\",\n",
    "    #     generation_config=config,\n",
    "    #     stream=True,\n",
    "    #     safety_settings=safety_config,\n",
    "    # )\n",
    "\n",
    "    \n",
    "    # final_response_text = \"\"\n",
    "    # for response in final_responses:\n",
    "    #     final_response_text += response.text\n",
    "    # output = final_response_text.replace(\"\\n\\n\", \". \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "id": "acc812f6-2d3a-431b-b579-26f4877dcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"Months after leaving the White House, former President Donald Trump began plotting his return to Wall Street. That return, delayed by years of regulatory and legal hurdles, is now on the verge of becoming a reality — and it could make Trump a fortune.\n",
    "\n",
    "US regulators have finally given the green light to a controversial merger between Truth Social owner Trump Media & Technology Group and a blank-check company. The blessing from the Securities and Exchange Commission removes the last major obstacle holding back the deal.\n",
    "\n",
    "The merger, if approved by shareholders, would pave the way for Trump Media to become a publicly-traded company — one where Trump will own a dominant stake that could be worth billions.\n",
    "\n",
    "Digital World Acquisition Corp., the blank-check firm, announced that on Wednesday the SEC signed off on the merger proxy for the deal. A date for a shareholder vote will be set by Friday.\n",
    "\n",
    "“It does look like this deal is going to reach the finish line now — after more than two years of delays,” said Jay Ritter, a finance professor at the University of Florida.\n",
    "\n",
    "Trump stake could be worth $4 billion\n",
    "Shares of Digital World, a special purpose acquisition company, or SPAC, spiked 15% on the major milestone. The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress.\n",
    "\n",
    "Ritter estimates the merger could pave the way for about $270 million of cash coming into Trump Media, funds the company could fuel Truth Social’s growth.\n",
    "\n",
    "Trump is set to hold a dominant position in the newly-combined company, owning roughly 79 million shares, according to new SEC filings.\n",
    "\n",
    "The former president’s stake would be valued at $4 billion based on Digital World’s current trading price of about $50.\n",
    "\n",
    "Of course, as Ritter notes, it would be very difficult for Trump to translate that paper wealth into actual cash.\n",
    "\n",
    "Not only would Trump be subject to a lock-up period that would prevent he and other insiders from selling until six months after the merger, but the new company’s fortunes would be closely associated with the former president. That could make it difficult for Trump to sell even after the lock-up period expires.\n",
    "\n",
    "‘This is a meme stock’\n",
    "Moreover, there are major questions about the sky-high valuation being placed on this media company.\n",
    "\n",
    "“This is a meme stock. The valuation is totally divorced from the fundamental value of the company,” said Ritter.\n",
    "\n",
    "Digital World’s share price values the company at up to about $8 billion on a fully diluted basis, which includes all shares and options that could be converted to common stock, according to Ritter.\n",
    "\n",
    "He described that valuation as “crazy” because Trump Media is generating little revenue and burning through cash.\n",
    "\n",
    "New SEC filings indicate Trump Media’s revenue amounted to just $1.1 million during the third quarter. The company posted a loss of $26 million.\n",
    "\n",
    "Since the merger was first proposed in October 2021, legal, regulatory and financial questions have swirled about the transaction.\n",
    "\n",
    "In November, accountants warned that Trump Media was burning cash so rapidly that it might not survive unless the long-delayed merger with Digital World is completed soon.\n",
    "\n",
    "Shareholder vote looms\n",
    "Now, Trump execs are cheering the green light from the SEC.\n",
    "\n",
    "“Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. “Moving forward, we aim to accelerate our work to build a free speech highway outside the stifling stranglehold of Big Tech.”\n",
    "\n",
    "Eric Swider, Digital World’s CEO, described the SEC approval as a “significant milestone” and said executives are “immensely proud of the strides we’ve taken towards advancing” the merger.\n",
    "\n",
    "One of the final remaining hurdles is for Digital World shareholders to approve the merger in an upcoming vote.\n",
    "\n",
    "The shareholders have enormous incentive to approve the deal because if the merger fails, the blank-check firm would be forced to liquidate. That would leave shareholders with just $10 a share, compared with $50 in the market today.\n",
    "\n",
    "“Anyone who holds shares and votes against the merger is crazy,” said Ritter, the professor.\n",
    "\n",
    "“Then again, I might argue that everyone holding DWAC shares is crazy,” he added, referring to the company’s thin revenue and hefty valuation.\n",
    "\n",
    "Matthew Tuttle, CEO of Tuttle Capital Management, said he’s not surprised by the ups and downs surrounding this merger.\n",
    "\n",
    "“The thing about Trump and anything related to Trump is, love him or hate him, there is going to be drama,” said Tuttle, who purchased options to buy Digital World shares in his personal account. “Really, I would not have expected anything less.”\n",
    "\n",
    "Going forward, Tuttle said Trump Media’s share price will live and die by how everything plays out for Trump personally — from his legal troubles to his potential return to the White House.\n",
    "\n",
    "“Anything bullish for Trump is going to be bullish for the stock,” said Tuttle.\n",
    "\n",
    "Trump is no stranger to Wall Street, where he has a history, one marked by bankruptcies.\n",
    "\n",
    "Although Trump has never filed for personal bankruptcy, he has filed four business bankruptcies — all of them linked to casinos he used to own in Atlantic City.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "41149610-953a-4b48-a447-207b45c26035",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" Are Americans paying nearly $500 for an inhaler that would cost just $7 overseas?\n",
    "\n",
    "U.S. Sen. Tammy Baldwin, D-Wis., says there is a vast difference in the cost of prescriptions in the United States and the rest of the world. \n",
    "\n",
    "\"Big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US,\" Baldwin said Feb. 1 in a Facebook post. \"That has got to end. We've got to hold Big Pharma accountable for their price-gouging tactics. I won't stop fighting until we do.\"\n",
    "\n",
    "That massive cost difference piqued our interest.\n",
    "\n",
    "How much would patients pay? \n",
    "When we asked for backup information, Baldwin’s campaign staff directed us to drug pricing websites, news articles and news releases on the cost of Combivent Respimat (ipratropium bromide and albuterol), a combination medication used to treat chronic obstructive pulmonary disease (COPD). \n",
    "\n",
    "Combivent Respimat is available only as a brand-name medication and not available in generic form, according to Medical News Today, which pointed out that the actual price a patient would pay for the medication depends on type of insurance plan, location and pricing at the patient’s pharmacy. Medicare does cover Combivent Respimat. \n",
    "\n",
    "According to Drugs.com, a pricing website, Combivent Respimat costs about $525 for a supply of 4 grams, depending on the pharmacy. \n",
    "\n",
    "It’s also important to note, that on a practical basis, because of insurance and Medicare coverage, few people in the United States would actually pay $500 out of pocket\n",
    "\n",
    "\"Quoted prices are for cash-paying customers and are not valid with insurance plans,\" the website says  says. \n",
    "\n",
    "Another online drug pricing guide, GoodRx, puts the price of Combivent Respimat between about $477 and $584 at Madison, Wisconsin, pharmacies:\n",
    "\n",
    "Walgreens —    $508.39 \n",
    "\n",
    "Walmart —----   $514.45\n",
    "\n",
    "CVS Pharmacy-$508.14\n",
    "\n",
    "Hy-Vee —--------$477.97\n",
    "\n",
    "Costco —---------$584.59\n",
    "\n",
    "Target —----------$508.14\n",
    "\n",
    "FEATURED FACT-CHECK\n",
    "\n",
    "Instagram posts\n",
    "stated on February 15, 2024 in an Instagram post\n",
    "Because “17 million immigrants” were “let in” the U.S, “ foot and mouth disease is back. We got rid of that fifty years ago.”\n",
    "truefalse\n",
    "By Jeff Cercone • February 16, 2024\n",
    "Metro Market —-$511.00\n",
    "\n",
    "Pick ’n Save—---$511.00\n",
    "\n",
    "So, Baldwin is on target on the cost in the US.\n",
    "\n",
    "What about overseas?\n",
    "According to a Jan. 8 news release from U.S. Sen. Bernie Sanders, I-Vt., Combivent Respimat sold for just $7 In France.\n",
    "\n",
    "Sanders, chairman of the Senate Committee on Health, Education, Labor, and Pensions, sent letters to the CEOs of four pharmaceutical companies announcing an investigation into the high prices the companies are charging for inhalers. Baldwin and Democratic Sens. Ben Ray Luján of New Mexico and Ed Markey of Massachusetts also signed the letters.\n",
    "\n",
    "The letters were sent to the four biggest manufacturers of inhalers sold in the United States — AstraZeneca, Boehringer Ingelheim, GlaxoSmithKline (GSK) and Teva.\n",
    "\n",
    "\"It is beyond absurd that Boehringer Ingelheim charges $489 for Combivent Respimat in the United States, but just $7 in France,\" Sanders said in the news release.\n",
    "\n",
    "The news release said the Committee’s source for the price of Combivent Respimat in France was the Navlin international drug pricing database. \n",
    "\n",
    "Baldwin, in the news release, accuses companies of \"jacking up prices and turning record profits.\"\n",
    "\n",
    "Experts weigh in \n",
    "Dr. William B. Feldman noted that Baldwin is referring to list prices here — which are the prices that uninsured patients in the U.S. pay and the prices to which out-of-pocket costs are often tied.\n",
    "\n",
    "\"Manufacturers give sizable (confidential) rebates to insurers, and so the net prices for inhalers in the U.S. are below list prices — but still much higher than the net prices abroad,\" Feldman said in an email to PolitiFact Wisconsin. \n",
    "\n",
    "Feldman, who works at Brigham and Women’s Hospital in Boston and Harvard Medical School, said a key reason inhaler prices remain so high in the U.S. is that there is very little generic competition. \n",
    "\n",
    "\"Brand-name manufacturers have erected large patent thickets that keep generic competitors off the market,\" Feldman said. \" Inhaler prices are low elsewhere, in part, because governments negotiate prices based on the value of the drugs compared to existing therapies.\"\n",
    "\n",
    "David Kreling, professor emeritus in the School of Pharmacy at the University of Wisconsin-Madison, said the U.S. price quoted by Baldwin sounds about right.\n",
    "\n",
    "\"The $500 number may be in the ballpark for U.S. patented (brand-name, newer) drugs,\" Kreling said in an email to PolitiFact Wisconsin. \"That would be consistent with my understanding of market data on sales by firms in the U.S. Things in the $7 range, here, only reside within the off-patent generic drug market (where we have low prices, sometimes at or near lowest in the world).\" \n",
    "\n",
    "Our ruling\n",
    "Baldwin said \"big drug companies charge as little as $7 for an inhaler overseas and nearly $500 for the exact same one here in the US.\"\n",
    "\n",
    "Our review, and that of experts, found the numbers checked out.\n",
    "\n",
    "Experts cite a variety of reasons for the price differences, including very little generic competition in the United States, and few people in the United States would actually pay $500 out of pocket because of insurance and Medicare coverage. \n",
    "\n",
    "For a statement that is accurate but needs clarification or additional information, our rating is Mostly True.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf46ad3-6be1-4403-bcbd-1da7bdf416e6",
   "metadata": {},
   "source": [
    "#### Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "e96e64b1-270d-4bf3-b824-4b2bd8884e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = GenAI_article_truth_processing(news, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "id": "60a4ebc6-39d0-407b-a053-192a25e6a49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"27 out of 28 statements in the text could be rated. The following score and explanation is based on these 27 statements. The average truthfulness score from these 28 statements is 83.9/100. Some of the lowest rated statements are provided below.\\n\\nStatement: Digital World’s share price values the company at up to about $8 billion on a fully diluted basis, which includes all shares and options that could be converted to common stock, according to Ritter. \\nScore: 50. \\nExplanation: The statement is somewhat true because it is based on the share price of Digital World, which can be volatile and may not reflect the fundamental value of the company.\\n\\nStatement: “Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. \\nScore: 50. \\nExplanation: The statement is made by Devin Nunes, who has a history of pushing social media companies to restrict speech that he objects to. He has also filed several defamation lawsuits against media companies and critics. This suggests that he may not be genuinely committed to free expression.\\n\\nStatement: The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress. \\nScore: 70. \\nExplanation: The statement is mostly true because the stock has nearly tripled this year, and Trump's political success in the Republican presidential primary and the merger progress are likely contributing factors.\\n\\n\""
      ]
     },
     "execution_count": 1112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweaking_output = final_response[0][0][1]\n",
    "tweaking_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce79fbc-8802-41e6-b38a-57fc1ebb5c69",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "a177edc0-bac2-484c-b422-b12aa1c2907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.preview.generative_models._PreviewGenerativeModel at 0x37b540310>"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019ce28-d054-4dc4-be10-dd967f3ca57e",
   "metadata": {},
   "source": [
    "## Gradio (Website) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "cc37feb9-8a60-49e8-b852-e0c57e2c22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "14b810b0-e509-4351-a88f-19dd56ef6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = gr.Blocks()\n",
    "prompt_placeholder = \"Insert your news article here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "7401e9a3-5946-4525-822d-a00f932f5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with block:\n",
    "    gr.Markdown(\"\"\"<h1><center>Generative AI News Article Truthfulness Evaluator</center></h1>\n",
    "    \"\"\")\n",
    "    message = gr.Textbox(placeholder=prompt_placeholder)\n",
    "    chatbot = gr.Chatbot()\n",
    "    state = gr.State()\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    submit.click(GenAI_article_truth_processing, inputs=[message, state], outputs=[chatbot, state])\n",
    "    submit.click(make_plot, inputs=[message], outputs=gr.Plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "98a69c6a-dea1-471c-b61e-069f5e72360c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: http://f05db9db6f0e8ec5f6.disinformation-destroyers.com\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://f05db9db6f0e8ec5f6.disinformation-destroyers.com\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 230, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1590, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 678, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/1354177057.py\", line 20, in GenAI_article_truth_processing\n",
      "    history = list(sum(history, ()))\n",
      "                   ^^^^^^^^^^^^^^^^\n",
      "TypeError: can only concatenate tuple (not \"list\") to tuple\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response\n",
      " deployed_model_id: 4847858917050417152\n"
     ]
    }
   ],
   "source": [
    "block.launch(share=True, share_server_address=\"disinformation-destroyers.com:7000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf93b04-b2cd-4e28-986b-6b6df1447c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd8a63-43c4-4861-aca3-c7ab3846b949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
