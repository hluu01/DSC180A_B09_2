{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3a22c4dd-7522-42d0-aee0-9dd287628ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import chromadb\n",
    "from googleapiclient import discovery\n",
    "import json\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import GenerativeModel, Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3310d4cb-a74a-4de1-af37-c9f5343d42b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16831eb1-a5ff-415d-9469-5a58b6ad59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_short_strings(text):\n",
    "    return '' if len(text) < 7 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66a2d7b-8ec1-4192-8ffe-9c9acacfe19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d190ec3-4e6c-4408-b3bc-5dc908d8861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_into_chunks(text, min_words=75):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        if len(current_chunk) + len(words) < min_words:\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if any(sentence.endswith(p) for p in ['.', '!', '?', '¡', '¿']):\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = words\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fba32e66-62e5-4cf9-ad2f-2728561ae51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text with regex to convert to more readable format\n",
    "def clean_text(text):\n",
    "   \"\"\"\n",
    "   Cleans up text with specific spacing rules.\n",
    "\n",
    "   Args:\n",
    "       text: The text to be cleaned.\n",
    "\n",
    "   Returns:\n",
    "       The cleaned text.\n",
    "   \"\"\"\n",
    "\n",
    "   # Remove extra spaces around punctuation marks, except for commas.\n",
    "   text = re.sub(r\"\\s+([^,\\s\\w])\", r\"\\1\", text)\n",
    "   text = re.sub(r\"([^\\s\\w])\\s+\", r\"\\1\", text)\n",
    "\n",
    "   # Add a space before quotation marks, but not after.\n",
    "   text = re.sub(r\"([^\\s])\\'\", r\"\\1 '\", text)\n",
    "   text = re.sub(r'\"\\s', r'\"', text)\n",
    "\n",
    "   # Add a space after commas.\n",
    "   text = re.sub(r\",\", r\", \", text)\n",
    "\n",
    "   # Remove double backticks and single quotes that aren't part of the intended quotation.\n",
    "   text = re.sub(r\"`([^`]+)`\", r\"\\1\", text)\n",
    "   text = re.sub(r\"'([^']+)'\", r\"\\1\", text)\n",
    "\n",
    "   # Replace single backticks with standard apostrophes.\n",
    "   text = text.replace(\"`\", \"'\")\n",
    "\n",
    "   return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff905ef1-f846-42f4-9454-54747f87215d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c04f40a-88a8-4176-b467-cc51767e3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact articles\n",
    "pf_articles = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_articles.csv\")\n",
    "pf_articles = pf_articles.drop(columns='Unnamed: 0')\n",
    "pf_articles.rename(columns={'Statement': 'Title'}, inplace=True)\n",
    "pf_articles = pf_articles.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd711031-7b63-4af5-a8d2-e79755f84199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact truth datasets\n",
    "pf_statements = pd.read_csv(\"Data/Politifact_Data/CSV/politifact_truthometer_df.csv\")\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0')\n",
    "pf_statements = pf_statements.drop(columns='Unnamed: 0.1')\n",
    "pf_statements = pf_statements.dropna()\n",
    "pf_statements_full = pf_statements\n",
    "pf_statements = pf_statements.sample(frac=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fb33555-68fa-4d47-9ec1-44a8184ac6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factcheckorg_articles = pd.read_csv(\"Data/FactCheckOrg/factcheckorg_webscrape_200pages.csv\")\n",
    "factcheckorg_articles['List_data'].fillna('', inplace=True)\n",
    "factcheckorg_articles['List_data'] = factcheckorg_articles['List_data'].apply(filter_short_strings)\n",
    "factcheckorg_articles = factcheckorg_articles.dropna(subset=['Text'])\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace('Para leer en español, vea esta traducción de Google Translate.', '')\n",
    "factcheckorg_articles['Text'] = factcheckorg_articles['Text'].str.replace(r' Editor’s Note:.*$', '', regex=True)\n",
    "factcheckorg_articles = factcheckorg_articles.reset_index()\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['index'])\n",
    "factcheckorg_articles['Title_and_Date'] = factcheckorg_articles['Title'] + ' , ' + factcheckorg_articles['Date']\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['Title', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea624713-513c-49a2-978f-37efef9e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sciencefeedbackorg_articles = pd.read_csv(\"Data/ScienceFeedbackOrg/ScienceFeedbackOrg.csv\")\n",
    "sciencefeedbackorg_articles = sciencefeedbackorg_articles.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d2ea378e-cae2-4d27-805d-491e0f1ac8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scicheckorg_articles = pd.read_csv(\"Data/FactCheckOrg/scicheck_data.csv\")\n",
    "scicheckorg_articles['Title_and_Date'] = scicheckorg_articles['Title'] + ' , ' + scicheckorg_articles['Date']\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['Title', 'Date'])\n",
    "scicheckorg_articles.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced89dd-106a-4d60-adb3-a000944d564e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d85e0467-d390-42b5-80fa-15c959c0fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3949876440.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#FactCheckOrg Article Chunking\n",
    "factcheckorg_articles['chunks_text'] = factcheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "factcheckorg_articles['chunkslistdata'] = factcheckorg_articles['List_data'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = factcheckorg_articles['chunks_text'].apply(len).max()\n",
    "max_chunks_list_data = factcheckorg_articles['chunkslistdata'].apply(len).max()\n",
    "max_total_chunks = max(max_chunks_text, max_chunks_list_data)\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_total_chunks + 1):\n",
    "    factcheckorg_articles[f'chunk_text_{i}'] = factcheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "    factcheckorg_articles[f'chunklistdata{i}'] = factcheckorg_articles['chunkslistdata'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "factcheckorg_articles = factcheckorg_articles.drop(columns=['chunks_text', 'chunkslistdata', 'Text', 'List_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d04d655-1dd9-4e78-9104-3b41b6bea009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Politifact Statement Text Chunking\n",
    "pf_statements['chunks'] = pf_statements['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_statements['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_statements[f'chunk_{i}'] = pf_statements['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_statements = pf_statements.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9383641a-2e4c-45a3-b363-95aba9f5de1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/3076208089.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#Politifact Articles Chunking\n",
    "pf_articles['chunks'] = pf_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "max_chunks = pf_articles['chunks'].apply(len).max()\n",
    "\n",
    "for i in range(1, max_chunks + 1):\n",
    "    pf_articles[f'chunk_{i}'] = pf_articles['chunks'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "pf_articles = pf_articles.drop(columns=['chunks', 'Tldr_text_statements', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2fe76bdb-2700-4532-9380-1daec03e19eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
      "/var/folders/pb/7hrkp8tj05gbr513hzd6bv000000gp/T/ipykernel_56723/2555649897.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n"
     ]
    }
   ],
   "source": [
    "#SciCheckOrg Articles Chunking\n",
    "scicheckorg_articles['chunks_text'] = scicheckorg_articles['Text'].apply(tokenize_into_chunks)\n",
    "\n",
    "# Determine the maximum number of chunks across both columns\n",
    "max_chunks_text = scicheckorg_articles['chunks_text'].apply(len).max()\n",
    "\n",
    "# Create columns for each chunk in both 'Text' and 'List_data'\n",
    "for i in range(1, max_chunks_text + 1):\n",
    "    scicheckorg_articles[f'chunk_text_{i}'] = scicheckorg_articles['chunks_text'].apply(lambda x: x[i - 1] if len(x) >= i else None)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "scicheckorg_articles = scicheckorg_articles.drop(columns=['chunks_text', 'Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fe66e-bac9-461d-a7e2-a4311b23adec",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23b31956-0dbb-4bf3-8b2c-b7df2e88b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a961bb-41c9-4e28-bb96-360fbca000af",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_CONTEXT_VDB = chroma_client.create_collection(name=\"RAG_CONTEXT_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72fc7d10-34c5-4142-9965-9a674f004bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_STATEMENTS_VDB = chroma_client.create_collection(name=\"RAG_STATEMENTS_VDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40ea61f4-0dfe-4b95-9b43-46d3687fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding pf statement justifications to Context VDB\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "chunks_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements.iterrows():\n",
    "    statement = row['Statement']\n",
    "    claimer = row['Claimer']\n",
    "    for col in pf_statements.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                metadata_list.append({\"Statement\": statement, \"Context\": \"Yes\", \"Claimer\": claimer})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad55249-93cb-4196-8111-00946af61458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter justifications to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fce4111-b9e1-4692-b32e-d23ed3930d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding politifact truth-o-meter statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in pf_statements_full.iterrows():\n",
    "    truth_value = row['Truth_value']\n",
    "    claimer = row['Claimer']\n",
    "    statement = row['Statement']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value,\"Claimer\": claimer})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "afc8f317-5f3b-4ea7-81a1-fee50c26be78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf truth-o-meter statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c059fb88-3459-455a-9d21-33b62b13b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding factcheck.org data to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in factcheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in factcheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1\n",
    "        elif col.startswith('chunklist'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44369d55-9b8a-4e94-b4f4-876ff2e9bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "#Adding factcheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6af85c54-a3a7-4588-8d13-834688bc1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding SciCheckOrg articles to Context VDB\n",
    "chunks_list = []\n",
    "titles_list = []\n",
    "ids_list = []\n",
    "start_id = RAG_CONTEXT_VDB.count() + 1\n",
    "\n",
    "for index, row in scicheckorg_articles.iterrows():\n",
    "    title = row['Title_and_Date']\n",
    "    for col in scicheckorg_articles.columns:\n",
    "        if col.startswith('chunk_'):\n",
    "            chunk = row[col]\n",
    "            if chunk is not None:\n",
    "                chunks_list.append(chunk)\n",
    "                titles_list.append({\"Title_and_Date\": title, \"Context\": \"Yes\"})\n",
    "                ids_list.append(f\"id{start_id}\")\n",
    "                start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d4b02868-a4e4-4db1-8264-8adc05216e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "#Adding scicheckorg text to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_CONTEXT_VDB.add(\n",
    "        documents=chunks_list[start_size:batch_size],\n",
    "        metadatas=titles_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c40685c-37f7-4f1a-8bb2-d0780a3c343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding ScienceFeedbackOrg statements to Statements VDB\n",
    "statements_list = []\n",
    "ids_list = []\n",
    "metadata_list = []\n",
    "start_id = RAG_STATEMENTS_VDB.count() + 1\n",
    "\n",
    "for index, row in sciencefeedbackorg_articles.iterrows():\n",
    "    truth_value = row['label']\n",
    "    statement = row['claim']\n",
    "\n",
    "    metadata_list.append({\"Statements truthfulness\":truth_value})\n",
    "    statements_list.append(statement)\n",
    "    \n",
    "    ids_list.append(f\"id{start_id}\")\n",
    "    start_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "13af0dcf-fc27-446e-a249-d5869954d016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((\u001b[38;5;28mlen\u001b[39m(chunks_list)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mRAG_STATEMENTS_VDB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatements_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_size\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     start_size \u001b[38;5;241m=\u001b[39m start_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n\u001b[1;32m     11\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m+\u001b[39m batch_size_increment\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:146\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    106\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     (\n\u001b[1;32m    140\u001b[0m         ids,\n\u001b[1;32m    141\u001b[0m         embeddings,\n\u001b[1;32m    142\u001b[0m         metadatas,\n\u001b[1;32m    143\u001b[0m         documents,\n\u001b[1;32m    144\u001b[0m         images,\n\u001b[1;32m    145\u001b[0m         uris,\n\u001b[0;32m--> 146\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/models/Collection.py:545\u001b[0m, in \u001b[0;36mCollection._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    525\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     Optional[URIs],\n\u001b[1;32m    544\u001b[0m ]:\n\u001b[0;32m--> 545\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    547\u001b[0m         validate_embeddings(\n\u001b[1;32m    548\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    553\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    554\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/chromadb/api/types.py:213\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    215\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got []"
     ]
    }
   ],
   "source": [
    "#Adding pf sciencefeedback statements to vector database in batches of 5000 (max batch size is just over 5000)\n",
    "start_size = 0\n",
    "batch_size_increment = 5000\n",
    "batch_size = 5000\n",
    "for i in range(((len(chunks_list)//batch_size)+1)):\n",
    "    RAG_STATEMENTS_VDB.add(\n",
    "        documents=statements_list[start_size:batch_size],\n",
    "        metadatas=metadata_list[start_size:batch_size],\n",
    "        ids=ids_list[start_size:batch_size])\n",
    "    start_size = start_size + batch_size_increment\n",
    "    batch_size = batch_size + batch_size_increment\n",
    "    print(start_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a337cf1-74e2-4e5a-ba6b-1f9917e09d02",
   "metadata": {},
   "source": [
    "## FULL GEN AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b44f93a4-9895-4d5e-aeed-8dd8e2b5689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from vertexai.preview import generative_models\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f4e77-5130-4be5-8292-aaebc511f392",
   "metadata": {},
   "source": [
    "#### Perspective API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2f83baf7-d65d-42fd-81f3-c945bf22b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSPECTIVE_API_KEY = 'AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5c58d5c6-946d-422f-84b3-1df34ddc35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholds of output\n",
    "attributeThresholds = {\n",
    "    'INSULT': 0.8,\n",
    "    'TOXICITY': 0.8,\n",
    "    'THREAT': 0.5,\n",
    "    'SEXUALLY_EXPLICIT': 0.5,\n",
    "    'PROFANITY': 0.8\n",
    "}\n",
    "requestedAttributes = {}\n",
    "for key in attributeThresholds:\n",
    "    requestedAttributes[key] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359fa40-6fc2-4c82-961c-3d800bdb28da",
   "metadata": {},
   "source": [
    "# Liar Liar Dataset Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6d52eb5a-93b3-4b58-8060-8c878ad4c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_liar_plus = pd.read_csv(\"Data/Liar_plus/train.tsv\", delimiter='\\t', header=None)\n",
    "liar_liar_plus = liar_liar_plus[[3, 2]]\n",
    "liar_liar_plus.dropna(inplace=True)\n",
    "llp_statements = liar_liar_plus[3]\n",
    "llp_labels = liar_liar_plus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "53e04697-0e2f-44df-b892-dd286e4af9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Says the Annies List political group supports ...\n",
       "1        When did the decline of coal start? It started...\n",
       "2        Hillary Clinton agrees with John McCain \"by vo...\n",
       "3        Health care reform legislation is likely to ma...\n",
       "4        The economic turnaround started at the end of ...\n",
       "                               ...                        \n",
       "10237    There are a larger number of shark attacks in ...\n",
       "10238    Democrats have now become the party of the [At...\n",
       "10239    Says an alternative to Social Security that op...\n",
       "10240    On lifting the U.S. Cuban embargo and allowing...\n",
       "10241    The Department of Veterans Affairs has a manua...\n",
       "Name: 3, Length: 10240, dtype: object"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llp_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d8e76cef-2794-4659-8c98-02655571549b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[373], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m llp_statements:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[43mGenAI_article_truth_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      6\u001b[0m         label_prediction\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[342], line 156\u001b[0m, in \u001b[0;36mGenAI_article_truth_processing\u001b[0;34m(news_article, history)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m#Perspective API output safety check\u001b[39;00m\n\u001b[1;32m    152\u001b[0m analyze_request \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomment\u001b[39m\u001b[38;5;124m'\u001b[39m: { \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: output},\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequestedAttributes\u001b[39m\u001b[38;5;124m'\u001b[39m: requestedAttributes\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 156\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manalyze_request\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m attributes_surpassed \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattributeScores\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=AIzaSyCElMgVeT2_ng6hSnJMNHXt4t78fOv8J9U&alt=json returned \"Attribute SEXUALLY_EXPLICIT does not support request languages: de\". Details: \"[{'@type': 'type.googleapis.com/google.commentanalyzer.v1alpha1.Error', 'errorType': 'LANGUAGE_NOT_SUPPORTED_BY_ATTRIBUTE', 'languageNotSupportedByAttributeError': {'detectedLanguages': ['de'], 'attribute': 'SEXUALLY_EXPLICIT'}}]\">"
     ]
    }
   ],
   "source": [
    "label_prediction = []\n",
    "for i in llp_statements:\n",
    "    try:\n",
    "        label_prediction.append(GenAI_article_truth_processing(i,[])[0][0][1])\n",
    "    except (ValueError, IndexError) as e:\n",
    "        label_prediction.append(None)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "219d4d6c-dc30-45bb-af12-6338a7f31bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(label_prediction)):\n",
    "    # Check if the current value matches the target value\n",
    "    if label_prediction[i] == 'False':\n",
    "        label_prediction[i] = 'false'\n",
    "    elif label_prediction[i] == 'Half-true':\n",
    "        label_prediction[i] = 'half-true'\n",
    "    elif label_prediction[i] == 'Mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'Mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'True':\n",
    "        label_prediction[i] = 'true'\n",
    "    elif label_prediction[i] == 'mostly-false':\n",
    "        label_prediction[i] = 'barely-true'\n",
    "    elif label_prediction[i] == 'mostly-true, mostly-true':\n",
    "        label_prediction[i] = 'mostly-true'\n",
    "    elif label_prediction[i] == 'Pants-on-fire':\n",
    "        label_prediction[i] = 'pants-fire'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "1cf10b70-cc1a-46b1-89c6-73262048f736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'true'}"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_llp_labels = set(llp_labels)\n",
    "unique_llp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "a17b1a74-6c21-4dd5-b1c1-17ca0ff37383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Barely true',\n",
       " 'Barely-true',\n",
       " 'Half True',\n",
       " 'Half-True',\n",
       " 'Mostly False',\n",
       " 'Mostly True',\n",
       " 'Mostly-False',\n",
       " 'Mostly-True',\n",
       " None,\n",
       " 'None provided',\n",
       " \"['half-true', 'false']\",\n",
       " \"['half-true', 'mostly-true']\",\n",
       " \"['mostly-true', 'false']\",\n",
       " \"['true', 'false', 'mostly-true', 'mostly-true', 'half-true', 'true', 'half-true', 'half-true', 'mostly-true', 'mostly-true', 'pants-on-fire', 'pants-on-fire', 'mostly-true', 'true']\",\n",
       " \"['true', 'mostly-true', 'mostly-true']\",\n",
       " \"['true', 'mostly-true']\",\n",
       " 'barely-true',\n",
       " 'false',\n",
       " 'half-true',\n",
       " 'half-true\\nmostly-true',\n",
       " 'half-true, false',\n",
       " 'half-true, mostly-true',\n",
       " 'half-true, pants-on-fire',\n",
       " 'half-true, true',\n",
       " 'mostly-false, mostly-true',\n",
       " 'mostly-true',\n",
       " 'mostly-true, half-true',\n",
       " 'mostly-true, mostly-false',\n",
       " 'mostly-true, true',\n",
       " 'pants-fire',\n",
       " 'pants-on-fire',\n",
       " 'pants-on-fire, half-true',\n",
       " 'pants-on-fire, true',\n",
       " 'true',\n",
       " 'true, mostly-true'}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_label_predictions = set(label_prediction)\n",
    "unique_label_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "62ab7a55-081a-4eec-a10a-3d5dc8f96d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_matches = 0\n",
    "for value1, value2 in zip(label_prediction, llp_labels):\n",
    "    if value1 == value2:\n",
    "        num_matches += 1\n",
    "\n",
    "accuracy = num_matches / len(label_prediction) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a62e276a-a999-4e48-9a58-76ed0c9c69c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.44951140065146"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2d0be6-c7ef-481e-b3da-dd323a9ef99f",
   "metadata": {},
   "source": [
    "## Cohere API implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "876b0001-2280-44d3-8fbe-c85337ad6619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting FlagEmbedding\n",
      "  Downloading FlagEmbedding-1.2.5.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.33.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (4.35.2)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.15.0)\n",
      "Collecting accelerate>=0.20.1\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentence_transformers in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from FlagEmbedding) (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (23.2)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from accelerate>=0.20.1->FlagEmbedding) (0.4.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch>=1.6.0->FlagEmbedding) (2023.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers>=4.33.0->FlagEmbedding) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (2.1.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from datasets->FlagEmbedding) (3.8.6)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (1.3.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (1.11.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sentence_transformers->FlagEmbedding) (0.1.99)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (3.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp->datasets->FlagEmbedding) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers>=4.33.0->FlagEmbedding) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch>=1.6.0->FlagEmbedding) (2.1.3)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers->FlagEmbedding) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk->sentence_transformers->FlagEmbedding) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->datasets->FlagEmbedding) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn->sentence_transformers->FlagEmbedding) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch>=1.6.0->FlagEmbedding) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision->sentence_transformers->FlagEmbedding) (10.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->FlagEmbedding) (1.16.0)\n",
      "Building wheels for collected packages: FlagEmbedding\n",
      "  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for FlagEmbedding: filename=FlagEmbedding-1.2.5-py3-none-any.whl size=43013 sha256=e77a829aed3df41b6f92ab0ddbf4f8d0e659969843a3873d09d006fbfe73b3f1\n",
      "  Stored in directory: /Users/nicholasshor/Library/Caches/pip/wheels/fa/88/19/e943a5c1531d0db10db20cfd3c124a881f2d7e06d7cf4aaac1\n",
      "Successfully built FlagEmbedding\n",
      "Installing collected packages: accelerate, FlagEmbedding\n",
      "Successfully installed FlagEmbedding-1.2.5 accelerate-0.27.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.11 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U FlagEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "5ffbc8d2-b09d-4231-bf90-06c9c6c0a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fc753de7334870a884c85a2fb9b222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dd4012ad6c4b37a8e155081635444d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd0a03807864e9b954aab3f8db02c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda7d34178c4a45ab7a043599fc4140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b22c646f774f959c2431f528440885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a744c626b34f9488f05e325c4da458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from FlagEmbedding import FlagReranker\n",
    "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "3f2d65f5-846a-46f6-8785-cffd160e4cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_article_list = tokenize_into_chunks(news_article, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "318d56a3-3a0f-46d9-82d7-4dbcad1f2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_response_text = []\n",
    "context_list = []\n",
    "for i in range(len(chunked_article_list)):\n",
    "    input = chunked_article_list[i]\n",
    "    context = RAG_CONTEXT_VDB.query(\n",
    "        query_texts=[input],\n",
    "        n_results=8,\n",
    "    )\n",
    "    context_list.append(context)\n",
    "        \n",
    "fact_checks_list=[]\n",
    "for i in range(len(chunked_article_list)):\n",
    "    input = chunked_article_list[i]\n",
    "    fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "        query_texts=[input],\n",
    "        n_results=8,\n",
    "    )\n",
    "    fact_checks_list.append(fact_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "05ffd161-82be-4e47-b8f7-22ec6dad3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(context_list)):\n",
    "    input=chunked_article_list[i]\n",
    "    fact_checks = fact_checks_list[i]\n",
    "    context = context_list[i]\n",
    "    prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "    next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "    \n",
    "    history = [prev_chunk, input, next_chunk]\n",
    "    \n",
    "\n",
    "    statement_rerank_list = []\n",
    "    #Iterating through the fact_check data and context data and creating a dictionary for each individual statement for Gen AI processing\n",
    "    for j in range(len(fact_checks['ids'][0])):\n",
    "        reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "        statement_rerank_list.append(reranking_statementSearch)\n",
    "\n",
    "    \n",
    "    scores = reranker.compute_score(statement_rerank_list)\n",
    "    combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "    sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "    sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "\n",
    "\n",
    "    context_rerank_list = []\n",
    "    for k in range(len(context['ids'][0])):\n",
    "        reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "        context_rerank_list.append(reranking_contextSearch)\n",
    "        \n",
    "    scores = reranker.compute_score(context_rerank_list)\n",
    "    combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "    sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "    sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "\n",
    "context_window = 4\n",
    "prepared_context = []\n",
    "prepared_fact_checks = []\n",
    "for i in range(context_window):\n",
    "    prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "    prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437b56d-bb04-4331-907f-d611f7030277",
   "metadata": {},
   "source": [
    "#### GEN AI Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "id": "34385bfe-546f-4a04-9777-b437011329c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenAI_article_truth_processing(news_article, history):\n",
    "    #getting input history for context and starting chat\n",
    "    news_article = f\"\"\"{news_article}\"\"\"\n",
    "    history = history or []\n",
    "    #instantiating gemini pro\n",
    "    PROJECT_ID = \"gen-lang-client-0321728687\"\n",
    "    REGION = \"us-central1\"\n",
    "    vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "    model = generative_models.GenerativeModel(\"gemini-pro\")\n",
    "    config = {\"max_output_tokens\": 2048, \"temperature\": 0.0}\n",
    "    \n",
    "    safety_config = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH\n",
    "    }\n",
    "    chat = model.start_chat()\n",
    "    history = list(sum(history, ()))\n",
    "\n",
    "    #PerspectiveAPI output check\n",
    "    client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=PERSPECTIVE_API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #chunking the news article for improved processing\n",
    "    chunked_article_list = tokenize_into_chunks(news_article, 50)\n",
    "    \n",
    "    #getting context and fact checks from vector database based on the provided input\n",
    "    all_response_text = []\n",
    "    context_list = []\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        context = RAG_CONTEXT_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        context_list.append(context)\n",
    "        \n",
    "    fact_checks_list=[]\n",
    "    for i in range(len(chunked_article_list)):\n",
    "        input = chunked_article_list[i]\n",
    "        fact_checks = RAG_STATEMENTS_VDB.query(\n",
    "            query_texts=[input],\n",
    "            n_results=7,\n",
    "        )\n",
    "        fact_checks_list.append(fact_checks)\n",
    "\n",
    "\n",
    "    for i in range(len(context_list)):\n",
    "        input=chunked_article_list[i]\n",
    "        fact_checks = fact_checks_list[i]\n",
    "        context = context_list[i]\n",
    "        prev_chunk = chunked_article_list[i - 1] if i > 0 else None\n",
    "        next_chunk = chunked_article_list[i + 1] if i + 1 < len(chunked_article_list) else None\n",
    "        \n",
    "        history = [prev_chunk, input, next_chunk]\n",
    "        \n",
    "    \n",
    "        statement_rerank_list = []\n",
    "        #Iterating through the fact_check data and context data and creating a dictionary for each individual statement for Gen AI processing\n",
    "        for j in range(len(fact_checks['ids'][0])):\n",
    "            reranking_statementSearch = [input, fact_checks['documents'][0][j]]\n",
    "            statement_rerank_list.append(reranking_statementSearch)\n",
    "    \n",
    "        \n",
    "        scores = reranker.compute_score(statement_rerank_list)\n",
    "        combined_statement_scores = list(zip(scores, statement_rerank_list, fact_checks['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_statement_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_statement_scores, sorted_statement_rerank_list, sorted_factCheck_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "    \n",
    "        context_rerank_list = []\n",
    "        for k in range(len(context['ids'][0])):\n",
    "            reranking_contextSearch = [input, context['documents'][0][k]]\n",
    "            context_rerank_list.append(reranking_contextSearch)\n",
    "            \n",
    "        scores = reranker.compute_score(context_rerank_list)\n",
    "        combined_context_scores = list(zip(scores, context_rerank_list, context['metadatas'][0]))\n",
    "        sorted_combined_data = sorted(combined_context_scores, key=lambda x: x[0], reverse=True)\n",
    "        sorted_context_scores, sorted_context_rerank_list, sorted_context_metadata = zip(*sorted_combined_data)\n",
    "    \n",
    "        context_window = 3\n",
    "        prepared_context = []\n",
    "        prepared_fact_checks = []\n",
    "        for i in range(context_window):\n",
    "            prepared_context.append([sorted_context_metadata[i], sorted_context_rerank_list[i][1]])\n",
    "            prepared_fact_checks.append([sorted_factCheck_metadata[i], sorted_statement_rerank_list[i][1]])\n",
    "\n",
    "        #Changing chunks from list of strings to one combined string for Gen AI processing\n",
    "        chunk_history_string = ''\n",
    "        for chunk in history:\n",
    "            if chunk != None:\n",
    "                chunk_history_string += chunk + \" \"\n",
    "\n",
    "\n",
    "        #generating initial response with prompt template\n",
    "        responses = model.generate_content(f\"\"\"Answer the question below marked inside <<<>>> in a full sentence based on the\n",
    "        knowledge you already have access to answer the question.\n",
    "    \n",
    "        If you are not very sure of your answer to the question, then use the additional information I've provided below within the \n",
    "        ((())) symbols to help you.\n",
    "        (((\n",
    "        Refer to these fact checked statements as well to determine your answer and be sure to pay close attention to the \n",
    "        metadata that is provided: {prepared_fact_checks}.\n",
    "        Use the following context to help answer the question: {prepared_context}.\n",
    "        You may also use the chat history provided to help you understand the context better if available: {chunk_history_string}.\n",
    "        Make sure you provide a short explanation of why you chose that score.\n",
    "        )))\n",
    "        <<<\n",
    "        Question: How true is the following statement on a scale of 1-100? + {input}. You must provide the score in this format Score:XX., \n",
    "        followed by the input statement, and then followed by your short explanation.\n",
    "        >>>\n",
    "       \"\"\",\n",
    "            generation_config=config,\n",
    "            stream=True,\n",
    "            safety_settings=safety_config,                          \n",
    "        )\n",
    "\n",
    "        \n",
    "    #obtaining individual responses\n",
    "        response_text = \"\"\n",
    "        for response in responses:\n",
    "            response_text += response.text\n",
    "        response_text = response_text.replace(\"\\n\\n\", \". \")\n",
    "        all_response_text.append(response_text)\n",
    "        \n",
    "\n",
    "    #combining all responses    \n",
    "    entire_text_string = \"\"\n",
    "    for text in all_response_text:\n",
    "        entire_text_string += text\n",
    "    cleaned_text = clean_text(entire_text_string)\n",
    "    \n",
    "    #this section is finding and removing the statements that can't be rated by the chatbot\n",
    "    unratable_sentences = []\n",
    "    rated_sentences = []\n",
    "    \n",
    "    for response in all_response_text:\n",
    "        if \"article does not\" in response.lower() or \"context does not\" in response.lower() or \"statement is not\" in response.lower():\n",
    "            unratable_sentences.append(response)\n",
    "        else:\n",
    "            rated_sentences.append(response)\n",
    "    \n",
    "    not_enough_context = len(unratable_sentences)\n",
    "    enough_context = len(rated_sentences)\n",
    "    all_statements_count = len(all_response_text)\n",
    "\n",
    "    # Regular expression pattern\n",
    "    number_pattern = r'\\b(\\d+(?:\\.\\d+)?)(?:\\.|%)'\n",
    "    total_score = 0\n",
    "    \n",
    "    # Extracting the number value\n",
    "    for statement in rated_sentences:\n",
    "        match = re.search(number_pattern, statement)\n",
    "        if match:\n",
    "            number = match.group(0)[:-1]  # Removing the period\n",
    "            number = int(number)\n",
    "            total_score += number\n",
    "            \n",
    "    overall_score = total_score / len(rated_sentences)\n",
    "    overall_score = round(float(overall_score), 1)\n",
    "    \n",
    "    #model generation for output to user\n",
    "    final_responses = model.generate_content(f\"\"\"Each entry in the list of statements provided below inside <<<>>> begins with a number\n",
    "    that explains how truthful a statement is and is followed by a text explanation to why that score was chosen. I need you to select the \n",
    "    three statements with the lowest scores and return them back with their explanations to why they received low scores. \n",
    "    I would like you to format your response like this, and continue to follow it for each statement you choose to include.\n",
    "    \n",
    "    \"{enough_context} out of {all_statements_count} statements in the text could be rated. \n",
    "    The following score and explanation is based on these {enough_context} statements. The average truthfulness score from these {all_statements_count} statements is {overall_score}. Some of the lowest rated statements are provided below\"\n",
    "    \n",
    "    \\nScore: XX\n",
    "    Statement: \"Statement here\"\n",
    "    Explanation: \"Explanation here\"\n",
    "\n",
    "    <<<\n",
    "    {rated_sentences}\n",
    "    >>>\"\"\",\n",
    "        generation_config=config,\n",
    "        stream=True,\n",
    "        safety_settings=safety_config,\n",
    "    )\n",
    "\n",
    "    # final_responses = model.generate_content(f\"\"\"Each entry in the list of statements provided below inside <<<>>> begins with a number\n",
    "    # that explains how truthful a statement is and is followed by a text explanation to why that score was chosen. I need you to convert that\n",
    "    # number to a label based on the following categories. Scores from 0-16 should be labeled pants-on-fire, scores from 17-33 should be\n",
    "    # labeled false, scores from 34-50 should be labeled mostly-false, scores from 50-66 should be labeled half-true, scores from 67-83 should be\n",
    "    # labeled mostly-true, and scores from 84-100 should be labeled true. All I want you to return to me is the label.\n",
    "    # <<<\n",
    "    # {rated_sentences}\n",
    "    # >>>\"\"\",\n",
    "    #     generation_config=config,\n",
    "    #     stream=True,\n",
    "    #     safety_settings=safety_config,\n",
    "    # )\n",
    "    \n",
    "    \n",
    "    final_response_text = \"\"\n",
    "    for response in final_responses:\n",
    "        final_response_text += response.text\n",
    "    output = final_response_text.replace(\"\\n\\n\", \". \")\n",
    "\n",
    "    #Perspective API output safety check\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': output},\n",
    "      'requestedAttributes': requestedAttributes\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    \n",
    "    attributes_surpassed = []\n",
    "    for key in response['attributeScores']:\n",
    "        if response['attributeScores'][key]['summaryScore']['value'] > attributeThresholds[key]:\n",
    "            attributes_surpassed.append((key, response['attributeScores'][key]['summaryScore']['value']))\n",
    "    \n",
    "    #crafting output warning message if necessary or regular output message  \n",
    "    history_output = []\n",
    "    if len(attributes_surpassed) == 1:\n",
    "        attributes_violated = \"\"\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}category so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "        \n",
    "    elif len(attributes_surpassed) > 1:\n",
    "        attributes_violated = \"\"\n",
    "        counter = 1\n",
    "        attributes_count = len(attributes_surpassed)\n",
    "        for i in attributes_surpassed:\n",
    "            attributes_violated += i[0] + \" \"\n",
    "            if counter < attributes_count:\n",
    "                attributes_violated += \"and \"\n",
    "            counter += 1\n",
    "        warning_message = f\"\"\"We're sorry, the output message surpasses our threshold for the {attributes_violated}categories so we cannot safely provide a response. Please try again with a different input.\"\"\"\n",
    "        history_output.append([news_article, warning_message])\n",
    "\n",
    "    else:\n",
    "        history_output.append([news_article, output])\n",
    "    return history_output, history_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "acc812f6-2d3a-431b-b579-26f4877dcf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = \"\"\"Months after leaving the White House, former President Donald Trump began plotting his return to Wall Street. That return, delayed by years of regulatory and legal hurdles, is now on the verge of becoming a reality — and it could make Trump a fortune.\n",
    "\n",
    "US regulators have finally given the green light to a controversial merger between Truth Social owner Trump Media & Technology Group and a blank-check company. The blessing from the Securities and Exchange Commission removes the last major obstacle holding back the deal.\n",
    "\n",
    "The merger, if approved by shareholders, would pave the way for Trump Media to become a publicly-traded company — one where Trump will own a dominant stake that could be worth billions.\n",
    "\n",
    "Digital World Acquisition Corp., the blank-check firm, announced that on Wednesday the SEC signed off on the merger proxy for the deal. A date for a shareholder vote will be set by Friday.\n",
    "\n",
    "“It does look like this deal is going to reach the finish line now — after more than two years of delays,” said Jay Ritter, a finance professor at the University of Florida.\n",
    "\n",
    "Trump stake could be worth $4 billion\n",
    "Shares of Digital World, a special purpose acquisition company, or SPAC, spiked 15% on the major milestone. The stock has nearly tripled this year, fueled by Trump’s political success in the Republican presidential primary, and now the merger progress.\n",
    "\n",
    "Ritter estimates the merger could pave the way for about $270 million of cash coming into Trump Media, funds the company could fuel Truth Social’s growth.\n",
    "\n",
    "Trump is set to hold a dominant position in the newly-combined company, owning roughly 79 million shares, according to new SEC filings.\n",
    "\n",
    "The former president’s stake would be valued at $4 billion based on Digital World’s current trading price of about $50.\n",
    "\n",
    "Of course, as Ritter notes, it would be very difficult for Trump to translate that paper wealth into actual cash.\n",
    "\n",
    "Not only would Trump be subject to a lock-up period that would prevent he and other insiders from selling until six months after the merger, but the new company’s fortunes would be closely associated with the former president. That could make it difficult for Trump to sell even after the lock-up period expires.\n",
    "\n",
    "‘This is a meme stock’\n",
    "Moreover, there are major questions about the sky-high valuation being placed on this media company.\n",
    "\n",
    "“This is a meme stock. The valuation is totally divorced from the fundamental value of the company,” said Ritter.\n",
    "\n",
    "Digital World’s share price values the company at up to about $8 billion on a fully diluted basis, which includes all shares and options that could be converted to common stock, according to Ritter.\n",
    "\n",
    "He described that valuation as “crazy” because Trump Media is generating little revenue and burning through cash.\n",
    "\n",
    "New SEC filings indicate Trump Media’s revenue amounted to just $1.1 million during the third quarter. The company posted a loss of $26 million.\n",
    "\n",
    "Since the merger was first proposed in October 2021, legal, regulatory and financial questions have swirled about the transaction.\n",
    "\n",
    "In November, accountants warned that Trump Media was burning cash so rapidly that it might not survive unless the long-delayed merger with Digital World is completed soon.\n",
    "\n",
    "Shareholder vote looms\n",
    "Now, Trump execs are cheering the green light from the SEC.\n",
    "\n",
    "“Truth Social was created to serve as a safe harbor for free expression and to give people their voices back,” Trump Media CEO Devin Nunes, a former Republican congressman, said in a statement. “Moving forward, we aim to accelerate our work to build a free speech highway outside the stifling stranglehold of Big Tech.”\n",
    "\n",
    "Eric Swider, Digital World’s CEO, described the SEC approval as a “significant milestone” and said executives are “immensely proud of the strides we’ve taken towards advancing” the merger.\n",
    "\n",
    "One of the final remaining hurdles is for Digital World shareholders to approve the merger in an upcoming vote.\n",
    "\n",
    "The shareholders have enormous incentive to approve the deal because if the merger fails, the blank-check firm would be forced to liquidate. That would leave shareholders with just $10 a share, compared with $50 in the market today.\n",
    "\n",
    "“Anyone who holds shares and votes against the merger is crazy,” said Ritter, the professor.\n",
    "\n",
    "“Then again, I might argue that everyone holding DWAC shares is crazy,” he added, referring to the company’s thin revenue and hefty valuation.\n",
    "\n",
    "Matthew Tuttle, CEO of Tuttle Capital Management, said he’s not surprised by the ups and downs surrounding this merger.\n",
    "\n",
    "“The thing about Trump and anything related to Trump is, love him or hate him, there is going to be drama,” said Tuttle, who purchased options to buy Digital World shares in his personal account. “Really, I would not have expected anything less.”\n",
    "\n",
    "Going forward, Tuttle said Trump Media’s share price will live and die by how everything plays out for Trump personally — from his legal troubles to his potential return to the White House.\n",
    "\n",
    "“Anything bullish for Trump is going to be bullish for the stock,” said Tuttle.\n",
    "\n",
    "Trump is no stranger to Wall Street, where he has a history, one marked by bankruptcies.\n",
    "\n",
    "Although Trump has never filed for personal bankruptcy, he has filed four business bankruptcies — all of them linked to casinos he used to own in Atlantic City.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "id": "e96e64b1-270d-4bf3-b824-4b2bd8884e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response = GenAI_article_truth_processing(news, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "60a4ebc6-39d0-407b-a053-192a25e6a49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26 out of 28 statements in the text could be rated. \\n    The following score and explanation is based on these 26 statements. The average truthfulness score from these 28 statements is 85.8. Some of the lowest rated statements are provided below\". Score: 50\\n    Statement: “ Truth Social was created to serve as a safe harbor for free expression and to give people their voices back , ” Trump Media CEO Devin Nunes , a former Republican congressman , said in a statement ..\\n    Explanation: The statement is made by Devin Nunes, who has a history of pushing social media companies to restrict speech that he objected to. He has also filed several defamation lawsuits against media companies and critics. This suggests that he may not be fully committed to free speech.. Score: 70\\n    Statement: “ Then again , I might argue that everyone holding DWAC shares is crazy , ” he added , referring to the company ’ s thin revenue and hefty valuation.\\\\nExplanation: The statement is somewhat true because the company\\'s revenue is thin and its valuation is hefty.. Score: 70\\n    Statement: The stock has nearly tripled this year , fueled by Trump ’ s political success in the Republican presidential primary , and now the merger progress .\\\\nExplanation: The statement is mostly true because the stock has nearly tripled this year, and Trump\\'s political success in the Republican presidential primary and the merger progress are likely contributing factors.'"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_response[0][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce79fbc-8802-41e6-b38a-57fc1ebb5c69",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "a177edc0-bac2-484c-b422-b12aa1c2907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<vertexai.preview.generative_models._PreviewGenerativeModel at 0x37b540310>"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9ab08f-7b19-475d-974d-12bcb3ea0ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865f823-e151-40aa-a264-229f5171d467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc001d-3d44-4698-9acf-c563b6c0d72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885aa358-3a5e-4874-b4b7-113a3331d6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945612fd-61e1-4c95-9b98-aabf7cd0e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f019ce28-d054-4dc4-be10-dd967f3ca57e",
   "metadata": {},
   "source": [
    "## Gradio (Website) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "cc37feb9-8a60-49e8-b852-e0c57e2c22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "14b810b0-e509-4351-a88f-19dd56ef6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = gr.Blocks()\n",
    "prompt_placeholder = \"Insert your news article here!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "7401e9a3-5946-4525-822d-a00f932f5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with block:\n",
    "    gr.Markdown(\"\"\"<h1><center>Generative AI News Article Truthfulness Evaluator</center></h1>\n",
    "    \"\"\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=prompt_placeholder)\n",
    "    state = gr.State()\n",
    "    submit = gr.Button(\"SEND\")\n",
    "    submit.click(GenAI_article_truth_processing, inputs=[message, state], outputs=[chatbot, state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "6aa25263-7dd9-4688-9044-b158e41642f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on public URL: https://c9cf96d887b56476f1.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c9cf96d887b56476f1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://c9cf96d887b56476f1.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.launch(debug = True, share = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f9a2bb-698b-4f1f-a724-10ce32a10eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
